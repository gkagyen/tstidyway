[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Timeseries Analysis - the Tidyverse Way",
    "section": "",
    "text": "Welcome\nA practical, beginner-friendly guide using tsibble, fable and friends\nHave you ever looked at a line chart of sales, website traffic, stock prices or weather patterns and thought, ‚ÄúI wonder what happens next?‚Äù ‚Äì then you are in the right place.\nIn this book, we will take your basic R skills and transform them into real-world forecasting power ‚Äì all using the clean modern tools of the Tidyverse. No black-box algorithms or confusing jargon. Just a step by step journey into understanding patterns over time and predicting the future with confidence.\nWhether you are a student, analyst or data enthusiast, time series skills are essential in your field ‚Äì and surprisingly accessible. By the end of this guide, you will be able to:\nSo grab your pen and paper (whatever it is you want to grab üòÅ), fire up RStudio and let‚Äôs turn your curiosity into prediction. üìä‚ú®",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Timeseries Analysis - the Tidyverse Way",
    "section": "",
    "text": "Visualise trends and seasonality like a pro\nDecompose complex patterns\nBuild and compare forecasts using smart, automated models\nAnd most importantly explain your results clearly",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 What is Time Series Data?\nTime series data consists of observations recorded over time, usually at regular intervals (e.g., daily, monthly, yearly). Some examples include:\nWhat makes time series special is that time is not just a variable, It carries important structure and dependencies. What happened today can depend on what happened yesterday, last month or even last year. This temporal uniqueness is what makes time series very powerful.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-time-series-data",
    "href": "intro.html#what-is-time-series-data",
    "title": "1¬† Introduction",
    "section": "",
    "text": "Monthly rainfall totals\nDaily COVID-19 cases\nHourly temperature readings\nYearly population counts",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-time-series-analysis-matters",
    "href": "intro.html#why-time-series-analysis-matters",
    "title": "1¬† Introduction",
    "section": "1.2 Why Time Series Analysis Matters",
    "text": "1.2 Why Time Series Analysis Matters\nTime series analysis helps us to understand the past, monitor the present, and predict the future. Some real world examples are:\n\nPublic Health: Forecasting disease outbreaks or hospital admissions\nFinance: Predicting stock prices, currency exchange rates, or sales revenue.\nEnvironmental Science: Analysing temperature trends or rainfall patterns for climate studies\nEngineering: Monitoring sensor data to detect faults or changes in performance\n\nIn many cases, this analysis supports decision making ‚Äî whether it is planning resources, anticipating risks, or detecting unusual patterns.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#traditional-vs.-tidy-approach",
    "href": "intro.html#traditional-vs.-tidy-approach",
    "title": "1¬† Introduction",
    "section": "1.3 Traditional vs.¬†Tidy Approach",
    "text": "1.3 Traditional vs.¬†Tidy Approach\nHistorically, time series in R has been handled using ts objects and packages like forecast. These are still useful, but they do not fit perfectly with the modern tidy philosophy; data frames, pipelines and consistent syntax.\nWe will follow the tidy time series workflow using the tidyverse ecosystem. The workflow uses packages from the tidyverts ecosystem:\n\ntsibble: A tidy data structure for time series (like a tibble but with special time handling)\nfable: For forecasting models (ARIMA, ETS, etc) in a tidy way.\nfeasts: For exploratory analysis (seasonal plots, decomposition, autocorrelation).\nlubridate: For working with dates and times.\nggplot2: For beautiful and flexible visualisations\ndplyr/tidyr/tibble: For general data wrangling\n\n\n1.3.1 The Tidy Time Series Workflow\nThe tidyverse ecosystem in R emphasises clean, readable code and consistent data structure. For time series, the modern approach uses the tidyverts suite of packages.\nThe typical flow we will follow includes:\n\nüîß Data Preparation: Load and tidy data. Convert the data to a tsibble object so R knows how to handle time.\nüîç Explore: Visualise trends, seasonality, patterns\nüß© Decompose: Break down components (trends, seasonal, noise)\nüñ•Ô∏è Model: Fit forecasting models (simple ‚Äî&gt; advanced)\nüìà Forecast: Generate future predictions\nüìè Evaluate: Check how good the forecasts are (accuracy assessment)\n\nThis workflow is clean, consistent and integrates smoothly with other tidyverse tools you might already know.\n\n\n\n\n\n\nTip\n\n\n\ntidyverse: collection of packages designed for general data science\ntidyverts: collection of packages specifically for time series analysis\nThey all follow the tidy philosophy, structure and grammar\n\n\n\n\n1.3.2 Data We Will Use\nWe will start with some built-in datasets from the tsibble package so every on can follow along without downloading external files:\n\naus_production ‚Äî Quarterly production values for Australian industries.\ntourism ‚Äî Quarterly overnight trips in different Australian regions.\n\nLater, we shall also show how to use real-life datasets ‚Äî for example, population growth or GDP growth data from Ghana, to make the examples relatable and practical\nYou can download the datasets used here gh_data.csv .\n\n\n1.3.3 What You Will Need to Follow Along\n\nBasic R Knowledge: You should know how to load packages, run functions, and work with data frames\nRStudio installed for a smooth workflow\nInternet connection (for package installation and possible data download)\n\n\n\n1.3.4 By the end of this book, you will be able to:\n\nHandle date/time data with ease\nExplore time series visually and statistically\nBuild forecasts using tidyverse-style functions\nApply your skills to your own datasets in research or work",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setting Up",
    "section": "",
    "text": "Before diving into analysis, let‚Äôs make sure we have all the tools we need to get ready. We need to set up our R environment with the right tools. The tidyverts ecosystem ‚Äî built around the tidyverse ‚Äî makes time series analysis intuitive, consistent and visual.\n\n\n\n\n\n\n\nTidyverts Package Collection\n\n\n\n\n\nTidyverse Package Collection\n\n\nThis section will cover installing and loading the packages we will use and checking that your R environment is prepared for a smooth time series work\nWe will address the following:\n\n2¬† Required Packages: This part will cover all the packages we will need, how to install and load them for use in our R environment.\n3¬† Setting Up Your RStudio Environment: Here you will see how to keep things organised in a more seamless and orderly manner on your system.\n\nAfter going through this section your R environment will be a fully-equipped time series analysis workshop. You will understand the role of each major package (tsibble, fable, feasts) and how they interoperate.\nMore importantly you will get the practical knowledge to install and load them correctly, setting a stable foundation for every analysis you perform in the following chapters.",
    "crumbs": [
      "Setting Up"
    ]
  },
  {
    "objectID": "required-packages.html",
    "href": "required-packages.html",
    "title": "2¬† Required Packages",
    "section": "",
    "text": "2.1 Installing the Packages\nAs stated earlier we will be using a combination of tidyverse and time series specific packages (tidyverts)\nThere is another supporting package in the tidyverts ecosystem called fabletools. This is a supporting package that provides some underlying tools for the fables package\nTo install the above packages, run the following code once in your R console:\ninstall.packages(c(\"tidyverse\", \"tsibble\", \"fable\", \"feasts\", \"readxl\"))",
    "crumbs": [
      "Setting Up",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Required Packages</span>"
    ]
  },
  {
    "objectID": "required-packages.html#installing-the-packages",
    "href": "required-packages.html#installing-the-packages",
    "title": "2¬† Required Packages",
    "section": "",
    "text": "Tip\n\n\n\nIf you already have them installed, you do not need to reinstall. You can check if a package is installed by typing, for instance \"fable\" %in% installed.packages() to check if the fable package is already installed.\n\n\n\n2.1.1 Loading the Packages\nAfter installing any package you have to load it before it becomes available for use. Each time you start a new R session, you will need to load the packages you will use again. Doing it correctly saves headaches. You can load a package in R with the library() function.\n\n# Load all packages\nlibrary(tidyverse)          # Core data wrangling + visualisation\nlibrary(tsibble)            # Tidy time series structure\nlibrary(fable)              # Forecasting\nlibrary(feasts)             # Time series exploration tools\nlibrary(readxl)             # Importing Excel files\n\nThink of the install command as buying a tool and placing it in your garage (your computer‚Äôs library). The library() function is like grabbing the tool from your garage and bringing it to your workbench (your current R session) so you can use it\n\n\n\n\n\n\nTip\n\n\n\nIf you see a message like ‚Äúthere is no package called‚Ä¶‚Äù, it means you have not installed it yet. Go back to the installation step.",
    "crumbs": [
      "Setting Up",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Required Packages</span>"
    ]
  },
  {
    "objectID": "environment-setup.html",
    "href": "environment-setup.html",
    "title": "3¬† Setting Up Your RStudio Environment",
    "section": "",
    "text": "3.1 Quick Test Run\nWe can go ahead and start building our time series analysis task right away after installing and loading the needed packages. However, to keep things organised you might want to;\nLet‚Äôs run a quick test to ensure everything is working. We shall use a built in dataset\n# Use the 'us_employment' dataset from the fpp3 package\nlibrary(fpp3)\nhead(us_employment)\n\n# Lets see if it is a proper tsibble and then we can plot it\nus_employment |&gt; \n  filter(Title == \"Total Private\") |&gt; \n  autoplot(Employed) +\n  labs(title = \"US Total Private Employment\",\n       y = \"People\")\nThis ‚Äúhead(us_employment)‚Äù line of code should show you a tidy time series dataset with index (time column) and key (group identifier). The codes that follow, if it runs without errors, should also produce a time series plot.\nIf you see all the things mentioned earlier, congratulations! Your environment is ready.",
    "crumbs": [
      "Setting Up",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Setting Up Your RStudio Environment</span>"
    ]
  },
  {
    "objectID": "environment-setup.html#quick-test-run",
    "href": "environment-setup.html#quick-test-run",
    "title": "3¬† Setting Up Your RStudio Environment",
    "section": "",
    "text": "Note\n\n\n\nThe fpp3 package is a collection of tools and datasets (which we have already seen) for time series forecasting. It was built to accompany the third edition of the book Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos .",
    "crumbs": [
      "Setting Up",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Setting Up Your RStudio Environment</span>"
    ]
  },
  {
    "objectID": "tidy-timeseries.html",
    "href": "tidy-timeseries.html",
    "title": "Working with Time Series Data",
    "section": "",
    "text": "The tidy Approach to Time Series\nTime is the most fundamental dimension in data analysis, yet working with temporal data has traditionally been one of the most frustrating aspects of data science. If you have ever felt less enthusiastic trying to force dates and data into a standard data frame, then here is some good news for you. The Tidyverse provides a coherent, intuitive framework for taming the chaos of temporal data.\nIn this section, we will explore how to work with time series data using the tidy data principles. Rather than wrestling with complex time series objects or memorising some mysterious date formatting codes, you will learn to treat temporal data as just another type of structured data that can be manipulated, visualised and modelled using the same consistent grammar you already know.\n%%{init: {'themeVariables': { 'fontSize': '15pt'}}}%%\n\nflowchart TD\n    A[Raw Time Series Data] --&gt; B{Data Structure}\n    B --&gt;|Traditional| C[Specialized TS Objects]\n    B --&gt;|Tidy| D[tsibble]\n    \n    C --&gt; E[Limited Functions]\n    C --&gt; F[Complex Syntax]\n    C --&gt; G[Isolated Workflow]\n    \n    D --&gt; H[dplyr Verbs]\n    D --&gt; I[ggplot2 Graphics]\n    D --&gt; J[Tidyverse Integration]\n    \n    H --&gt; K[filter, mutate, summarize...]\n    I --&gt; L[geom_line, facet_wrap...]\n    J --&gt; M[Seamless Data Pipeline]\n    \n    K --&gt; N[Flexible Analysis]\n    L --&gt; N\n    M --&gt; N\n    \n    E --&gt; O[Rigid Analysis]\n    F --&gt; O\n    G --&gt; O\n    \n    style D fill:#e1f5fe\n    style N fill:#c8e6c9\n    style O fill:#ffcdd2\nTraditional time series analysis in R has been handled using ts objects and packages like forecast. These are still useful, but they do not fit perfectly with the modern tidyverse philosophy. The tidy approach treats time series as regular dataframes which extends the tibble data structure by incorporating time series components (index and key).\nTable¬†1: A tidy time series data structure (tsibble)\n\n\n# A tsibble: 6 x 5 [1Q]\n# Key:       Region, State, Purpose [1]\n  Quarter Region   State           Purpose  Trips\n    &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n1 1998 Q1 Adelaide South Australia Business  135.\n2 1998 Q2 Adelaide South Australia Business  110.\n3 1998 Q3 Adelaide South Australia Business  166.\n4 1998 Q4 Adelaide South Australia Business  127.\n5 1999 Q1 Adelaide South Australia Business  137.\n6 1999 Q2 Adelaide South Australia Business  200.\nThis approach unlocks the full power of dplyr's data manipulation verbs, ggplot2's visualisation capabilities and the broader tidyverse ecosystem for temporal data analysis.",
    "crumbs": [
      "Working with Time Series Data"
    ]
  },
  {
    "objectID": "tidy-timeseries.html#what-we-will-cover",
    "href": "tidy-timeseries.html#what-we-will-cover",
    "title": "Working with Time Series Data",
    "section": "What We will Cover",
    "text": "What We will Cover\nThis section is organised into three complementary chapters that builds your time series skills progressively.\n\n4¬† Tidy Time Series Basics: This chapter will help you master the fundamental building blocks of time series data analysis. It establishes the foundation for all subsequent time series work.\n5¬† Dealing with Time Gaps and Irregularities: Real world time series data is messy, with missing observations, irregular intervals and unexpected gaps. This chapter teaches you to identify understand and handle these imperfections.\n6¬† Importing Data and Creating a tsibble (Bonus): Time series data comes from everywhere ‚Äì csv files, APIs, databases and excel files. Some of these sources might have inconsistent date formats, dates being stored as strings and time zones being mixed up. In this chapter you will be provided with practical strategies for importing temporal data and handling different date formats\n\nThroughout this section we will primarily use these essential packages lubridate, tsibble, dplyr, tidyr. By the end of this section you will have mastered the crucial first step in any time series analysis: creating a valid, gap-aware tsibble from raw data. This is not just busy work, it is about building a solid foundation.\nA well structured tsibble ensures that every analysis, visualisation, and forecast that follows is built on accurate and understandable temporal data.",
    "crumbs": [
      "Working with Time Series Data"
    ]
  },
  {
    "objectID": "timeseries-basics.html",
    "href": "timeseries-basics.html",
    "title": "4¬† Tidy Time Series Basics",
    "section": "",
    "text": "4.1 What is a tsibble\nAt its core, a time series is just a data where when it was collected is a crucial part of the story. It is a sequence of data points indexed in time and order. Regardless, a time series should still be a tidy data frame. In a tidy dataset (following Hadley Wickham‚Äôs principles), Each row is an observation, each column is a variable.\nA tsibble (short for ‚Äútidy temporal tibble‚Äù) is a special data frame for time series. It extends the tibble data structure by formally declaring one column as the index (the time variable) and optionally, one or more columns as key(s) (unique identifiers for different series)",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidy Time Series Basics</span>"
    ]
  },
  {
    "objectID": "timeseries-basics.html#what-is-a-tsibble",
    "href": "timeseries-basics.html#what-is-a-tsibble",
    "title": "4¬† Tidy Time Series Basics",
    "section": "",
    "text": "4.1.1 Key terminologies\nIndex: The variable (usually a date/time) that defines the time order of observations\nKey: A variable (or combination of variables) that uniquely identifies different time series within a single table e.g.¬†country, stock_symbol etc.\nInterval: The regular frequency of the measurements e.g.¬†daily, monthly, quarterly etc.\nTo illustrate what we just discussed below is a standard dataframe with a date column. This data is not time aware yet (not a time series)\n\n\n# A tibble: 4 √ó 3\n  date       product sales\n  &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n1 2023-01-01 A         120\n2 2023-01-02 A         145\n3 2023-01-01 B          88\n4 2023-01-02 B         102\n\n\na tsibble on the other hand is self-aware. It knows its index and its key, unlocking powerful analysis tool. compare the table below. The ‚Äú[1D]‚Äù beside the tsibble dimension represents the interval (daily) and the ‚Äú[2]‚Äù represents the number of unique keys/series.\n\n\n# A tsibble: 4 x 3 [1D]\n# Key:       product [2]\n  date       product sales\n  &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n1 2023-01-01 A         120\n2 2023-01-02 A         145\n3 2023-01-01 B          88\n4 2023-01-02 B         102",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidy Time Series Basics</span>"
    ]
  },
  {
    "objectID": "timeseries-basics.html#creating-and-converting-data-into-a-tsibble",
    "href": "timeseries-basics.html#creating-and-converting-data-into-a-tsibble",
    "title": "4¬† Tidy Time Series Basics",
    "section": "\n4.2 Creating and Converting Data into a tsibble\n",
    "text": "4.2 Creating and Converting Data into a tsibble\n\ncreating a tsibble follows the same procedure as creating a normal data frame using the tsibble() function or simply from an existing data frame or tibble using the as_tsibble() function.\nAs explained earlier, these two functions require you to specify at least one of these two things (index or key) or both depending on your data.\nWe see how to create a tsibble from using tsibble() and as_tsibble below\n\n# single time series\ntsibble(\n  year = 2010:2050,\n  value = rnorm(41),\n  index = year\n)\n\nThe above code creates a simple time series data using the tsibble() function, exactly like creating a dataframe. Indicating the index variable distinguishes a tsibble from a tibble.\nNow we will create a tsibble from the as_tsibble() function using an existing tibble\n\n# creating a tsibble from monthly data sales\n# sales come from two different shops\nsales_data &lt;- tibble(\n  Date = ymd(c('2025-01-01','2025-02-01','2025-03-01','2026-04-01','2025-05-01','2025-01-01','2025-02-01','2025-03-01','2025-04-01','2025-05-01')),\n  Store = rep(c('Phone Shop', 'Beauty Shop'), each = 5),\n  Sales = c(225, 150, 130, 90, 220, 190, 145, 180, 110, 180)\n)\nprint(sales_data)\n\n\n# We use as_tsibble to convert the tibble to a tsibble\n# the date column becomes the index and the store column the key\nsales_data_ts &lt;- sales_data |&gt; \n  mutate(Date = yearmonth(Date)) |&gt;\n  as_tsibble(\n    index = Date,\n    key = Store\n  )\nprint(sales_data_ts)\n\nWe now have our sales data in a time aware tsibble format. The yearmonth() is an index function to represent our Date in a year-month format.\n\n4.2.1 Practice tsibble conversion\nHere we will learn how to convert real world data into a tsibble. The data we will use is a time series data for Ghana containing certain world bank indicators. The data can be found here gh_data.csv. The data comes in a wide format (where years are columns). To make it tidy, where each row represents one indicator so that we a year and value column, we first need to reshape it into a long format\nWe will use pivot_longer from tidyr to reshape the data\n\n# load data\ngh_data_raw &lt;- read_csv('data/gh_data.csv', show_col_types = FALSE) \n  \n# convert wide data to long\ngh_data_long &lt;- gh_data_raw |&gt; \n  pivot_longer(\n    cols = starts_with(c('19','20')), # select all date columns\n    names_to = \"Year\",                # New column for years\n    values_to = 'Value'               # New column for values\n  )\n\n# View the first few rows\nhead(gh_data_long)\n\n# A tibble: 6 √ó 5\n  `Country Name` `Indicator Name` `Indicator Code` Year    Value\n  &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;   &lt;dbl&gt;\n1 Ghana          Population_total SP.POP.TOTL      1960  6961215\n2 Ghana          Population_total SP.POP.TOTL      1961  7162667\n3 Ghana          Population_total SP.POP.TOTL      1962  7337375\n4 Ghana          Population_total SP.POP.TOTL      1963  7514714\n5 Ghana          Population_total SP.POP.TOTL      1964  7695739\n6 Ghana          Population_total SP.POP.TOTL      1965  7882606\n\n\nSuccess! Now each row is an observation of an indicator in a given year\nThe next step is to convert the Year to a Proper Date. Right now Year is a character. We need it as a date so tsibble can understand time ordering. we will use the lubridate package functions and convert it to a Date object (assuming January 1st of each year).\n\n# convert year to date\ngh_data_long &lt;-  gh_data_long |&gt; \n  mutate(\n    Year = as.integer(Year),                       # convert to integer number\n    Date = lubridate::ymd(paste0(Year, \"-01-01\")), # Create date:Jan 1 of each year\n    .after = Year                                  # add new Date column after year column\n  )\n\n# check structure\nstr(gh_data_long$Date)\n\n Date[1:1430], format: \"1960-01-01\" \"1961-01-01\" \"1962-01-01\" \"1963-01-01\" \"1964-01-01\" ...\n\n\nNow Date is a proper Date object essential for time series analysis.\nFinally we can convert our data to a time series dataframe (tsibble) that is time aware of - index (time)and keys (unique series identifiers). In our case the Date column is our index and the Indicator_name becomes our index since we have multiple time series -population, life expectancy etc.\n\n# convert data to tsibble\ngh_ts &lt;- gh_data_long |&gt; \n  as_tsibble(\n    index = Date,              # Time index\n    key = `Indicator Name`     # Key column: each indicator is a separate series\n  )\n\n# View the tsibble\ngh_ts\n\n# A tsibble: 1,430 x 6 [1D]\n# Key:       Indicator Name [22]\n   `Country Name` `Indicator Name`      `Indicator Code`  Year Date        Value\n   &lt;chr&gt;          &lt;chr&gt;                 &lt;chr&gt;            &lt;int&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1960 1960-01-01 NA    \n 2 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1961 1961-01-01  3.43 \n 3 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1962 1962-01-01  4.11 \n 4 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1963 1963-01-01  4.41 \n 5 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1964 1964-01-01  2.21 \n 6 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1965 1965-01-01  1.37 \n 7 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1966 1966-01-01 -4.26 \n 8 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1967 1967-01-01  3.08 \n 9 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1968 1968-01-01  0.369\n10 Ghana          Annual GDP growth ra‚Ä¶ NY.GDP.MKTP.KD.‚Ä¶  1969 1969-01-01  6.01 \n# ‚Ñπ 1,420 more rows\n\n\nWe have our time series tsibble ready! But there are some nuances in the data, the first obvious ones are the column names- Country Name, Indicator Name and Indicator Code-we see that they are surrounded in back ticks (`) . This tells us that they do not follow the correct naming convention for variables in R (no spaces between words). The next one is the interval [1D]. The tsibble package thinks our data is supposed to be a daily data because we have a full date with month and days available. We will see how to fix this in the next chapter",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidy Time Series Basics</span>"
    ]
  },
  {
    "objectID": "timegaps.html",
    "href": "timegaps.html",
    "title": "5¬† Dealing with Time Gaps and Irregularities",
    "section": "",
    "text": "5.1 Why Time Gaps Matter\nReal-world data is messy. Sensors fails, public holidays happen, data does not get entered. This leads to gaps in your time series ‚Äì missing entries in the index where we expect a measurement. Traditional data frames ignore this, but tsibble helps you find and handle them.\nMany time series models and visualisations assume regular spaced data. Gaps can lead to errors, misleading plots and inaccurate forecasts.",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Dealing with Time Gaps and Irregularities</span>"
    ]
  },
  {
    "objectID": "timegaps.html#how-tsibble-helps",
    "href": "timegaps.html#how-tsibble-helps",
    "title": "5¬† Dealing with Time Gaps and Irregularities",
    "section": "\n5.2 How tsibble Helps",
    "text": "5.2 How tsibble Helps\nWe can check the time gaps in our tsibble using the scan_gaps function from the tsibble package. This is a very handy function that compares our actual data against a complete regular timeline and tells us exactly what is missing. Other equally useful functions for handling time gaps include count_gaps and has_gaps.\nWe will scan our gh_ts tsibble to check if there are gaps. But before that, we will fix the column names with janitor::clean_names() to avoid complications later on in our analysis due to the unconventional column names.\n\ngh_ts &lt;- gh_ts |&gt; \n  janitor::clean_names() \n\ngh_ts |&gt; scan_gaps()\n## # A tsibble: 512,864 x 2 [1D]\n## # Key:       indicator_name [22]\n##   indicator_name         date      \n##   &lt;chr&gt;                  &lt;date&gt;    \n## 1 Annual GDP growth rate 1960-01-02\n## 2 Annual GDP growth rate 1960-01-03\n## 3 Annual GDP growth rate 1960-01-04\n## 4 Annual GDP growth rate 1960-01-05\n## 5 Annual GDP growth rate 1960-01-06\n## # ‚Ñπ 512,859 more rows\n\nthe scan gaps returns a long list of missing dates. However, this is not true for our data set. We are getting this many gaps because the inner workings of tsibble thinks that the days of all the months a missing, meanwhile our data is not a daily data but a yearly series. This whole misinterpretation comes from the incorrect interval [1D]. To fix this we can change the date to a year-month format or use the year column as the index and then rescan for gaps\n\n# fix interval  by assigning index to year\ngh_ts |&gt; \n  as_tsibble(\n    index = year\n  ) |&gt; head(2)\n## # A tsibble: 2 x 6 [1Y]\n## # Key:       indicator_name [1]\n##   country_name indicator_name         indicator_code  year date       value\n##   &lt;chr&gt;        &lt;chr&gt;                  &lt;chr&gt;          &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n## 1 Ghana        Annual GDP growth rate NY.GDP.MKTP.K‚Ä¶  1960 1960-01-01 NA   \n## 2 Ghana        Annual GDP growth rate NY.GDP.MKTP.K‚Ä¶  1961 1961-01-01  3.43\n\nHere we see that the interval is now correctly represented a [1Y] which is exactly what we expect. We do not want to change our index variable so what we will do is keep the date as index but modify it somehow for it to be recognised as a year interval.\n\ngh_ts &lt;- gh_ts |&gt; \n  mutate(date = yearmonth(date))\n\ngh_ts |&gt; head(3)\n## # A tsibble: 3 x 6 [12M]\n## # Key:       indicator_name [1]\n##   country_name indicator_name         indicator_code    year     date value\n##   &lt;chr&gt;        &lt;chr&gt;                  &lt;chr&gt;            &lt;dbl&gt;    &lt;mth&gt; &lt;dbl&gt;\n## 1 Ghana        Annual GDP growth rate NY.GDP.MKTP.KD.‚Ä¶  1960 1960 Jan NA   \n## 2 Ghana        Annual GDP growth rate NY.GDP.MKTP.KD.‚Ä¶  1961 1961 Jan  3.43\n## 3 Ghana        Annual GDP growth rate NY.GDP.MKTP.KD.‚Ä¶  1962 1962 Jan  4.11\n\nOur interval is [12M]-meaning 12 months, essentially indicating a year. We can go ahead and check for gaps now.\n\ngh_ts |&gt; scan_gaps()\n## # A tsibble: 0 x 2 [?]\n## # Key:       indicator_name [0]\n## # ‚Ñπ 2 variables: indicator_name &lt;chr&gt;, date &lt;mth&gt;\n\nThis returns an empty tsibble, telling us that there are no time gaps. we can confirm again with the has_gaps() function.\n\ngh_ts |&gt; has_gaps() |&gt; \n  pull(.gaps) |&gt; sum()\n## [1] 0\n\nConfirms Zero Gaps in our gh_ts data!",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Dealing with Time Gaps and Irregularities</span>"
    ]
  },
  {
    "objectID": "timegaps.html#handling-time-gaps",
    "href": "timegaps.html#handling-time-gaps",
    "title": "5¬† Dealing with Time Gaps and Irregularities",
    "section": "\n5.3 Handling Time Gaps",
    "text": "5.3 Handling Time Gaps\nIn order for us to understand the concept of time gaps and how to handle to properly I will simulate a data with implicit time gaps and then we will see how to deal with them using functions from the tsibble package\nWe will use the same product sales concept to simulate this series. Here we compare sales for 2 products Smart Phone and Laptop over a 12 month period\n\n# create a sequence of sales dates\nstart_date &lt;- ym('2025-01')  \nend_date &lt;- ym('2025-12')\nsales_period &lt;- seq.Date(\n  from = start_date,\n  to = end_date,\n  by = 'month'\n) |&gt; \n  yearmonth()     # change format to year-month\n\n# simulate data with missing dates for different products\nsales_data_gaps &lt;- tsibble(\n  Product = c(rep('Smart Phone', 10), rep('Laptop', 8)),\n  Sales = round(c(rnorm(10,300,65), runif(8, 620, 1000))),\n  Date = c(\n    sales_period[c(1:5,8:12)],      # Smart phone is missing Jun & Jul\n    sales_period[c(2,3,4,5,6,9:11)] # Laptop is missing Jan, Jul, Aug & Dec\n  ),\n  index = Date,\n  key = Product\n)\n\nprint(sales_data_gaps)\n## # A tsibble: 18 x 3 [1M]\n## # Key:       Product [2]\n##   Product Sales     Date\n##   &lt;chr&gt;   &lt;dbl&gt;    &lt;mth&gt;\n## 1 Laptop    771 2025 Feb\n## 2 Laptop    923 2025 Mar\n## 3 Laptop    699 2025 Apr\n## 4 Laptop    670 2025 May\n## 5 Laptop    968 2025 Jun\n## # ‚Ñπ 13 more rows\n\nOur simulated tsibble (sales_data_gaps) is now ready. I visual inspection of the Date column will reveal that there are missing dates (implicit). we can confirm this with the functions we learnt earlier and then decide what to do later.\n\n\n\n\n\n\nNote\n\n\n\nMore information on the seq.Date function in (Chapter 7)\n\n\n\nscan_gaps(sales_data_gaps)\n## # A tsibble: 4 x 2 [1M]\n## # Key:       Product [2]\n##   Product         Date\n##   &lt;chr&gt;          &lt;mth&gt;\n## 1 Laptop      2025 Jul\n## 2 Laptop      2025 Aug\n## 3 Smart Phone 2025 Jun\n## 4 Smart Phone 2025 Jul\n\nThe scan gaps tells us the gaps in our data. Notice how it is only showing only 2 months are missing for Laptop even though we know there are 4 months missing rather. We will see how to handle this very soon.\n\n\n\n\n\n\nImportant\n\n\n\nThe tsibble assumes that our data only starts from Feb and Ends in Nov for the Laptop Product.\n\n\nThe tsibble package makes it really easy to handle gaps in our time series data with the fill_gaps() function. The fill_gaps() makes the implicit gaps explicit by inserting rows for missing time periods and assigning NA to the observation values (the Sales column in our case)\n\n# fill in the missing time points \nsales_data_gaps |&gt; \n  fill_gaps() |&gt; pull(Date)\n## &lt;yearmonth[22]&gt;\n##  [1] \"2025 Feb\" \"2025 Mar\" \"2025 Apr\" \"2025 May\" \"2025 Jun\" \"2025 Jul\"\n##  [7] \"2025 Aug\" \"2025 Sep\" \"2025 Oct\" \"2025 Nov\" \"2025 Jan\" \"2025 Feb\"\n## [13] \"2025 Mar\" \"2025 Apr\" \"2025 May\" \"2025 Jun\" \"2025 Jul\" \"2025 Aug\"\n## [19] \"2025 Sep\" \"2025 Oct\" \"2025 Nov\" \"2025 Dec\"\n\nThe fill_gaps() function filled only the gaps which were identified by the scan_gaps function evidenced by only 22 time points instead of 24.\nSo you might be wondering, how do we deal with the missing Jan and Dec.¬†Well! the fill_gaps() function creates provision for such cases. The function can even help us set a start/ending time that allows us to expand the existing time span (we are not doing that).\nBack to our issue, we can set .full = TRUE inside fill_gaps to fill the time gaps over the entire time span of our data. What it does is, it checks the time span for the Smart Phone‚Äôs series and notices that it starts from Jan and ends in Dec so it applies the same time span to Laptops\n\n# fill gaps over entire time span\nsales_data_filled &lt;- sales_data_gaps |&gt; \n  fill_gaps(.full = TRUE)\n\nAlso, instead of assigning the values of the filled time gaps with NAs we can specify a value of our choosing which is very useful if we want to record zero (0) sales for the missing months instead of missing data.\n\n# fill gaps in Sales column with 0\nsales_data_gaps |&gt; \n  fill_gaps(\n    .full = TRUE,\n    Sales = 0\n  )\n\nNow, all missing time points have a Sales value of 0 instead of NA. This can be crucial for consistent time series modelling or calculations. Sometimes too the data might not have time gaps but rather missing values for a particular time period. We can deal with them by filtering out complete cases (which automatically introduces time gaps) or use models that can handle NAs within a time series data. The later choice would not help much. Since we will not go into imputing missing data my advice is to rely on the fill_gaps() and replacing NAs with zeros.\n\n\n\n\n\n\nTip\n\n\n\nthe imputeTS package provides several robust methods for estimating missing values in a time series data.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nAlways apply domain knowledge and check time series data characteristics before applying a particular imputation method from imputeTS\n\n\nThe next chapter is a bonus one. It briefly describes how to import data (time series data) into R from various sources with a simple code illustration. You can decide to skip it if you already know how to import the data from some common file formats like .csv or .xlsx (excel file) into R.",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Dealing with Time Gaps and Irregularities</span>"
    ]
  },
  {
    "objectID": "import-data.html",
    "href": "import-data.html",
    "title": "6¬† Importing Data and Creating a tsibble (Bonus)",
    "section": "",
    "text": "6.1 Get your Data into R\nThe journey always starts by getting your data into your R environment. The goal is to get a standard tibble/data frame with at least one column representing a date or datetime.",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Importing Data and Creating a tsibble (Bonus)</span>"
    ]
  },
  {
    "objectID": "import-data.html#common-workflows",
    "href": "import-data.html#common-workflows",
    "title": "6¬† Importing Data and Creating a tsibble (Bonus)",
    "section": "\n6.2 Common Workflows",
    "text": "6.2 Common Workflows\n\n\nFrom CSV/XLSX files: Use readr::read_csv() or readxl::read_excel(). Your date column might be a character string initially (e.g.¬†\"2023/01/15\") which you can convert to an actual date later.\n\nFrom Databases: Use dbplyr to query and pull into a tibble (beyond the scope of this book).\n\nFrom APIs: Use httr2 or jasonlite to pull JSON data, then parse it into a tibble (also beyond the scope of this book)",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Importing Data and Creating a tsibble (Bonus)</span>"
    ]
  },
  {
    "objectID": "import-data.html#the-crucial-step-parse-date",
    "href": "import-data.html#the-crucial-step-parse-date",
    "title": "6¬† Importing Data and Creating a tsibble (Bonus)",
    "section": "\n6.3 The Crucial Step: Parse Date",
    "text": "6.3 The Crucial Step: Parse Date\nOnce the data is in a tibble, we must ensure our data column is the correct Date or POSIXct data type This is where lubridate shines.\nFor instance functions like ymd(), mdy(), dmy() converts character strings like ‚Äú2023-01-12‚Äù or ‚Äú15/01/22‚Äù into proper dates. Also as_date() and as_datetime(), coerce numeric timestamps or other objects into dates.\nA simple import and parse pipeline is demonstrated below. the data used can be found here monthly_sales.csv\n\nlibrary(readr)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(dplyr)\n\n# step 1: Import\nsales_data &lt;- read_csv('data/monthly_sales.csv', show_col_types = FALSE)\n\n# step2: Inspect and parse Dates\n# initially the date column might be a character type\nsales_data |&gt; select(Month)\n\n# A tibble: 60 √ó 1\n   Month   \n   &lt;chr&gt;   \n 1 Jan 2015\n 2 Feb 2015\n 3 Mar 2015\n 4 Apr 2015\n 5 May 2015\n 6 Jun 2015\n 7 Jul 2015\n 8 Aug 2015\n 9 Sep 2015\n10 Oct 2015\n# ‚Ñπ 50 more rows\n\n\nThe Month column is a character as expected. We will use lubridate to parse it into a Date type\n\n# Parse into date type\nsales_data &lt;- sales_data |&gt; \n  mutate(Date = my(Month),  # create a date column from the Month column\n         .after = Month)  |&gt; \n  select(-Month)            # remove Month column\n\n\n# step 3: convert to tsibble\nsales_ts &lt;- sales_data |&gt; \n  as_tsibble(index = Date)\n\nThis pipeline ‚Äì Import ‚Üí Parse ‚Üí Convert to Tsibble ‚Äì is the foundation of almost every tidy time series analysis.\nThe next chapter will dive deeply into the lubridate package where we will learn how to handle dates and times in a tsibble gracefully.",
    "crumbs": [
      "Working with Time Series Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Importing Data and Creating a tsibble (Bonus)</span>"
    ]
  },
  {
    "objectID": "dates-and-time.html",
    "href": "dates-and-time.html",
    "title": "Working with Dates and Time",
    "section": "",
    "text": "The Journey Ahead\nCongratulations! You have successfully navigated the first critical steps of your time series journey. You can now import data, forge it into a proper tsibble, and even diagnose temporal gaps in your timeline. You have built a solid foundation, but a foundations is meant to be built upon.\nSo far, we have treated time as a structural element (the index that orders our observations). But in time series analysis, time is more than just an index, it is our most powerful predictor. It holds the hidden patterns, the recurring rhythms and the contextual clues that drive our forecasts. To unlock this predictive power, we must move from simply handling dates to truly understanding and engineering them.\nThis chapter is where we transition from data wranglers to time travellers. We will learn to manipulate the fabric of time itself within our datasets, transforming raw date stamps into rich source of insight.\nIn this section, we will delve into two powerful skill sets",
    "crumbs": [
      "Working with Dates and Time"
    ]
  },
  {
    "objectID": "dates-and-time.html#the-journey-ahead",
    "href": "dates-and-time.html#the-journey-ahead",
    "title": "Working with Dates and Time",
    "section": "",
    "text": "7¬† Using lubridate for Date Manipulation : Your tsibble knows that your data is a time series, but lubridate will give you the power to control how it understands time. In this chapter we go beyond basic date parsing and learn to:\n\neffortlessly extract components from dates (what day of the week was it? is this a quarter end?).\nperform intuitive date arithmetic (What is the date 6 weeks from now? How many days are in the interval)\nhandle time zones and daylight saving time with confidence, avoiding one of the most common pitfalls in time based data\n\n8¬† Creating Useful Time-Based Features : A timestamp like \"2023-11-24 14:30:00\" is a single point in time. But within it lies a universe of potential features. We will learn to decompose this single data point into a suite of powerful predictors in this chapter. We will look at Temporal context, Cyclical Patterns and Seasonal markers\n\n\nThe Alchemy of Feature Engineering - an Illustration\nRaw Timestamp: \"2023-11-23 14:30:00\"\n    |\n    |---[lubridate & feature engineering]---&gt;\n    |\n    V\n\nA Rich Feature Set:\n- year = 2023\n- month = November\n- day_of_week = Thursday\n- is_weekend = FALSE\n- hour_of_day = 14\n- is_business_hours = TRUE\n- season = Dry Season  &lt;-- Critical for agriculture, energy, retail\n- is_public_holiday = FALSE\n- is_festival_period = FALSE\n- is_election_period = FALSE  &lt;-- Important for economic activity\n- days_until_month_end = 16",
    "crumbs": [
      "Working with Dates and Time"
    ]
  },
  {
    "objectID": "dates-and-time.html#what-this-means-for-you",
    "href": "dates-and-time.html#what-this-means-for-you",
    "title": "Working with Dates and Time",
    "section": "What This Means for You",
    "text": "What This Means for You\nMastering dates and times is what separates basic time series plot from a truly diagnostic analysis. It is the difference between knowing that ‚Äúelectricity consumption is high‚Äù and knowing it specifically peaks on dry season weekdays between 7-9pm when air conditioning use is highest. It is the difference between seeing ‚Äúincreased sales‚Äù and understanding they spike during Easter celebrations, Christmas festivities, or a sales discount period within the month.\nBy the end of this section, your date columns will no longer be simple indexes. They will become fertile ground for feature engineering, providing the contextual richness that allows your models to capture real-world behaviour and generate remarkably accurate forecasts.\nLet us begin our journey into the fourth dimension üï∞Ô∏è.",
    "crumbs": [
      "Working with Dates and Time"
    ]
  },
  {
    "objectID": "lubridate.html",
    "href": "lubridate.html",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "",
    "text": "7.1 Understanding Date Formats\nNow that we know how to create a tidy time series data (tsibble) we will dive deeper into manipulating dates and times using the powerful lubridate package (part of the tidyverse package collection). In R this package makes working with dates and times very intuitive. In this chapter you will learn how to:\nWe will explore these features using some in-built datasets from the tsibble package. Make sure all required packages are loaded\nDates come in many formats: ‚Äú2025-01-22‚Äù, ‚Äú22/01/2025‚Äù, ‚ÄúJanuary 22, 2025‚Äù, 22-Jan-2025‚Äù. Without lubridate parsing these can be frustrating.\nlubridate provides intuitive functions that match the order of your date components\n# Different date formats parsed easily\ndate1 &lt;- ymd(\"2023-03-15\")        # Year-Month-Day\ndate2 &lt;- dmy(\"15/03/2023\")        # Day-Month-Year  \ndate3 &lt;- mdy(\"March 15, 2023\")    # Month-Day-Year\ndate4 &lt;- dmy(\"15-Mar-2023\")       # Day-Month-Year with text month\ndate1, date2, date3 and date4 all produces the same results thanks to lubridate‚Äôs intuitive functions ymd, dmy, and mdy.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#working-with-real-data",
    "href": "lubridate.html#working-with-real-data",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.2 Working with Real Data",
    "text": "7.2 Working with Real Data\nWe will use the pedestrian dataset from the tsibble package which contains hourly pedestrian counts from sensors around Melbourne in Australia. We will practice our lubridate skills on this data.\n\nhead(pedestrian)\n## # A tsibble: 6 x 5 [1h] &lt;Australia/Melbourne&gt;\n## # Key:       Sensor [1]\n##   Sensor         Date_Time           Date        Time Count\n##   &lt;chr&gt;          &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n## 1 Birrarung Marr 2015-01-01 00:00:00 2015-01-01     0  1630\n## 2 Birrarung Marr 2015-01-01 01:00:00 2015-01-01     1   826\n## 3 Birrarung Marr 2015-01-01 02:00:00 2015-01-01     2   567\n## 4 Birrarung Marr 2015-01-01 03:00:00 2015-01-01     3   264\n## 5 Birrarung Marr 2015-01-01 04:00:00 2015-01-01     4   139\n## # ‚Ñπ 1 more row\n\nWe see that this data is already a tsibble with a proper Datetime (stored as a POSIXct object) column Date_Time (index). The Sensor column is the key and it contains the various sensor names. The Date represents the date the counts were recorded, the Time is the hour associated with the Date_Time and the Count is the hourly pedestrian count.\nOur main variable of interest is the Date_Time, we will isolate this column and see how to extract various components from these time stamps\n\n\n\n\n\n\nTip\n\n\n\nIn R POSIXct is a class used to represent dates and times. It stores date-time information as the number of seconds that have elapsed since January 1, 1970 at 00:00:00 UTC\n\n\n\n# Isolate Date_Time column from the rest of the data\npedestrain_date_times &lt;- pedestrian |&gt; \n  select(Date_Time)\n\nThis is an hourly timestamp which contains lots of temporal information. We will go ahead and extract components using functions from the lubridate package.\n\n# extract a single timestamp\nsingle_timestamp &lt;- pedestrain_date_times$Date_Time[4]\n\n# extract temporal components\nyear(single_timestamp)               # extract year\nmonth(single_timestamp, \n      label = TRUE, abbr = FALSE)    # extract month\nday(single_timestamp)                # extract day of month\nwday(single_timestamp, label = TRUE) # extract day of week (Mon - Sun)\nhour(single_timestamp)               # extract hour\nminute(single_timestamp)             # extract minute\nsecond(single_timestamp)             # extract second\nquarter(single_timestamp)                  # extract quarter of year\nsemester(single_timestamp, with_year = T)  # extract semester of year \n\nThe year function extracts a four digit year from the single-timestamp variable. The month function extracts the month from the date, by default it extracts a numeric month but specifying label=TRUE changes the numeric month to a named month (e.g.¬†from 2 to February). Again the abbr argument control whether the name of the month should be abbreviated or not. the day and wday functions extracts the day of the month and the day of the week respectively. specifying label=TRUE in wday behaves the same way as in the month function. The hour, minute and second functions extracts the hour of the day (0-23), the minute (0-59) and second (0-59) respectively. The quarter function returns the fiscal quarter of the year (1-4) and the semester function returns either 1 or 2 for each semester of the year. A quarter divides the year into 4 (3 months in each quarter) and the semester divides the year into 2 (6-months in each semester).",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#date-arithmetic",
    "href": "lubridate.html#date-arithmetic",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.3 Date Arithmetic",
    "text": "7.3 Date Arithmetic\nlubridate also contains functions to perform simple arithmetic on dates. This type of manipulation can be vital when preparing your data for time series modelling. we will use the same single timestamp variable to demonstrate this.\n\n# rename to start date\nstart_date &lt;- single_timestamp\n\n# perform various date calculations\nstart_date + days(7)\nstart_date + months(2)\nstart_date + years(1)\ndays_in_month(start_date) - day(start_date)\nwday(start_date)  %in% c(1,7)   # check is day is a weekend (Sat or Sun)\n\nThe first line of code for the arithmetic calculations adds exactly 7 days to the original date. The new date now becomes ‚Äú2015-01-08 03:00:00 AEDT‚Äù. The second line of code adds exactly 2 calendar months. lubridate correctly handles complexities like moving from January 31st to February 28th/29th. The third line adds exactly one calendar year (correctly handling leap years). days_in_month(start_date) - day(start_date) calculates how many days remain in the current month. It subtracts the current day of the month from the total number of days in that month. The last line of code creates a logical flag (TRUE/FALSE) indicating if the start_date is a Saturday (7) or a Sunday (1), based on R‚Äôs default week-starting convention.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#handling-time-zones",
    "href": "lubridate.html#handling-time-zones",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.4 Handling Time Zones",
    "text": "7.4 Handling Time Zones\nHere we learn the critical concept of time zone handling using lubridate. We will use the pedestrian data which contains dates in the \"AEDT\" (Australian Eastern Daylight Time) [The time zone in Melbourne] time zone and convert them into various global time zones. This demonstrates how the same moment in time is represented differently around the world.\nWe will first sample a few timelines from the pedestrian data and the use the with_tz() function to convert these times to several international time zones\n\n# sample time stamps (first 5) from data\ntime_examples &lt;- pedestrain_date_times |&gt; \n  distinct() |&gt; \n  head(5)\nprint(time_examples)\n## # A tibble: 5 √ó 2\n##   Date_Time           Sensor        \n##   &lt;dttm&gt;              &lt;chr&gt;         \n## 1 2015-01-01 00:00:00 Birrarung Marr\n## 2 2015-01-01 01:00:00 Birrarung Marr\n## 3 2015-01-01 02:00:00 Birrarung Marr\n## 4 2015-01-01 03:00:00 Birrarung Marr\n## 5 2015-01-01 04:00:00 Birrarung Marr\n\nWe then go ahead to create new time zones (time zone conversion) from the original time zone as new columns in the data\n\nmore_time_examples &lt;- time_examples |&gt; \n  mutate(\n    utc_time = with_tz(Date_Time, tzone = 'UTC'),\n    london_time = with_tz(Date_Time, tzone = 'Europe/London'),\n    ny_time = with_tz(Date_Time, tzone = 'America/New_York'),\n    ghana_time = with_tz(Date_Time, tzone = 'Africa/Accra'),\n    timezone_offset = tz(Date_Time)\n  )\nmore_time_examples\n## # A tibble: 5 √ó 7\n##   Date_Time           Sensor        utc_time            london_time        \n##   &lt;dttm&gt;              &lt;chr&gt;         &lt;dttm&gt;              &lt;dttm&gt;             \n## 1 2015-01-01 00:00:00 Birrarung Ma‚Ä¶ 2014-12-31 13:00:00 2014-12-31 13:00:00\n## 2 2015-01-01 01:00:00 Birrarung Ma‚Ä¶ 2014-12-31 14:00:00 2014-12-31 14:00:00\n## 3 2015-01-01 02:00:00 Birrarung Ma‚Ä¶ 2014-12-31 15:00:00 2014-12-31 15:00:00\n## 4 2015-01-01 03:00:00 Birrarung Ma‚Ä¶ 2014-12-31 16:00:00 2014-12-31 16:00:00\n## 5 2015-01-01 04:00:00 Birrarung Ma‚Ä¶ 2014-12-31 17:00:00 2014-12-31 17:00:00\n## # ‚Ñπ 3 more variables: ny_time &lt;dttm&gt;, ghana_time &lt;dttm&gt;,\n## #   timezone_offset &lt;chr&gt;\n\nThe with_tz() function facilitates the display of a date-time object (POSIXct) in a different time zone. It is crucial to understand that with_tz() does not alter the actual moment in time but rather changes the representation of that moment in a new time zone. It requires the date-time and the new time zone to do its magic for instance this code ghana_time = with_tz(Date_Time, tzone = 'Africa/Accra' converts the Date_Time object from its original time zone of \"AEDT\" into a new time zone \"Africa/Accra\" (‚ÄúGMT‚Äù).\nIt recalculates the local clock time based on the new time zone while preserving the absolute point in time. This can result in changes to the displayed hour, day, or even date, depending on the time difference between the original and the target time zones as seen above.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the OlsonNames() function to see the various time zone names in R",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#common-date-challenges-and-solutions",
    "href": "lubridate.html#common-date-challenges-and-solutions",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.5 Common Date Challenges and Solutions",
    "text": "7.5 Common Date Challenges and Solutions\nWorking with dates and times in real-world frequently presents two major challenges: Inconsistent formatting (messy strings) and the need to create continuous, regular sequences (to fill gaps). The lubridate package together with dplyr offer elegant solutions to both problems\n\n7.5.1 Handling Multiple Date Formats\nWhen raw data comes from different sources, it may lead to dates stored as string/characters in unpredictable non-standard formats (YYYY-MM-DD). We demonstrate how to use an all purpose function parse_date_time() to handle such irregularities\n\n# simulate messy dates data\nmessy_dates &lt;- tibble(\n  date_string = c(\n  \"2016-01-01 08:00:00\",\n  \"01/01/2016 09:00\",\n  \"January 1, 2016 10:00\",\n  \"01-Jan-2016 11:00\"\n   )\n)\n\n# parse all formats using lubridates flexibility\nclean_dates &lt;- messy_dates |&gt; \n  mutate(\n    final_date = parse_date_time(\n      date_string,\n      c(\"%Y-%m-%d %H:%M:%S\",\n        \"%d/%m/%Y %H:%M\",\n        \"%B %d, %Y %H:%M\", \n        \"%d-%b-%Y %H:%M\"),\n      exact = TRUE\n    )\n  )\nclean_dates\n## # A tibble: 4 √ó 2\n##   date_string           final_date         \n##   &lt;chr&gt;                 &lt;dttm&gt;             \n## 1 2016-01-01 08:00:00   2016-01-01 08:00:00\n## 2 01/01/2016 09:00      2016-01-01 09:00:00\n## 3 January 1, 2016 10:00 2016-01-01 10:00:00\n## 4 01-Jan-2016 11:00     2016-01-01 11:00:00\n\nThis function is designed to specifically handle the scenarios where you have multiple possible date-time formats in a single vector. The parse_date_time) function requires the desired formats to be parsed via the orders argument using either R or C format codes (which begin with a percent sign %) or the more simplified intuitive lubridate non percent format like ( ymd HMS). The percent format date codes have meanings for the various letters;\n\n\n%Y: 4-digit year (e.g 2016)\n\n%m, %b and %B: for months in numeric, abbreviated name and full name respectively\n\n%d: Day of month\n\n%H: Hour\n\n%M: Minute\n\n%S: Second\n\nYou can find more information about these formats in the help documentation on the parse_date_time() function.\n\n\n\n\n\n\nImportant\n\n\n\nBe careful when using the simplified intuitive lubridate format since it might not always parse the date correctly as expected\n\n\n\n7.5.2 Working with Partial Dates and Creating Sequences\nTime series analysis often requires a complete, ordered sequence of time points, even if the raw data is sparse or irregular. We have already seen how to check and repair implicit missing time stamps in our data but what we want to learn here is to generate a series of dates/times between two specified endpoints. To achieve this, we can use seq.Date and seq.POSIXt() to generate these continuous sequences. These two functions are all base R functions that can be very handy sometimes\n\n# create date sequences for analysis\nstart_date &lt;-  ymd('2016-01-01')\nend_date &lt;-  ymd('2016-01-31')\n\n# generate daily sequences\ndaily_dates &lt;- seq.Date(\n  from = start_date,\n  to = end_date,\n  by = \"day\"\n)\n\n# generate hourly sequence for a specific day\nhourly_sequence &lt;- seq.POSIXt(\n  from = ymd_hms(\"2016-01-15 00:00:00\"),\n  to = ymd_hms(\"2016-01-15 23:00:00\"),\n  by = \"hour\"\n)\n\nseq.Date is used for creating sequences of Date objects (without time components). It requires a from (start date), a to (end date), and a by (interval e.g.¬†‚Äúday‚Äù, ‚Äúmonth‚Äù, ‚Äúyear‚Äù). The output is a sequence of simple dates. seq.POSIXT() on the other hand is used to for Date-time (POSIXt) sequences. It requires a full start and end time and a time based interval (e.g.¬†‚Äúhour‚Äù, ‚Äú5 min‚Äù, ‚Äúsec‚Äù). This is essential for high-frequency data.\nThese temporal sequence functions are the backbone of may time series operations, allowing you to create a time index that can be used to join sparse data, visualise gaps, or ensure a continuous flow of data points for modelling.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#periods-vs-durations",
    "href": "lubridate.html#periods-vs-durations",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.6 Periods vs Durations",
    "text": "7.6 Periods vs Durations\nThis sections deals with a sophisticated yet crucial concept in time series and date handling: the difference between a Period and a Duration in the lubridate package. This distinction is vital because adding a set amount of time (like ‚Äúone month) does not always equal the same number of seconds or days, due to factors like month length, leap years, and Daylight Saving Time (DST).\n\n7.6.1 The Core Difference\n\n\n\n\n\n\n\n\nConcept\nFunction type\nDefinition\nImpact on Time\n\n\n\nPeriod\nyears(), months(), days(), hours()\nRepresents a human-defined calendar-aware unit of time\nThe length in seconds is variable. Adding one month from January !st means landing exactly on February 1st\n\n\nDuration\ndmonths(), dyears(), ddays(), dseconds(), dminutes()\nRepresents an exact, absolute span of time, measured in seconds\nThe length in seconds is fixed. Adding 1 month duration is always 2,629,800 seconds (the average month length)\n\n\n\nThe code below demonstrates this sophistication\n\n# understanding the difference between periods and duration\ntime_concepts &lt;- tibble(\n  description = 'start of analysis period',\n  start_time = ymd_hms(\"2016-01-01 00:00:00\"),\n) |&gt; \n  mutate(\n    # periods respect calendar time \n    add_1_month_period = start_time + months(1),\n    \n    # duration respect exact time intervals (1month is always 30 days)\n    add_1_month_duration = start_time + dmonths(1),\n    \n    # difference between the two\n    difference = add_1_month_period - add_1_month_duration\n  )\n\ntime_concepts |&gt; select(\n  start_time,\n  add_1_month_period,\n  add_1_month_duration,\n  difference)\n## # A tibble: 1 √ó 4\n##   start_time          add_1_month_period  add_1_month_duration difference\n##   &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;               &lt;drtn&gt;    \n## 1 2016-01-01 00:00:00 2016-02-01 00:00:00 2016-01-31 10:30:00  13.5 hours\n\nThe period function months() respects the calendar and it moves the date one calendar month forward regardless of whether January has 31 days. The duration function dmonths() uses a fixed conversion; dmonths(1) is the equivalent of 30.4375 days in duration logic. It is the average length of a month in a 12year calender \\((\\frac{365.25days}{12months})\\). The difference between the duration month and the period month is recorded as 13.5hrs. (under the difference column) not 24hrs. (1day), this is because the two calculations used fundamentally different values for 1 month and that dmonth(1) ‚â† 30days.\nThis is how the difference is calculated\n\\[\nDifference = \\frac{(31days - 30.4375days)\\times24hours}{1day}=13.5hours\n\\]\nUsing the wrong concept can lead to significant errors in analysis and forecast. For instance if you want to forecast the sales of the next calendar months you should use Periods (e.g.¬†start_date + months(1)) to ensure the forecast window aligns with calendar cycles. If you want to compare today‚Äôs temperature to the temperature exactly 72hours ago, you must use Durations (e.g.¬†now() - ddays(3)) to get a fixed time difference independent of DST changes. A period of hours(72) might skip an hour of DST changes leading to an incorrect comparison.\nPeriods handle the complexities of DST and leap years transparently, preserving the clock time. Durations simply add or subtract the exact number of seconds.\n\n\n\n\n\n\nNote\n\n\n\nThe dataset time_concepts is created as a tibble intentionally just for demonstration purposes",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "lubridate.html#best-practices-and-next-steps",
    "href": "lubridate.html#best-practices-and-next-steps",
    "title": "7¬† Using lubridate for Date Manipulation",
    "section": "\n7.7 Best Practices and Next Steps",
    "text": "7.7 Best Practices and Next Steps\nWhen dealing with dates and times in a times series you must always;\n\nParse dates early in your analysis workflow\nUse consistent date formats throughout the project\nBe mindful of time zones when working with international dates\nValidate your dates by checking for impossible dates and time gaps\nUse the appropriate data types - Date for dates and POSIXct for date-times\n\nNow that you are comfortable with lubridate, you are ready to create powerful time-based features for your forecasting models. In the next section, we will learn how to transform these date components into meaningful predictors that can dramatically improve your time series forecasts.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Using `lubridate` for Date Manipulation</span>"
    ]
  },
  {
    "objectID": "new-time-variables.html",
    "href": "new-time-variables.html",
    "title": "8¬† Creating Useful Time-Based Features",
    "section": "",
    "text": "8.1 Basic Time Based Features\nNow that you have mastered lubridate for date manipulation, it is time to unlock the true power of time series analysis; feature engineering. Think of your date column as a treasure chest filled with valuable information. Feature engineering is the process of opening that chest and extracting the gems hidden within.\nIn this chapter, you will learn how to transform simple dates into powerful predictors that can dramatically improve your forecasting models. We will work with our sales_data and gh_ts data sets we used in Chapter 6 and Chapter 5 respectively to create features that capture seasonal patterns, economic cycles and long-term trends relevant to the Ghanaian context.\nFor this demonstration , we will only the sales_data which contains monthly sales from 2015 to 2019.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Creating Useful Time-Based Features</span>"
    ]
  },
  {
    "objectID": "new-time-variables.html#basic-time-based-features",
    "href": "new-time-variables.html#basic-time-based-features",
    "title": "8¬† Creating Useful Time-Based Features",
    "section": "",
    "text": "8.1.1 Calendar-Based Features\nThese are the most straightforward features derived directly from the date component. Here we use the date itself (The Month column) to categorise and enrich the data. This can be crucial for capturing predictable cyclic patterns that repeat yearly, quarterly, or monthly.\n\n# Extract basic calender features\nsales_features_basic &lt;- sales_data |&gt; \n  mutate(\n    # Basic date components\n    Year = year(Month),\n    Month_only = month(Month, label = TRUE),\n    Quarter = quarter(Month),\n    \n    # Ghana-specific seasonal features\n    Season = case_when(\n      Month_only %in% month.abb[4:10] ~ 'Rainy Season',\n      Month_only %in% month.abb[c(1,2,12)] ~ \"Dry Season\",\n      TRUE ~ \"Transition\"\n    ),\n    # Holiday/Business cycles\n    is_holiday_season = Month_only %in% month.abb[c(3,12)],  # Most Holiday months  \n    is_back_to_school = Month_only %in% month.abb[9:10],  # Typical start of academic year\n    \n    # economic calender features\n    is_year_start = Month_only == 'Jan',\n    is_year_end = Month_only == 'Dec',\n    is_mid_year = Month_only == 'Jun'\n  )\n\nThe basic date components captures the level of aggregation. Models use these to find annual trends or quarterly seasonality. The Ghana-specific seasonal features, creates a seasonal flag that may influence sales (like less travels during heavy rain). The holidays/business cycles, and economic calender features creates a logical (TRUE/FALSE) identifying major holiday periods, periods when schools reopens and end of period effects such as budget finalisation or semi annual targets that influence sales activities.\nThis whole process transforms a simple Date column into many powerful categorical and numerical features, which a predictive model can easily interpret. This can significantly improve the models ability to forecast.\n\n8.1.2 Creating Lag Features\nLag features are arguably the most important feature type for time series analysis. A lag feature is the value of the series at a previous time step. They capture temporal dependence (autocorrelation) ‚Äì the idea that a value today is highly correlated with its value yesterday, last week or last year.\nFor this demonstration we will use the gh_ts dataset focusing on some key indicators from the data (Annual GDP growth, Total Population, GDP per-capita in USD and Cereal yield per hectare).\n\ngh_ts_filtered &lt;- gh_ts |&gt; \n  filter(\n    indicator_name %in% c(\"Annual GDP growth rate\", \"Population_total\",\n                          \"GDP per capita usd\",\"Cereal yield _kg per hectare\")\n  )\n\n\n# Create lag and difference \ngh_ts_lags &lt;- gh_ts_filtered |&gt; \n  group_by(indicator_name) |&gt; \n  mutate(\n    # Lag features (previous values)\n    value_lag1 = lag(value, 1),\n    \n    # Difference features (changes over time)\n    value_diff = value - value_lag1,\n    value_pct_change = (value - value_lag1) / value_lag1 * 100\n  )\n\nThe lag features are calculated using the lag() function from the dplyr package. The function takes a vector of values (usually a column from a dataframe) and the number of positions to lag. In the code above we shift the values for each indicator by 1 year (value_lag1), calculate the difference between the original values and the lagged values (value_diff) and then further calculate the relative year-over-year change (value_pct_change). The differencing helps to remove the level (baseline value) and trend (long term direction) which is a key step in achieving stationarity when fitting an ARIMA model.\n\n\n\n\n\n\nImportant\n\n\n\nWhen fitting an ARIMA model (with the fable package) you do not need to manually calculate the lag and differencing. The model handles this automatically.\n\n\nBy engineering these calendar and lag features for fitting a particular model, the model gains profound understanding of the data‚Äôs historical movement, leading to significantly more accurate forecasts.",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Creating Useful Time-Based Features</span>"
    ]
  },
  {
    "objectID": "new-time-variables.html#feature-selection-and-analysis",
    "href": "new-time-variables.html#feature-selection-and-analysis",
    "title": "8¬† Creating Useful Time-Based Features",
    "section": "\n8.2 Feature Selection and Analysis",
    "text": "8.2 Feature Selection and Analysis\nAfter creating numerous features, the next step is to determine which ones are genuinely useful predictors and should be included in the final model.\n\n8.2.1 Correlation Analysis\nCorrelation measures the linear relationship between two variables. To check feature importance using correlation, we are essentially looking at how strongly each feature is linearly related to the target variable. We can demonstrate this with the gh_ts_lags data we just created.\nWe will add more features to the data and check how they all relate to the target variable (value). The new features will include another lagged value that shifts the value 2 time steps back, A binary feature (TRUE/FALSE) for election years since 1960.\n\n# create political and economic cycle features \ngh_contextual &lt;- gh_ts_lags |&gt; \n  group_by(indicator_name) |&gt; \n  mutate(\n    # add 2nd lag feature\n    value_lag2 = lag(value, 2),\n    \n    # election cycles\n    election_year = year %in% c(1960, 1969, 1979, seq(1992, 2024, by=4))\n    ) |&gt; \n  ungroup()\n\nWe can now go ahead and check the importance of all these features using their correlation with the original value (value column). The data we have has 4 keys, meaning there are 4 separate time series‚Äô within the data. We are going to focus only on the values for the ‚ÄúAnnual GDP growth rate‚Äù series.\n\n# correlation analysis of features with GDP growth\ngdp_correlations &lt;- gh_contextual |&gt; \n  filter(indicator_name == \"Annual GDP growth rate\") |&gt; \n  as_tibble() |&gt; \n  select(value, value_lag1, \n         value_diff, value_pct_change, \n         value_lag2, election_year) |&gt; \n  mutate(\n    # recode election_year \n    election_year = case_match(\n      election_year,\n      TRUE ~ 1,\n      FALSE ~ 0\n    )\n  ) |&gt; \n  cor(use = \"complete.obs\")\n\nThe code above calculates the correlation matrix for the selected variables using the cor() function. Since the cor() function only works on numeric vectors, the election_year had to be converted to numeric binary values (0 and 1). The table below shows the correlation of the selected variables with the actual value.The features with high correlation (positive or negative) are strong candidates for inclusion in the final forecasting model.\n\ncorrelation coefficients of features vs actual value (value)\n\n\nvalue\n\n\n\nvalue_lag1\n0.372\n\n\nvalue_diff\n0.561\n\n\nvalue_pct_change\n0.349\n\n\nvalue_lag2\n0.125\n\n\nelection_year\n0.085\n\n\n\n8.2.2 Visualising Feature Relationships\nCreating visuals of your data is the best way to confirm the relationship derived from the correlation analysis and see how the engineered featured relate to the underlying data.\n\ngh_contextual |&gt; \n  filter(\n    indicator_name == \"Annual GDP growth rate\"\n  ) |&gt; ggplot(aes(x = date)) +\n  geom_line(aes(y= value, colour = 'Actual GDP'), size = 0.7) +\n  geom_line(aes(y = value_lag1, colour = '1 Time Step Back'), size = 0.7) +\n  geom_point(aes(y = if_else(election_year, value, NA), colour = \"Election Year\"), size = 2.5) +\n  scale_colour_brewer(type = 'qual', palette = 'Set1') +\n  labs(title = 'Ghana GDP Growth with Engineered Features',\n       subtitle = 'Showing lagged values and election year markers',\n       y = 'GDP Growth Rate (%)',\n       x = 'Date',\n       colour = 'Series')\n\n\n\n\n\n\nFigure¬†8.1\n\n\n\n\nThe above plot displays the actual GDP growth line and the lagged (1 time step back) GDP growth line visually confirming their relationship. There is also a large green dot over every data point that corresponds to an election_year. This allows you to check if GDP peaks or spikes tends to coincide with these politically charged years.\nYour dates are now transformed into a rich set of features that can significantly improve your forecasting model(s). In the next section we will learn how to perform exploratory analysis on our time series data and delve deeper into visualisations to uncover trends and patterns before building our predictive models.\n\n\n\n\n\n\nTip\n\n\n\nThe feasts package makes performing some of these feature engineering very simple and straightforward. We will explore the feasts package in the next chapter",
    "crumbs": [
      "Working with Dates and Time",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Creating Useful Time-Based Features</span>"
    ]
  },
  {
    "objectID": "exploratory-analysis.html",
    "href": "exploratory-analysis.html",
    "title": "Exploratory Time Series Analysis",
    "section": "",
    "text": "Why Exploratory Analysis Matters\nAfter mastering the date manipulation and feature engineering, you now have a well-structured dataset with rich temporal features. But raw data, no matter how well prepared, rarely tells its story openly.\nExploratory analysis is the process of asking your data the right questions and listening carefully to the answers through visualisations and statistical summaries. Think of yourself as a data detective, where each plot and visualisation is a clue that reveals patterns, trends and anomalies in your time series.\nPerforming exploratory analysis on your time series data is essential and very necessary for the following reasons:",
    "crumbs": [
      "Exploratory Time Series Analysis"
    ]
  },
  {
    "objectID": "exploratory-analysis.html#why-exploratory-analysis-matters",
    "href": "exploratory-analysis.html#why-exploratory-analysis-matters",
    "title": "Exploratory Time Series Analysis",
    "section": "",
    "text": "Pattern Discovery: Uncover seasonal patterns, trends, and cycles that drive your time series\nAnomaly Detection: Identify outliers and unusual events that might distort your model\nModel Selection: Understand the type of forecasting method that might work best for your data\nBusiness Insights: For financial time series data you can transform raw numbers into actionable intelligence\nData Quality: Catch any data issues before they compromise your analysis",
    "crumbs": [
      "Exploratory Time Series Analysis"
    ]
  },
  {
    "objectID": "exploratory-analysis.html#the-journey-ahead",
    "href": "exploratory-analysis.html#the-journey-ahead",
    "title": "Exploratory Time Series Analysis",
    "section": "The Journey Ahead",
    "text": "The Journey Ahead\nThis section will explore two powerful approaches to time series exploration; the flexibility of ggplot2 for custom visualisations and the specialised tools of the feasts package designed specifically for time series analysis.\n\n9¬† Visualising Trends with ggplot2 and autoplot(): In this chapter, we will master the art of time series visualisation sing both the versatile ggplot2 package and the specialised feasts::autoplot() function. You will learn how to\n\ncreate compelling time series plots that reveal trends and patterns\nuse autoplot() for quick, intelligent visualisations of tsibble objects\ncustomise plots to highlight specific features and insights\ncreate publication-quality visualisations that tell clear stories\n\n10¬† Exploring Patterns with feasts and fabletools: Here we will dive deep into the feasts package, which provides specialised tools for time series exploration together with fabletool. You will discover how to\n\nDecompose time series into trend, seasonal and remainder components\nAnalyse autocorrelation patterns to understand temporal dependencies\nIdentify seasonal patterns and their strength.\nUse summary statistics to characterise time series behaviour\ncreate comprehensive diagnostic plots for model selection.\n\n\nTime series data has special characteristics that require special exploration techniques, unlike cross-sectional data, time series observations are connected through time and often contains several layers (trend, cycles, seasonality, noise). For instance yesterday‚Äôs sales influence today‚Äôs sales and last year‚Äôs seasonal pattern likely repeats this year. Again the same time series can look dramatically different depending on whether you view it daily, weekly, monthly or annually.",
    "crumbs": [
      "Exploratory Time Series Analysis"
    ]
  },
  {
    "objectID": "exploratory-analysis.html#what-this-means-for-you",
    "href": "exploratory-analysis.html#what-this-means-for-you",
    "title": "Exploratory Time Series Analysis",
    "section": "What This Means for You",
    "text": "What This Means for You\nThe goal of exploratory analysis is not just to create pretty plots ‚Äì it is to develop a deep understanding of your data that will guide your modelling decisions and business recommendations.\nBy the end of this section you will be able to confidently visualise any time series dataset and quickly diagnose the components and patterns. This will help you make informed decisions about which forecasting approaches to use. You will be able to detect data quality issues and communicate insights effectively through compelling visuals.",
    "crumbs": [
      "Exploratory Time Series Analysis"
    ]
  },
  {
    "objectID": "visualising-trends.html",
    "href": "visualising-trends.html",
    "title": "9¬† Visualising Trends with ggplot2 and autoplot()",
    "section": "",
    "text": "10 Visualising Trends with ggplot2 and autoplot()\nThis is where we transform the raw data (tabular data) into compelling stories that reveal patterns, trends and insights hidden in your data. Since we now have control over date manipulations and feature engineering from the previous chapters, visualisation is the next crucial step.\nA well crafted plot can instantly reveal what might take hours to discover through numerical analysis alone. In this chapter we will learn how to visualise our time series data using the flexible, customisable world of ggplot2 and the specialised, intelligent autoplot() function from the feasts package.\nThis is where we transform the raw data (tabular data) into compelling stories that reveal patterns, trends and insights hidden in your data. Since we now have control over date manipulations and feature engineering from the previous chapters, visualisation is the next crucial step.\nA well crafted plot can instantly reveal what might take hours to discover through numerical analysis alone. In this chapter we will learn how to visualise our time series data using the flexible, customisable world of ggplot2 and the specialised, intelligent autoplot() function from the feasts package.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualising Trends with `ggplot2` and `autoplot()`</span>"
    ]
  },
  {
    "objectID": "visualising-trends.html#basic-time-series-plots-with-ggplot2",
    "href": "visualising-trends.html#basic-time-series-plots-with-ggplot2",
    "title": "9¬† Visualising Trends with ggplot2 and autoplot()",
    "section": "\n10.1 Basic Time Series Plots with ggplot2\n",
    "text": "10.1 Basic Time Series Plots with ggplot2\n\n\n10.1.1 Line Plots for Time Series\nWe will start with the most fundamental time series visualisation, the line plot. It is perfect for showing how values change over time\nTo demonstrate this we will use the gh_ts tsibble data and filter some few key indicators (Population_total, Life expectancy at birth, infant mortality_per_1000_births and Cereal yield _kg per hectare).\n\n# select key variables for tsibble for exploration \ngh_key &lt;- gh_ts |&gt;    \n  filter(     \n    indicator_name  %in% c(       \n      \"Population_total\",       \n      \"Life expectancy at birth\",       \n      \"Infant mortality_per_1000_births\",       \n      \"Cereal yield _kg per hectare\"     \n      )   \n    )\n\nAfter filtering our data into a manageable subset (gh_key), we can now go ahead and create individual line plots for each of the four indicators filtered on. We will use the same basic ggplot2 syntax to produce these plots where we define aesthetics (aes), specify the geometry (geom_line for a line plot) and add labels (labs).\n\n# basic line plot for population total \npop_plot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Population_total\") |&gt;    \n  ggplot(aes(x = date, y = value)) +   \n  geom_line(linewidth = 0.7) +   \n  labs(     \n    title = \"Ghana Annual Total Population (1960-2024)\",     \n    x = 'Year',     \n    y = \"Total Population\"   )  \npop_plot\n\n\n\n\n\n\nFigure¬†10.1: Time Series Plot of Ghana Annual Total Population from 1960 to 2024\n\n\n\n\n\n# basic line plot for life expectancy at birth \nlifexp_plot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Life expectancy at birth\") |&gt;    \n  ggplot(aes(x = date, y = value)) +   \n  geom_line(linewidth = 0.7) +   \n  labs(     \n    title = \"Ghana Yearly Life Expectancy at Birth (1960-2024)\",     \n    x = 'Year',     \n    y = \"Life Expectancy at Birth\"   )  \nlifexp_plot\n\n\n\n\n\n\nFigure¬†10.2: Time Series Plot of Ghana Yearly Life Expectancy at Birth from 1960 to 2024\n\n\n\n\n\n# basic line plot for infant mortality \nInfmort_plot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Infant mortality_per_1000_births\") |&gt;    \n  ggplot(aes(x = date, y = value)) +   \n  geom_line(linewidth = 0.7) +   \n  labs(     \n    title = \"Ghana Annual Infant Mortality (1960-2024)\",     \n    x = 'Year',     \n    y = \"Infant Mortality/1000 births\"   )  \nInfmort_plot\n\n\n\n\n\n\nFigure¬†10.3: Time Series Plot of Ghana Yearly Infant Mortality from 1960 to 2024\n\n\n\n\n\n# basic line plot for cereal yield \ncyield_plot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Cereal yield _kg per hectare\") |&gt;    \n  ggplot(aes(x = date, y = value)) +   \n  geom_line(linewidth = 0.7) +   \n  labs(     \n    title = \"Ghana Yearly Cereal Yield (1960-2024)\",     \n    x = 'Year',     \n    y = \"Cereal Yield/Hectare\")  \ncyield_plot\n\n\n\n\n\n\nFigure¬†10.4: Time Series Plot of Ghana Annual Cereal Yield from 1960 to 2024\n\n\n\n\nEach of the above code block generates a standard line plot for one of the selected indicators. this line of code ggplot(aes(x = date, y = value)) sets up the aesthetic mapping with time (date) on the x-axis and the values (value) on the y-axis. geom_line() creates the line connecting the data points and the linewidth = 0.7 sets the thickness of the line produced. The labs() function makes it possible to add informative titles and labels to the plots which you can specify as a string of texts (characters).\nWe can clearly see the dominant time series characteristic of these plots which is a strong trend. The annual total population plot shows a steep, continuous upward trend which signifies an accelerated population growth over the entire 64 year period. The life expectancy plot also shows a similar strong, persistent positive trend, illustrating how life expectancy has increased from around 45yrs between 1960 and 1970 to over 65yrs in 2024. This sharp increase can be attributed to improved medical care, public health and nutrition over the decades.\nThe plot for annual infant mortality has a continuous negative trend, indicating how infant mortality has fallen dramatically, dropping from over 120 per 1000 births in 1960 to under 30 in 2024. This reinforces the conclusion of improving health and sanitation from the life expectancy plot. The yearly cereal yield over the 64-year period also shows a generally positive long-term trend, but with significant fluctuations (spikes and dips) compared to the other plots. A pronounced upward acceleration is visible after the mid 1980s and especially after 2000. The sharp rise after 1980 likely reflects the intervention of the IMF (international monetary fund) and adoption of the Economic Recovery Program (ERP) in 1983 which significantly boosted the countries economy and reduced poverty levels.\n\n10.1.2 Introducing autoplot()\n\nWhiles ggplot gives you control, autoplot() from the feasts package provides intelligent defaults specifically designed for time series data. We can plot the same cereal yield time series in Figure¬†10.4 with the autoplt() function using the code below\n\n# simple autoplot for cereal yield\ncyield_autoplot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Cereal yield _kg per hectare\") |&gt;\n  autoplot(value, linewidth = 0.7) + \n  labs(     \n    title = \"Ghana Yearly Cereal Yield (1960-2024)\",     \n    x = 'Year',     \n    y = \"Cereal Yield/Hectare\") \ncyield_autoplot\n\n\n\n\n\n\nFigure¬†10.5: Time Series Plot of Ghana Annual Cereal Yield from 1960 to 2024 using the autoplot function\n\n\n\n\nThe autoplot() function is quick and saves time with aesthetic mappings and geometry specifications when using traditional ggplot2 layers. Although it actually is a specialised ggplot2 function, it automatically respects the index (time) and key (series) of the time series data (tsibble). It chooses appropriate geometry, formats date axes appropriately and applies sensible styling. All other ggplot2 graphical layers can be added to the autoplot() like we do in traditional ggplot2 graphs.\n\n10.1.3 Enhancing Basic Plots\nLet us make our cyield_plot more informative by adding some additional features and improving the styling.\n\n# basic line plot for cereal yield \nenhanced_cyield_plot &lt;- gh_key |&gt;    \n  filter(indicator_name == \"Cereal yield _kg per hectare\") |&gt;    \n  ggplot(aes(x = date, y = value)) +   \n  geom_line(linewidth = 0.7) +  \n  geom_point(aes(colour = value &gt;= 1000), size = 1.5) +\n  scale_colour_manual(\n     na.translate = FALSE,\n    values = c(\"TRUE\" = 'darkgreen',\"FALSE\" = 'red',\"NA\" = NULL),\n    labels = c(\"FALSE\" = 'False', \"TRUE\" = 'True',\"NA\" = NULL),\n    name = \"High Yield\"\n  ) +\n  geom_vline(xintercept = ymd(\"1983-04-20\"),\n             colour = 'navy',\n             linetype = 'dashed',\n             linewidth = 0.65) +\n  labs(     \n    title = \"Ghana Yearly Cereal Yield (1960-2024)\", \n    subtitle = \"Vertical line indicates Year of Adopting ERP Policy\",\n    x = 'Year',     \n    y = \"Cereal Yield per Hectare\"\n  )  +\n  geom_smooth(method = 'loess', se=FALSE) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\nenhanced_cyield_plot\n\n\n\n\n\n\nFigure¬†10.6: Time Series Plot of Ghana Annual Total Cereal Yield with Additional Features to Enhance Visual Storytelling\n\n\n\n\nThe plot above tells a detailed story about the Cereal Yield in Ghana from 1960 to 2024. Here we explicitly combine the time series data (gh_key ) with two very interesting engineered features ‚Äì a threshold flag (red and green points) and a policy intervention marker (dashed vertical line) with a smoothed trend line (blue curved line). we use the ggplot2 package to build this detailed visualisation layer by layer.\nWe start the whole process by filtering out the data to only include cereal yield _kg per hectare just like we did for Figure¬†10.4 we then initialise our plot, mapping the date and value columns to the x and y-axis respectively. geom_line(...) draws the the raw time series data. Inside the geom_point() function we create a threshold flag (High Yield) where we add points to the time series line and the colour is determined by the logical expression value &gt;= 1000. If the yield is ‚â• 1000, the colour is mapped to TRUE and if the yield is ÀÇ 1000, the colour is mapped to FALSE.\nWe use scale_colour_manual(...) to explicitly assign TRUE and FALSE logical values to specific colours and label names where TRUE (High Yield) is mapped to dark green and FALSE (Low Yield) is mapped to red. name = \"High Yield\" sets the title of the legend to ‚ÄúHigh Yield‚Äù. The yield value filtered from the data contains some data so our logical expression will automatically return NA‚Äôs together with the logical flag of TRUE and FALSE. By default the NA‚Äôs a labelled as part of the legend so we need to explicitly omit them with na.translate = FALSE and then map its colour to NULL.\nThe next feature that enhances the plots visual appeal is a vertical line that marks the point of policy change, a crucial contextual feature. geom_vline() helps us to achieve this feat. It draws a vertical line fixed at a particular point on the x-axis. This fixed x-intercept is set at the exact date of the intervention corresponding to the adoption of Ghana‚Äôs Economic Recovery Program (ERP) using xintercept = ymd(\"1983-04-20\").\nThe final feature is a smoothed trend line which is drawn using the LOESS (Locally estimated Scatter-plot Smoothing) method with the help of geom_smooth(). This method is an excellent tool for creating flexible non-linear trend lines. se = FALSE suppresses the standard error band around the smooth line, keeping the plot clean. theme(...) is used to customise all aspects of the plot but for this we only use it to centre the title and subtitle.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualising Trends with `ggplot2` and `autoplot()`</span>"
    ]
  },
  {
    "objectID": "visualising-trends.html#multiple-time-series-plots-for-comparison",
    "href": "visualising-trends.html#multiple-time-series-plots-for-comparison",
    "title": "9¬† Visualising Trends with ggplot2 and autoplot()",
    "section": "\n10.2 Multiple Time Series Plots for Comparison",
    "text": "10.2 Multiple Time Series Plots for Comparison\n\n10.2.1 Multiple Series with autoplot()\n\nThe autoplot() function truly shines when working with multiple time series data. However, it only works best when the series has the same value range since they are all plotted on a single graph.\n\ngh_key |&gt; autoplot(value, linewidth = 0.7)\n\ngh_key |&gt; filter(\n  indicator_name %in% c(\"Infant mortality_per_1000_births\", \"Life expectancy at birth\")\n) |&gt; autoplot(value, linewidth = 0.7)\n\n\n\n\n\n\nFigure¬†10.7: _ ‚ÄúMultiple series with autoplot (different value scales)‚Äù _ ‚ÄúMultiple series with autoplot (resonable range value scale)‚Äù\n\n\n\n\n\n\n\n\n\nFigure¬†10.8: _ ‚ÄúMultiple series with autoplot (different value scales)‚Äù _ ‚ÄúMultiple series with autoplot (resonable range value scale)‚Äù\n\n\n\n\nNotice how the plot in Figure¬†10.7 shows all the keys for all the series in the legend but the plot has only two lines representing population_total and life expectancy at birth. This happens because all the 4 series keys have varying range of values that would not fit perfectly in a single plot with a fixed scale. For example Population_total (in tens of millions) alongside life expectancy at birth (in hundreds) on the same y-axis renders the smaller series nearly flat and invisible.\nThe plot in Figure¬†10.8 however is able to display all the two series keys in the legend because their value ranges fall within the y-axis scale of values although their appearance do not truly reflect their true characteristic if they were visualised individually as seen in Figure¬†10.3 and Figure¬†10.2.\n\n10.2.2 Faceted Plots\nTo overcome the hurdle of inconsistent series value ranges on a single axis, we can facet our plots into separate panels using facet_wrap() from ggplot2 together with the autoplot() function. This allows each series to use its own optimised y-axis scale, enabling clear visual comparison of trends and patterns.\n\ngh_key |&gt; \n  autoplot(value, linewidth = 0.7) +\n  facet_wrap(vars(indicator_name), scales = 'free_y') +\n  labs(\n    title = \"Multiple Indicators in Ghana\",\n    subtitle = \"Faceted by Variable\",\n    x = 'Year'\n  ) +\n  # remove legend from plot\n  theme(legend.position = 'none')\n\n\n\n\n\n\nFigure¬†10.9: Faceted Time Series Plot of All Keys within the gh_key tsibble Dataset\n\n\n\n\nThe code output displays or four plots ‚Äì each in its own panel. The core function in the above code is facet_wrap(vars(indicator_name),...). It tells ggplot2 to create a new, separate panel for every unique value found in the indicator_name column. The next critical aspect is scales = 'free_y', which instructs the facet_wrap() function to allow the y-axis to vary independently in each panel. Without this all panels would share the same scale, defeating the purpose of faceting the series. We finally remove the legend using theme(legend.position = 'none') since the series labels become redundant because they are given by the facet titles.\nFaceted plots are the standard, most effective way to visualise multiple disparate time series data streams.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualising Trends with `ggplot2` and `autoplot()`</span>"
    ]
  },
  {
    "objectID": "visualising-trends.html#visualising-monthly-data",
    "href": "visualising-trends.html#visualising-monthly-data",
    "title": "9¬† Visualising Trends with ggplot2 and autoplot()",
    "section": "\n10.3 Visualising Monthly Data",
    "text": "10.3 Visualising Monthly Data\nSo far we have only seen the series plot of yearly data from the gh_ts data set. While the yearly visuals is a good way of identifying long term trends it mostly lacks seasonality. On the other hand a monthly dataset has monthly frequency, which opens up possibilities for detecting cyclical seasonal patterns\n\n10.3.1 Seasonal Patterns and Trends\nTo demonstrate seasonal patterns which is mostly common in monthly datasets (repeating every 12 months) we will use the sales_ts tsibble.\n\n# Basic monthly time series plot\nsales_ts |&gt; \n  autoplot(Sales, linewidth = 0.7) +\n  labs(\n    title = \"Monthly Sales (2015-2019)\",\n    y = \"Sales (‚Çµ)\",\n    x = \"Date\"\n  )\n\nsales_ts |&gt; \n  autoplot(Units_Sold, linewidth = 0.7) +\n  labs(\n    title = \"Monthly Items Sold (2015-2019)\",\n    y = \"Items\",\n    x = \"Date\"\n  )\n\n\n\n\n\n\nFigure¬†10.10: A time Series Plot Showing Monthly Frequency of Sales from 2015 to 2019\n\n\n\n\n\n\n\n\n\nFigure¬†10.11: Another Similar Time Series Plot From the Same Data But Showing Monthly Items Sold from 2019 to 2015\n\n\n\n\nThe two plots show some similar patterns, which is expected since sales is directly related to items sold.\nThe monthly sales series exhibits repeating seasonal peaks and troughs within each year and the pattern appears to involve some distinct high months (peaks) and low months (valleys) annually. This is the annual seasonality that most monthly datasets reveal. We can equally visualise the seasonal plot using the gg_season() function from the ggtime package. It overcomes the limitations of a simple line plot for monthly data by separating the data by year and laying the seasonal patterns on top of one another\n\nlibrary(ggtime)\nsales_ts |&gt; \n  gg_season(Sales, period = \"1y\", linewidth = 0.7) +\n  labs(\n    title = \"Seasonal Plot: Monthly Sales by Year\"\n  )\n  \n\n\n\n\n\n\nFigure¬†10.12: A Time Series Seasonal Sub-series Plot of Monthly Sales for 5 years\n\n\n\n\nIn terms of trend, there is no clear strong long term trend (upward or downward) across the 5-year period. for the monthly series plot in Figure¬†10.10 and Figure¬†10.11. The overall level of sales appears stationary fluctuating between GH‚Çµ10,000 and GH‚Çµ50,000. The seasonal plot however shows some declining trend in the 2016 season (olive green line) and a positive trend in the 2019 season (pink line).\nThe Sold items plot mirrors the Sales plot, showing the same seasonal pattern with minimal long-term trend. These plots confirm that any forecasting model for this data must account for the seasonal component.\nIn the next chapter we will delve into a more detailed visualisation of seasonal patterns ans trends.\n\n\n\n\n\n\nImportant\n\n\n\nThe gg_season() function is part of the feasts package but has been deprecated and moved to the new ggtime package which was basically designed for visualising time series patterns. It contains functions for exploring time series patterns like trends, seasonality, cycles and holidays",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualising Trends with `ggplot2` and `autoplot()`</span>"
    ]
  },
  {
    "objectID": "visualising-trends.html#summary-and-next-steps",
    "href": "visualising-trends.html#summary-and-next-steps",
    "title": "9¬† Visualising Trends with ggplot2 and autoplot()",
    "section": "\n10.4 Summary and Next Steps",
    "text": "10.4 Summary and Next Steps\nSo far you have mastered how to visualise our time series data using basic line plots with ggplot2 and autoplot(). You have seen how ggplot2 grants absolute control over every detail of your time series visualisations and how autoplot() delivers smart time series-specific defaults which drastically accelerates initial data exploration.\nYou can now make basic time series plots that reveal long term trends and seasonal cycles and also visualise multiple series simultaneously for comparative analysis.\nIn the next section, we are going to dive deeper into the feasts package to explore advanced time series decomposition, autocorrelation analysis and statistical pattern detection. The visual foundation we have built here will help us interpret those more advanced analysis.\nRemember that visualisation is both an art and a science. The best plots are those that not only look good but also communicate clear, accurate insights about your time series data. Practice creating different types of visualisations and always consider what story your data is trying to tell!",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Visualising Trends with `ggplot2` and `autoplot()`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html",
    "href": "exploring-patterns.html",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "",
    "text": "10.1 Decomposing a Time Series with STL()\nVisual inspection with ggplot2 and autoplot() gives us an intuitive feel for our time series. We might suspect a trend, see hints of seasonality, and notice unusual observations. But how can we move from intuition to a more formal, statistical understanding of these components? This is where the feasts and fabletools package comes in.\nThe package feasts, which stands for Feature Extraction And Statistics for Time Series, is a core part of the tidyverts framework which we have mentioned earlier (Chapter 1). Its purpose is to provide a toolkit for analysing the features of a time series data (tsibble), while fabletools supplies the underlying infrastructure that makes this analysis seamless and efficient. Think of it as a Doctor‚Äôs diagnostic kit: It does not treat the patients (that is the job of forecasting models in Chapter 14 and Chapter 15), but it runs the tests needed to understand what is going on.\nA fundamental concept in time series analysis is decomposition. The idea is to split, \\(y_{t}\\), into three additive parts (where the time series is represented as the sum of its parts):\n\\[\ny_{t}=Trend_{t}+Seasona_{t}+Remainder_{t}\n\\]",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html#decomposing-a-time-series-with-stl",
    "href": "exploring-patterns.html#decomposing-a-time-series-with-stl",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "",
    "text": "Trend (\\(T_{t}\\)): The long-term progression (the increasing or decreasing direction over time)\n\nSeasonal (\\(S_{t}\\)): Regular, repeating patterns over a fixed period (example, yearly or quarterly).\n\nRemainder (\\(R_{t}\\)): The ‚Äúleftover‚Äù part after trend and seasonality are removed; this is often considered the random noisy component.\n\n\n\n10.1.1 Basic and Advanced Decomposition with Special Arguments\nThe most robust method for this STL, which stands for Seasonal and Trend decomposition using Loess. Loess is a method for estimating non-linear relationships, and its use makes STL method versatile and robust to outliers as it handles a wide range of seasonal and trend shapes. The feasts package provides the STL() function to perform this decomposition directly on a tsibble object. Let us decompose the Sales series in the sales_ts data.\n\n# Decompose the Sales series using STL\ndcmp &lt;- sales_ts |&gt; \n  model(STL(Sales))\n\nThe simplest implementation use the default STL() function, which automatically selects suitable window sizes for the trend and seasonal components as illustrated in the above code. The model() function from the fabletools package is used to fit a ‚Äúmodel‚Äù (STL Decomposition) to the tsibble sales_ts. The variable to decomposed is specified inside the STL function.\nFor greater control you can include the variable of interest as a formula where you manually define the smoothing window for the trend and the seasonal components using the special arguments trend and season.\n\n# decompose Sales series using STL with special arguments\ndcmp_sp &lt;- sales_ts |&gt; \n  model(stl = STL(Sales ~ trend(window = 7) + season(window = \"periodic\")))\n\ntrend(window = 7) sets the window size for the trend smoothing, specifying that a trend should be estimated using a window of \\(2\\times7+1=15\\) observations. This comes from how the Loess (Locally Estimated Scatterplot Smoothing) method is typically implemented within the STL algorithm to calculate the trend component. A larger window results in a smoother trend.\n\n10.1.2 Extracting and Viewing Components\nThe dcmp object we created now contains the fitted decomposition model. To see the results, we use the components() function.\n\n# extract the components from the decomposition model\ncomponents(dcmp)\n\nThis returns a fabletools object dataset ‚Äúdable‚Äù (decomposition table) which is a tsibble-like data structure for representing decompositions. It contains the original time index with the Sales series together with four new columns;\n\n\ntrend: The estimated trend component.\n\nseason_year: The estimated seasonal component.\n\nremainder: The residual or error component (noise).\n\nseason_adjust: The final seasonally adjusted series (Original Series - Seasonal Component)\n\nThis allows for direct analysis and visualisation of how much of the variations in the Sales series is attributed to long-term movements, seasonal factors, and irregular noise. The autoplot() function when applied on this dable output will automatically generate a multi-panel plot displaying the decomposition of the Sales series into its components as calculated by the STL method.\n\ncomponents(dcmp) |&gt; \n  autoplot(linewidth = 0.7)\n\n\n\n\n\n\n\nThe plot features for main panels; the original Sales time series (\\(y_{t}\\)), The long term direction of the trend, the estimated periodic pattern and the noise or irregular fluctuations.\nFrom the STL decomposed plot we see that the original series is highly affected by both seasonality and a trend. The smooth long-term trend shows the overall level of sales over the years. Its starts around 35000 and dips noticeably around early 2018 and then rises back up towards 40,000 by late 2019\nThe seasonal panel shows a strong, consistent repeating pattern. The vertical scale (from approximately -5000 to 5000) indicates the magnitude of the seasonal effect relative to the trend.\nThe last panel shows the irregular, left over variations (from approximately -10,000 to 10,000). The spikes are quite large relative to the seasonal components. This implies a high amount of irregular unexplained variability in the data. The large spikes indicate months where sales were unusually high or low, which could be attributed to a variety of factors.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html#analysing-autocorrelation-patterns",
    "href": "exploring-patterns.html#analysing-autocorrelation-patterns",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "\n10.2 Analysing Autocorrelation Patterns",
    "text": "10.2 Analysing Autocorrelation Patterns\nAnother crucial property of time series is autocorrelation. Simply put, autocorrelation measures the relationship between a value in a series and its past values (called ‚Äúlags‚Äù). For example, is today;s sales figure correlated with the sales figure from the same quarter last year? This is a key concept for building effective forecasting models.\nThe feasts package provides two very useful functions, ACF (Autocorrelation Function) and PACF() (Partial Autocorrelation Function) to do this. Below, we see how to check autocorrelation for our Sales series.\n\n10.2.1 Autocorrelation Function (ACF)\nThe ACF measures the correlation between a time series \\(y_{t}\\) and its past values \\(y_{t-k}\\) (where \\(k\\) is the lag) at all observed lags, without isolating the effect of intermediate lags. High autocorrelation at a seasonal lag (e.g.¬†lag 12 for monthly data) is a strong indicator of seasonality. We use the ACF() function to calculate these correlations for the Sales series\n\n# compute the Autocorrelation for Sales\nsales_acf &lt;- sales_ts |&gt; \n  ACF(Sales)\n\nsales_acf\n## # A tsibble: 17 x 2 [1M]\n##        lag      acf\n##   &lt;cf_lag&gt;    &lt;dbl&gt;\n## 1       1M -0.0897 \n## 2       2M  0.0896 \n## 3       3M -0.271  \n## 4       4M -0.00704\n## 5       5M -0.191  \n## # ‚Ñπ 12 more rows\n\nWe can also specify the maximum number of lags to use for computing the correlations with the lag_max argument. By default it calculates this number using the formula ‚Äú\\(10\\times log_{10}(N/m)\\)‚Äù.\n\n# compute autocorrelation with specified lags\nsales_acf &lt;- sales_ts |&gt; ACF(Sales, lag_max = 24)\n\nThe resulting sales_acf object is a tsibble that contains the correlation coefficient at the different lags. A significant spike at a particular lag indicates a strong relationship.\nWe can visualise the output for easier interpretation using autoplot(). The plot of the ACF displays a series of vertical lines known as correlogram, where the height of each line represents the correlation at that specific lag.\n\nautoplot(sales_acf)\n\n\n\n\n\n\nFigure¬†10.1: Correlogram of Sales series Autocorrelations\n\n\n\n\nThe dashed blue lines in the plot represents the significance bounds. Any bar extending outside these bounds is considered a statistically significant correlation. The spikes do not show any slow, steady decline in the values from positive to negative. This suggests there is no strong trend.\nThere is no single large significant spike at regular intervals (like at lag 12M or 24M for monthly data). This indicates a lack of strong consistent annual seasonality. However there is a pattern of significant spikes in the first few lags, followed by a series of alternating pattern of negative and positive correlations in the short term. This implies a short term cyclical behaviour with a period of about 6 months.\nThis is what this ACF plot might mean in a business context:\n\nA customer makes a purchase (Lag0).\nThe negative correlation at Lag 3 suggests that three months after a purchase, sales tend to be opposite to what they were. If sales were high in a given month, they are likely to be low three months later and vice versa.\nThe positive correlation at Lag 6 suggests that whatever the sales pattern was at a given time, it tends to repeat itself every 6 months.\n\nThe other spikes later on at lag 14, 15 and 18 are likely harmonics of the primary 6-month cycle. Once a cyclical pattern is in the data, it can create correlations at multiple or fractions of its main period. They are part of the same underlying effect and not separate seasonal effects\n\n10.2.2 Partial Autocorrelation Function (PACF)\nThe PACF calculates the correlation between \\(y_{t}\\) and \\(y_{t-k}\\) after removing the influence of the intermediate lags \\(y_{t-1}\\), \\(y_{t-2}\\), \\(\\dots\\), \\(y_{t-(k-1)}\\). This helps to precisely identify the direct relationship between the current observation and a past observation, filtering out the ‚Äúflow-through‚Äù correlation effects. Think of it this way: The sales at lag 3 (3 months ago) might be correlated with sales today because;\n\nSales today are correlated with sales at lag 1 (ACF at lag 1)\nSales at lag 1 are correlated with sales at lag2.\nSales at lag 2 are correlated with sales at lag 3.\n\nThe PACF cuts through this chain and asks: ‚ÄúIf I already know the values at lags 1 and 2, does lag 3 still provide new, direct information today?\nThe PACF() function is used to calculate the partial autocorrelations.\n\n# compute the Partial Autocorrelation for Sales\nsales_pacf &lt;- sales_ts |&gt; \n  PACF(Sales, lag_max = 20) \n\nsales_pacf\n## # A tsibble: 20 x 2 [1M]\n##        lag    pacf\n##   &lt;cf_lag&gt;   &lt;dbl&gt;\n## 1       1M -0.0897\n## 2       2M  0.0822\n## 3       3M -0.260 \n## 4       4M -0.0571\n## 5       5M -0.170 \n## # ‚Ñπ 15 more rows\n\nsimilar to the ACF, the sales_pacf object also contains the partial autocorrelation values for each lag. We can again visualise this output using autoplot(). The PACF plot is primarily used to identify the order of an autoregressive (AR) component in a model\n\nsales_pacf |&gt; autoplot()\n\n\n\n\n\n\nFigure¬†10.2: Correlogram of Sales series Partial Autocorrelations\n\n\n\n\nThe most important aspect of our PACF plot is the behaviour in the first few lags. We notice that the is a significant negative spike at lag 3, with the subsequent lags generally becoming smaller and non significant, with no clear pattern. This is a classic signature of a PACF plot where the beginning few vertical bars shows a significant spike and the cuts off.\nThe PACF and ACF plots are considered diagnostic plots for identifying an ARIMA model‚Äôs specification (more information in Chapter 15).",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html#feature-based-explorations",
    "href": "exploring-patterns.html#feature-based-explorations",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "\n10.3 Feature Based Explorations",
    "text": "10.3 Feature Based Explorations\nAnother powerful aspect of the fabletools is its ability to compute a wide array of summary statistics or features from a time series. This is incredibly useful when you have many series (like in the gh_ts data with several indicators) and you want to quickly understand their characteristics or group similar series together.\n\n10.3.1 Identifying Seasonal Patterns and Strength\nThe most common feature to compute is the seasonal strength, which measures quantitatively how dominant the recurring patterns are compared to the overall trend and random noise. This is typically done using the STL decomposition. We demonstrate this with the tsibbledata::aus_production tsibble dataset which contains the quarterly production of selected commodities in Australia.\n\n# convert wide data to long \naus_prod_long &lt;- aus_production |&gt; \n  pivot_longer(cols = -Quarter, names_to = 'Product', values_to = 'Quantity')\n\n# calculate seasonal strength and other key features\naus_prod_features &lt;- aus_prod_long |&gt; \n  features(Quantity, features = feat_stl)\n\n# Display seasonal and trend metrics\naus_prod_features |&gt; \n  select(Product, contains(c('seasonal', 'trend')))\n## # A tibble: 6 √ó 5\n##   Product    seasonal_strength_year seasonal_peak_year seasonal_trough_year\n##   &lt;chr&gt;                       &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n## 1 Beer                        0.952                  0                    2\n## 2 Bricks                      0.859                  3                    1\n## 3 Cement                      0.834                  3                    1\n## 4 Electrici‚Ä¶                  0.906                  3                    1\n## 5 Gas                         0.981                  3                    1\n## # ‚Ñπ 1 more row\n## # ‚Ñπ 1 more variable: trend_strength &lt;dbl&gt;\n\nYou have to change the data to a long format so that the tsibble has a ‚Äòkey‚Äô variable (Product). The structure is essential for the fabletools feature functions. The features() function calculates metrics based on the STL decomposition for each Product series separately. The feat_stl feature set automatically returns several key metrics including:\n\n\nseasonal_strength_year: Measures the proportion of variance explained by the seasonal components. The values range between 0 and 1 (higher values mean stronger seasonality).\n\nseasonal_peak/seasonal_trough_year: Identifies the periods (Month for monthly data & Quarter for Quarterly data) of highest and lowest seasonal activity\n\ntrend_strength: Measures the proportion of variance explained by the trend component (higher values mean stronger trend)\n\n10.3.2 Comprehensive Feature Analysis\nBeyond standard seasonality, fabletools allows you to calculate a wide array of statistics that characterises every nuance of a time series‚Äô behaviour, including measures of shape, volatility and distribution.\nSTL Features for Non-Seasonal Characteristics\nThe STL feature set offers metrics beyond simple strength, focusing on the shape of the trend and the nature of the remainder (noise).\n\n# compute stl feature set for non-seasonal (yearly) data \nstl_features &lt;- gh_ts |&gt; \n  features(value, features = feature_set(tags = 'stl'))\n\nThis line of code within the the feature() function; faeture_set(tags = 'stl') instructs fabletools to compute the full set of STL-related features. These features include;\n\ntrend_strength: Measures how strong the trend component is relative to the remainder (noise). Values close to 1 have strong trend and values close to zero have weak or no trend. Example ‚ÄúFemale population‚Äù (\\(0.9999953\\)) has and extremely strong trend but‚ÄùAnnual GDP growth rate‚Äù (\\(0.3091004\\)) has a weak trend.\nspikiness: Measures the presence of sharp intermittent spikes in the remainder (noise) component. High values = more spiky/volatile. Example ‚ÄúRural Population‚Äù has extremely high spikiness (\\(2.086846\\times10^{14}\\)), indicating major irregularities.\nlinearity: Measures how linear the trend is (vs curved) based on fitting a linear model to the trend component. Positive values suggest trend can be well-approximated by a straight line. Negative values suggest non-linear trends. *‚ÄúCereal¬†yield¬†_kg¬†per¬†hectare‚Äù* (\\(3395.64\\)) is highly linear.\ncurvature: Measures the degree of bending in the trend. It complements linearity -captures how much the trend curves. Higher absolute values means more curved. ‚ÄúFemale population percentage‚Äù (\\(-1.741482\\)) shows negative curvature (concave down)\nstl_e_acf1: First order autocorrelation of the remainder component (lag1). The values ranges from -1 to 1. Values near 0 means remainders are random (which is good!). High positive or negative values indicates patterns left in residuals that the STL decomposition did not capture.\nstl_e_acf10: Sum of squares of the first 10 autocorrelations of the remainder. Measures the overall autocorrelation structure in residuals. Low values are good and high value indicate significant patterns remain in the residuals.\n\nCustom Feature Calculations with Lambda Expressions\nIf a desired feature is not pre-defined, you can compute it using a built in function or a simple lambda expression (a short, anonymous function)\n\n# compute any other features using their function or a lambda expression\npercentile_features &lt;- aus_prod_long |&gt; \n  features(Quantity, features = ~quantile(., na.rm=TRUE)) \n\nfeatures = ~quantile(., na.rm=TRUE) defines a custom function to run on the Quantity values. The ~ symbol creates the lambda expression and the . refers to the vector of time series values. This is a compact way of calculating the \\(0^{th}\\), \\(25^{th}\\), \\(50^{th}\\), \\(75^{th}\\) and \\(100^{th}\\) percentiles for each Product series, giving insight into its distribution and volatility range.\nMultiple Custom Features (List of Functions)\nYou can also calculate several custom features simultaneously by passing a named list of functions to the features argument.\n\n# for multiple custom features create a list of functions \ncent_tend_features &lt;- gh_ts |&gt; \n  features(value, features = list(sd = ~sd(., na.rm=T), median = ~median(., na.rm=T), mean = ~mean(., na.rm=T)))\ncent_tend_features\n## # A tibble: 22 √ó 4\n##   indicator_name                            sd     median       mean\n##   &lt;chr&gt;                                  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Annual GDP growth rate                 4.20        4.40       3.71\n## 2 Annual population groeth rate          0.290       2.46       2.50\n## 3 Cereal yield _kg per hectare         476.       1141.      1249.  \n## 4 Crude death rate_per 1000 people       4.18       11.4       12.6 \n## 5 Female population                4132889.    8127176    8926929.  \n## # ‚Ñπ 17 more rows\n\nThe custom anonymous functions supplied to the features argument in a list, calculates three measures of central tendency and dispersion. For every indicator in the gh_ts data, the standard deviation (sd), the median, and the mean are calculated. This can help you understand the average level of the series and its overall volatility across the entire period.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html#comprehensive-diagnostic-plots-for-model-selection",
    "href": "exploring-patterns.html#comprehensive-diagnostic-plots-for-model-selection",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "\n10.4 Comprehensive Diagnostic Plots for Model Selection",
    "text": "10.4 Comprehensive Diagnostic Plots for Model Selection\nThe gg_tsdisplay() function from the feasts package (now often found in the ggtime package) makes it possible to create specialised plots that combine multiple diagnostics to guide model selection. Especially an ARIMA model.\n\n# Create a comprehensive diagnostic plot for Gross National Expenditure\ngh_ts |&gt; \n  filter(indicator_name == \"Gross national expenditure (% of GDP)\") |&gt; \n  gg_tsdisplay(value, plot_type = \"partial\") +\n  labs(title = \"Comprehensive Diagnostic Plot for GNE (% of GDP)\",\n       subtitle = \"Series plot, ACF, and PACF for model selection guidance\")\n\n\n\n\n\n\nFigure¬†10.3: An Ensemble of TIme Series Plots Containing the Orignial Series(Top), ACF plot (bottom left) and PACF plot (bottom right)\n\n\n\n\nHere we filter the gh_ts data to isolate the specific series of interest; the Gross national expenditure (% of GDP) indicator. gg_tsdisplay(value, plot_type = \"partial\") generates a three panel plot:\n\nThe main panel (top) shows the Series Plot of the Gross national expenditure (% of GDP) values over the time period\nThe bottom left panel shows the ACF\nThe bottom right panel shows the PACF (specified by plot_type = \"partial\") )\n\nThe plot_type argument controls what type of plot is displayed in the bottom right panel it can either be a PACF, histogram, lagged scatterplot or spectral density depending on what you specify. The values that can be specified for this argument can be found in its help documentation.\nThe series show high volatility with no clear trend, the ACF shows a slow decay where the spikes remain significantly positive until lag 7, the PACF shows a sharp cut-off after Lag 1. All these indications serve as diagnostic signs for a forecasting model selection.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "exploring-patterns.html#summary-and-next-steps",
    "href": "exploring-patterns.html#summary-and-next-steps",
    "title": "10¬† Exploring Patterns with feasts and fabletools",
    "section": "\n10.5 Summary and Next Steps",
    "text": "10.5 Summary and Next Steps\nIn this chapter we moved beyond visual inspection and explored the feasts and fabletools package to perform a rigorous, quantitative diagnosis of our time series. We systematically:\n\nDecomposed the a time series using STL to isolate the trend, seasonal and remainder components.\nAnalysed autocorrelation with ACF and PACF to understand how each value depends on its past.\nQuantified pattern strength using features() to calculate statistics like seasonal_strength and trend_strenght, confirming our visual observations with hard numbers.\nCreated diagnostic plots using gg_tsdisplay() to get comprehensive multi-panel view of a time series‚Äô behaviour, which is essential for guiding our model selection.\n\nWith a deep understanding of patterns in our data, we are now ready to build forecasting models. But before we do, a critical step remains. In the next section, we will cover ‚ÄúPreparing Data for Forecasting‚Äù.",
    "crumbs": [
      "Exploratory Time Series Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Exploring Patterns with `feasts` and `fabletools`</span>"
    ]
  },
  {
    "objectID": "forecast-prepare.html",
    "href": "forecast-prepare.html",
    "title": "Preparing for Forecasting",
    "section": "",
    "text": "Why Preparation Matters\nExploring our time series has given us a deep understanding of our time series data ‚Äì we have been able to identify its patterns, rhythms and peculiarities. Now we stand at the crucial bridge between understanding what our data has been doing and predicting what it will do next.\nPreparing for forecasting is the process of transforming your explored data into a modelling ready series, ensuring your forecasts are built on solid statistical foundations rather than shaky assumptions.\nSkipping proper preparation is like building a house without a foundation ‚Äì It might look good initially, but it will not withstand the test of time. Proper preparation ensures:",
    "crumbs": [
      "Preparing for Forecasting"
    ]
  },
  {
    "objectID": "forecast-prepare.html#why-preparation-matters",
    "href": "forecast-prepare.html#why-preparation-matters",
    "title": "Preparing for Forecasting",
    "section": "",
    "text": "Model Reliability: Transformations and stationarity checks prevent statistical violations that undermines forecast accuracy\nRealistic Evaluation: Careful train-test splitting provides honest assessment of how your models will perform in the real world\nEfficient Modelling: Clean, structured data saves you from debugging mysterious model failures later",
    "crumbs": [
      "Preparing for Forecasting"
    ]
  },
  {
    "objectID": "forecast-prepare.html#the-journey-ahead",
    "href": "forecast-prepare.html#the-journey-ahead",
    "title": "Preparing for Forecasting",
    "section": "The Journey Ahead",
    "text": "The Journey Ahead\nBefore we can leverage powerful forecasting algorithms, we must address some fundamental prerequisites that transform our explored data into a modelling ready series. This section will guide you through three essential preparation stages that directly impact forecast accuracy and reliability.\n\n11¬† Transformations for Variance Stabilisation: This chapter will provide crucial knowledge on why and when variance stabilisation is necessary, how to identify heteroscedasticity (unequal variance) in a time series and applying Box-cox transformations to achieve constant variance. It goes on to further interpret the effects of different transformation parameters and how to assess their effectiveness via visualisations.\n12¬† Checking for Stationarity: In this chapter, you will learn to understand what stationarity means and its importance in forecasting, how to use statistical tests to detect non-stationarity and how to implement differencing techniques to achieve stationarity. We will also cover when seasonal differencing is needed and how to keep the transformed series easy to interpret.\n13¬† Splitting Temporal Data for Modelling: This chapter will teach you how to split a time series data into training and testing sets while keeping its structure intact. You will understand why temporal partitioning is important, how to choose the right split ratio and when to use time series cross-validation for better evaluation. We will also look at common mistakes that lead to data leakage and how to avoid them.\n\nTime series forecasting has special requirements that demand careful preparation. Unlike cross-sectional data, time series violates many standard statistical assumptions, requiring transformations and structured validations to produce reliable predictions.",
    "crumbs": [
      "Preparing for Forecasting"
    ]
  },
  {
    "objectID": "advanced-models.html",
    "href": "advanced-models.html",
    "title": "15¬† Advanced Forecasting Models",
    "section": "",
    "text": "by checking the patterns of the bars and their significance. Generally you can read the plots patterns and suggest a model type as follows:\n\n\n\n\n\n\n\nPattern\nSuggests Model Type\n\n\n\n\nACF: Dies down slowly / geometrically\nPACF: Significant spikes for first p lags, then cuts off\nAR(p) - Autoregressive model of order p\n\n\nACF: Significant spikes for first q lags, then cuts off\nPACF: Dies down slowly / geometrically\nMA(q) - Moving Average model of order q\n\n\nACF: Dies down slowly\nPACF: Dies down slowly\nARMA(p,q) or data needs differencing",
    "crumbs": [
      "Forecasting with the fable Framework",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Advanced Forecasting Models</span>"
    ]
  },
  {
    "objectID": "forecast-prepare.html#what-this-means-for-you",
    "href": "forecast-prepare.html#what-this-means-for-you",
    "title": "Preparing for Forecasting",
    "section": "What This Means for You",
    "text": "What This Means for You\nThe goal of preparation is to ensure your forecasting models have the best possible chance of success by giving them clean, appropriate data to learn from. By the end of this section, you will understand how to diagnose and fix common time series issues, create robust validation framework, and transform your data into its most forecast-ready state ‚Äì setting the stage for building powerful, reliable forecasting models in the next section.",
    "crumbs": [
      "Preparing for Forecasting"
    ]
  },
  {
    "objectID": "stabilise-variance.html",
    "href": "stabilise-variance.html",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "",
    "text": "11.1 Identifying Heteroscedasticity in Time Series Data\nMany time series models operate under the crucial assumption of homoscedasticity‚Äìthat the variance of the data remains constant over time. However, real world data often violates this assumption, displaying heteroscedasticity where the variability changes with the level of the series. Think of a growing business where seasonal sales swings become larger as the overall sales volume increases, or a temperature variation that become more extreme as climate patterns shift.\nWhen variance is not constant, our models can produce unreliable prediction intervals‚Äìthey might be too narrow during high variance periods and too wide during low variance periods. Variance stabilisation transforms our data to have more consistent variability throughout, making it better suited for modelling while often helping to normalise the distribution of errors.\nHeteroscedasticity refers to a change in the volatility and spread of a time series over time. Time series models (like ARIMA or ETS) assume that the variance of the forecast errors is constant. If the variance is changing it compromises the accuracy of confidence intervals and predictions.\nWe will examine the gh_ts and sale_ts data and check for signs of changing variance",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  },
  {
    "objectID": "stabilise-variance.html#identifying-heteroscedasticity-in-time-series-data",
    "href": "stabilise-variance.html#identifying-heteroscedasticity-in-time-series-data",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "",
    "text": "11.1.1 Visual Inspection of Variance\nThe initial step is always to plot the data and visually inspect the magnitude of the fluctuations\n# monthly sales data\nsales_ts |&gt; \n  autoplot(Sales, linewidth = 0.7)\n\n# Ghana: annual GDP growth rate \ngh_ts |&gt; \n  filter(indicator_name == \"Annual GDP growth rate\") |&gt; \n  autoplot(value, linewidth = 0.7)\n\n\n\n\n\n\n\n\nFigure¬†11.1: Monthly Sales Time Series (2015-2019)\n\n\n\n\n\n\n\n\n\nFigure¬†11.2: Annual GDP Growth Rate Time Series (1960-2024)\n\n\n\n\n\nThe monthly sales plot does not show a clear change in variance across the observed period. The magnitude of the fluctuations (the vertical distance between the seasonal peaks and troughs) remains roughly the same across the entire 5-year period. The values consistently oscillate between approximately \\(\\text{GH‚Çµ}10,000\\) and \\(\\text{GH‚Çµ}50,000\\) from 2015 through 2019. This implies the series is generally homoscedastic with respect to the mean.\nOn the other hand The Annual GDP growth rate plot is a classic example of variance change. The fluctuations are much larger and more dramatic in the early period (1960s to early 1980s). During this time, the values frequently swing between +10 and -10. After the mid-1980s, the fluctuations generally reduce and stabilise, mostly staying within the 0 to +10 range. There is a strong indication of variance reduction around the mid 1980s.\n\n11.1.2 Quantitative Check Using the guerrero Feature\nWhile a formal statistical test for heteroscedasticity requires fitting a model and examining the squared residuals, the Guerrero method offers a quick quantitative diagnostic to determine if a transformation is required to achieve homoscedasticity or not. This is a technique to automatically select the optimal parameter (\\(\\lambda\\)) for a Box-Cox transformation. Th feature() function will help us do this.\n\n# calculate optimum lambda value for sales data\nsales_ts |&gt; \n  features(Sales, guerrero)\n## # A tibble: 1 √ó 1\n##   lambda_guerrero\n##             &lt;dbl&gt;\n## 1           0.726\n\n# calculate optimum lambda value for Annual GDP growth rate series\ngh_ts |&gt; \n  filter(indicator_name == \"Annual GDP growth rate\") |&gt; \n  drop_na() |&gt; \n  features(value, feature = guerrero)\n## # A tibble: 1 √ó 2\n##   indicator_name         lambda_guerrero\n##   &lt;chr&gt;                            &lt;dbl&gt;\n## 1 Annual GDP growth rate            1.48\n\nThe \\(\\lambda\\) value is interpreted based on its proximity to three key values: 1, 0, or a non-integer value. The lambda value produced for the sale series (0.73) is close to 1 which suggests no transformation. This strongly supports the initial visual inspection which showed that the monthly sales data is largely homoscedastic (stable variance).\nThe lambda value for the annual GDP growth rate series (1.48, also close to 1) suggests that the variance is not proportional to the mean in a way that requires a a Box-Cox transformation (where \\(\\lambda\\le1\\)). This means the variance change observed in the visual inspection is not a simple multiplicative effect. The standard Box-Cox transformation is not sufficient here.",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  },
  {
    "objectID": "stabilise-variance.html#applying-box-cox-transformations",
    "href": "stabilise-variance.html#applying-box-cox-transformations",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "\n11.2 Applying Box-Cox Transformations",
    "text": "11.2 Applying Box-Cox Transformations\nThe Box-Cox transformation which we mentioned earlier is a family of power transformations that can stabilise variance across different levels of our time series. The transformation is defined as:\n\\[\ny^{(\\lambda)}= \\left\\{ \\begin{array}{cl}\\frac{y^{\\lambda}-1}{\\lambda} & if\\ \\ \\lambda\\geq 0 \\\\log(y) & if\\ \\ \\lambda = 0\\end{array} \\right.\n\\]\n$$$$\nRather than guessing the optimal \\(\\lambda\\) value, we can use the guerrero() feature as seen earlier from the feasts package to find it automatically.\nWe have already seen how to do this and also know that the Sales series and the Annual GDP growth rate series do not warrant any type of Box-Cox transformation. We will use the aus_production data to demonstrate how the Box-Cox application works\n\n# Visualise Quarterly Cement production in Australia \naus_production |&gt; \n  autoplot(Cement, linewidth = 0.7)\n\n\n\n\n\n\nFigure¬†11.3: Quarterly Cement Production in Australia (1956-2010)\n\n\n\n\nThe visual inspection shows a clear evidence of unequal variance, specifically increasing variance over time. The series is at a low level in the early periods (1960-1980Q1) and the fluctuations are small relative to the mean (seasonal peaks and troughs closer together).\nIn the later period (1990 - 2008Q4) the series is at a high level and the fluctuations are visually much larger (the distance between peaks and troughs has increased). This makes the series heteroscedastic because the variance increase as the level (mean production) increases.\nThis pattern is known as multiplicative variance and is a characteristic of a time series that exhibit growth. We will now calculate the optimal lambda needed for a Box-Cox transformation for this series.\n\n# calculate and extract optimal lambda using the guerrero method\noptimal_lambda &lt;- aus_production |&gt; \n  features(Cement, features = guerrero) |&gt; \n  pull(lambda_guerrero)\n\nThe optimal lambda value calcuated is \\(-0.309022\\) (close to 0) which clearly suggests a transformation is required and confirms our visual inspection. In practice you would typically use a log transformation \\((\\lambda=0)\\) for such values (closer to zero) rather than the exact \\(\\lambda=-0.309022\\). This is because a log transformation is simpler and more interpretable which performs similarly to nearby lambda values (in which our optimal_lambda falls squarely in that range). Now let us transform and compare the results\n\n# Apply Box-Cox transformation with optimal lambda\ntransformed_aus_cement &lt;- aus_production |&gt; \n  select(Quarter, Cement) |&gt; \n  mutate(trans_Cement = box_cox(Cement, lambda = round(optimal_lambda)))\n\n# Visualise original vs transformed series\ntransformed_aus_cement |&gt; \n  pivot_longer(\n    cols = -Quarter,\n    names_to = 'Version',\n    values_to = 'Sales_value'\n  ) |&gt; \n  autoplot(Sales_value, linewidth = 0.7, show.legend = FALSE) +\n  facet_wrap(vars(Version), scales = 'free_y',\n             labeller = labeller(Version = c('Cement' = 'Original Series', 'trans_Cement' = \"Transformed Series\"))) +\n  labs(title = \"Original vs. Box-Cox Transformed Cement Production Series\",\n       subtitle = paste(\"Transformation parameter Œª =\", round(optimal_lambda)),\n       y = 'Cement Production',\n       x = \"Quarter\")\n\n\n\n\n\n\nFigure¬†11.4: Visually Comparing the Original Cement Series with The Log Transformed Series\n\n\n\n\nThe transformation is applied using the box_cox() function from the fabletools package. It takes the raw Cement values and applies the Box-Cox formula using the provided lambda. We round lambda to the nearest whole number (0) to get a log transformation.\nComparing the original and transformed series you can clearly see how the fluctuations look similar throughout the entire period for the transformed series, whiles still showing the upward trend from the original series (preserves trend direction).\nThe peaks and troughs for all quarters seems to have similar amplitudes throughout the years. Also notice how the y-axis scale changed from ~500 - 2500 in the original series to ~6.5 - 7.5 (log scale) in the transformed series giving it a much more constant variance over time. The transformed series is now suitable for ARIMA or ETS modelling as the constant amplitude of fluctuations satisfies the constant variance assumption.\n\n\n\n\n\n\nNote\n\n\n\nRounding the exact \\(\\lambda\\) to a simpler, nearby value (like 0 for a log transformation) is a common, pragmatic step for better interpretability. After forecasting we can back-transform to original scale: exp(log_forecast)",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  },
  {
    "objectID": "stabilise-variance.html#interpreting-transformation-parameters",
    "href": "stabilise-variance.html#interpreting-transformation-parameters",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "\n11.3 Interpreting Transformation Parameters",
    "text": "11.3 Interpreting Transformation Parameters\nDifferent lambda values correspond to different transformations. If the output of lambda is close to 1 the series is already homoscedastic, and no transformation is needed \\((Y_{t}^{1}=Y_{t})\\). A lambda close to 0 has a variance proportional to its level (common in growth series), and a log transformation is required \\((\\lambda\\simeq0\\ \\Rightarrow\\ log(Y_{t}))\\). Some common lambda interpretations are presented below\n\n\n\\(Œª = 1\\): No transformation needed (data is already suitable)\n\n\\(\\lambda\\ \\text{close to 1}\\) (e.g., 0.7 to 1.3): Transformation provides minimal benefit\n\n\\(Œª = 0.5\\): Square root transformation\n\n\\(Œª = 0\\): Log transformation\n\n\\(Œª = -1\\): Inverse transformation\n\n\n# apply and compare multiple common lambda values for demonstration\ncomparison_data &lt;- aus_production |&gt; \n  select(Quarter, Original = Cement) |&gt;\n  mutate(\n    Log_transform = box_cox(Original, lambda = 0),    # Natural log\n    Sqrt_transform = box_cox(Original, lambda = 0.5), # Square root\n    Cube_root = box_cox(Original, lambda = 1/3),      # Cube root\n    Optimal_Œª = box_cox(Original, lambda = optimal_lambda)\n  )\n\n# plot all transformations for comparison\ncomparison_data |&gt; \n  pivot_longer(\n    cols = -Quarter,\n    names_to = \"Transformations\",\n    values_to = \"Value\"\n  ) |&gt; \n  ggplot(aes(x=Quarter, y=Value, colour = Transformations)) +\n  geom_line(linewidth = 0.7) +\n  facet_wrap( ~Transformations, scales = \"free_y\", ncol = 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure¬†11.5: Comparing Different Box-Cox Transformations with their Original Series Visually\n\n\n\n\nNotice how each series has varying y-scale due to the type of transformation applied.",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  },
  {
    "objectID": "stabilise-variance.html#assessing-transformation-effectiveness",
    "href": "stabilise-variance.html#assessing-transformation-effectiveness",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "\n11.4 Assessing Transformation Effectiveness",
    "text": "11.4 Assessing Transformation Effectiveness\nAfter applying our transformation, we need to verify that it actually improved variance stability. We will create a before and after variance diagnostic and then visually inspect them\n\nlibrary(slider)          # for roling mean and standard deviation\nlibrary(patchwork)       # for plot layout\n\n# Calculate rolling mean and variance\nvariance_mean_relationship &lt;- transformed_aus_cement |&gt; \n  as_tibble() |&gt; \n  mutate(\n    # Original series\n    orig_mean = slide_dbl(Cement, mean, .before = 16, .complete = TRUE),\n    orig_sd = slide_dbl(Cement, sd, .before = 16, .complete = TRUE),\n    \n    # Transformed series\n    trans_mean = slide_dbl(trans_Cement, mean, .before = 16, .complete = TRUE),\n    trans_sd = slide_dbl(trans_Cement, sd, .before = 16, .complete = TRUE)\n  ) |&gt; \n  filter(!is.na(orig_mean))\n\n# Plot variance-mean relationship\np1 &lt;- variance_mean_relationship |&gt; \n  ggplot(aes(x = orig_mean, y = orig_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Original Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\np2 &lt;- variance_mean_relationship |&gt; \n  ggplot(aes(x = trans_mean, y = trans_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n  labs(\n    title = \"Transformed Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\nlibrary(patchwork)\np1 + p2 \n\n\n\n\n\n\nFigure¬†11.6: Mean-Variance Relationship For Original Series and Transformed Series\n\n\n\n\nThe Left plot (Original Series) shows a clear linear relationship between its rolling mean and rolling standard deviation (SD). This is a clear heteroscedasticity where variance grows with the series‚Äô level, which further confirms that the original series has a variance proportional to the mean. The nearly flat blue line in the left plot indicates how the transformation has now stabilised the variance across all levels. The rolling SD stays relatively constant (~0.07 to ~0.125) regardless of the rolling mean.\nPicking a rolling window that captures the full business cycle is very essential for a stable estimation while still remaining sensitive to local patterns. With quarterly data 16 quarters or 4 years (rolling window) is a good compromise between smoothness and responsiveness. You could also quantify this relationship with correlations\n\n# correlation for original series and transformed series \ncor_original &lt;- cor(\n  variance_mean_relationship$orig_mean,\n  variance_mean_relationship$orig_sd\n)\n\ncor_transformed &lt;- cor(\n  variance_mean_relationship$trans_mean,\n  variance_mean_relationship$trans_sd\n)\n\n\n## Original Series correlation (mean vs SD): 0.883\n## Transformed series correlation (mean vs SD: 0.018",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  },
  {
    "objectID": "stabilise-variance.html#interpreting-transformation-parameters-and-assessing-effectiveness",
    "href": "stabilise-variance.html#interpreting-transformation-parameters-and-assessing-effectiveness",
    "title": "11¬† Transformations for Variance Stabilisation",
    "section": "\n11.3 Interpreting Transformation Parameters and Assessing Effectiveness",
    "text": "11.3 Interpreting Transformation Parameters and Assessing Effectiveness\n\n11.3.1 Interpreting Transformation Parameters\nThe Box-Cox transformation is governed by the parameter \\(\\lambda\\) (lambda), which dictates the power applied to the data. Different lambda values correspond to different transformations. Understanding the relationship between lambda and the resulting transformation is key to time series data preparation.\n\n\n\n\n\n\n\nlambda \\((\\lambda)\\) value\nRecommended Transformation\nVariance Implication\n\n\n\n\\(\\lambda=1\\)\nNo Transformation \\((Y_{t})\\)\n\nSeries is already Homoskedastic\n\n\n\n\\(\\lambda\\ \\text{close to 1}\\) (e.g., 0.7 to 1.3)\n\nTransformation provides minimal benefit\nVariance is largely stable\n\n\n\\(\\lambda=0.5\\)\nSquare Root \\((\\sqrt{Y_{t}})\\)\n\nModerate stabilisation required\n\n\n\\(\\lambda=0\\)\nNatural Logarithm \\((\\log(Y_{t}))\\)\n\nVariance is proportional to the level (Multiplicative variance)\n\n\n\\(\\lambda=-1\\)\nInverse \\((1/Y_{t})\\)\n\nSevere stabilisation required; common when variance decreases as the mean decreases\n\n\n\nWe demonstrate how different lambda values visually impact the cement production data with the code below.\n\n# apply and compare multiple common lambda values for demonstration\ncomparison_data &lt;- aus_production |&gt; \n  select(Quarter, Original = Cement) |&gt;\n  mutate(\n    Log_transform = box_cox(Original, lambda = 0),    # Natural log\n    Sqrt_transform = box_cox(Original, lambda = 0.5), # Square root\n    Cube_root = box_cox(Original, lambda = 1/3),      # Cube root\n    Optimal_Œª = box_cox(Original, lambda = optimal_lambda)\n  )\n\n# plot all transformations for comparison\ncomparison_data |&gt; \n  pivot_longer(\n    cols = -Quarter,\n    names_to = \"Transformations\",\n    values_to = \"Value\"\n  ) |&gt; \n  ggplot(aes(x=Quarter, y=Value, colour = Transformations)) +\n  geom_line(linewidth = 0.7) +\n  facet_wrap( ~Transformations, scales = \"free_y\", ncol = 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure¬†11.5: Comparing Different Box-Cox Transformations with their Original Series Visually\n\n\n\n\nThe comparative plot confirms that transformations with lambda values near zero (Log, Cube Root, and Optimal Œª) produce visually similar variance-stabilised series, while the Square Root transform \\((\\lambda=0.5)\\) slightly undershoots the required stabilisation. Notice how each series has varying y-scale due to the type of transformation applied.\n\n11.3.2 Assessing Transformation Effectiveness\nAlthough visual inspections provide insightful patterns and trends, we can statistically verify that the transformation successfully made the variance independent of the mean. This is done by examining the relationship between the rolling mean (level) and the rolling standard deviation (volatility)\n\nlibrary(slider)          # for roling mean and standard deviation\nlibrary(patchwork)       # for plot layout\n\n# Calculate rolling mean and variance\nvariance_mean_relationship &lt;- transformed_aus_cement |&gt; \n  as_tibble() |&gt; \n  mutate(\n    # Original series\n    orig_mean = slide_dbl(Cement, mean, .before = 16, .complete = TRUE),\n    orig_sd = slide_dbl(Cement, sd, .before = 16, .complete = TRUE),\n    \n    # Transformed series\n    trans_mean = slide_dbl(trans_Cement, mean, .before = 16, .complete = TRUE),\n    trans_sd = slide_dbl(trans_Cement, sd, .before = 16, .complete = TRUE)\n  ) |&gt; \n  filter(!is.na(orig_mean))\n\n# Plot variance-mean relationship\np1 &lt;- variance_mean_relationship |&gt; \n  ggplot(aes(x = orig_mean, y = orig_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Original Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\np2 &lt;- variance_mean_relationship |&gt; \n  ggplot(aes(x = trans_mean, y = trans_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n  labs(\n    title = \"Transformed Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\np1 + p2 \n\n\n\n\n\n\nFigure¬†11.6: Mean-Variance Relationship For Original Series and Transformed Series\n\n\n\n\nThe code above uses the slide_dbl function from the slider package to calculate the rolling standard deviation over a 17-quarter window (.before = 16)\nThe Left plot (Original Series) shows a clear linear relationship between its rolling mean and rolling standard deviation (SD). This is a clear heteroscedasticity where variance grows with the series‚Äô level, which further confirms that the original series has a variance proportional to the mean. The nearly flat blue line in the left plot indicates how the transformation has now stabilised the variance across all levels. The rolling SD stays relatively constant (~0.07 to ~0.125) regardless of the rolling mean.\nPicking a rolling window that captures the full business cycle is very essential for a stable estimation while still remaining sensitive to local patterns. With quarterly data 17 quarters or ~4 years (rolling window) is a good compromise between smoothness and responsiveness. You could also quantify this relationship with correlations\n\n# correlation for original series and transformed series \ncor_original &lt;- cor(\n  variance_mean_relationship$orig_mean,\n  variance_mean_relationship$orig_sd\n)\n\ncor_transformed &lt;- cor(\n  variance_mean_relationship$trans_mean,\n  variance_mean_relationship$trans_sd\n)\n\n\n## Original Series correlation (mean vs SD): 0.883\n## Transformed series correlation (mean vs SD: 0.018\n\nThis is the most objective evidence for assessing the transformations effectiveness. The strong positive correlation (88.3%) in the original series confirms heteroscedasticity (volatility depends on the level). The near zero correlation (1.8%) in the transformed data provides conclusive statistical evidence that the Box-Cox transformation was highly effective and has properly prepared the Cement Production time series for subsequent time series modelling",
    "crumbs": [
      "Preparing for Forecasting",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Transformations for Variance Stabilisation</span>"
    ]
  }
]