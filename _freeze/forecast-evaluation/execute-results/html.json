{
  "hash": "fa6ad2c333fe79d7e05dac7bcff16b6e",
  "result": {
    "engine": "knitr",
    "markdown": "# Model Evaluation and Forecasting {#sec-model-evaluation}\n\nWe have now reached the final and important stage of our time series journey where we bring everything together. After cleaning and preparing our data, exploring patterns and building both simple and advanced forecasting models, we now face the ultimate test; determining which models actually work in practice and deploying them responsibly. Think of this chapter as your forecasting graduation ceremony.\n\nThis chapter is not just about technical metrics â€“ it is developing judgement to separate reliable forecast from wishful thinking. We will learn how to compare models objectively, understand their uncertainties, and create production-ready forecasting systems that deliver real value whether you are predicting sales, weather, crop yields or website traffic.\n\n\n\n\n\n## Fitting Multiple models and Generating Forecasts\n\nIn the real world, relying on a single forecasting model is like bringing only one tool to a construction site. Imaging you are investing in stocks. Would you put all your money in a single company? Ofcourse not! You would diversify. No single model is perfect for all situations so smart forecasters create a model portfolio. Building different models help to capture different aspects of your data, and by creating a diverse model portfolio, you gain both robustness and insights.\n\n### Data Preparation: Cocoa Prices\n\nWe introduce a new dataset ([`gh_cocoa.csv`](https://github.com/gkagyen/tstidyway/blob/1043865dd84ba98047394fc6957fdf46f1f4b1b6/data/gh_cocoa.csv)) for this work. This data was obtained from the Bank of Ghana website (<https://www.bog.gov.gh/economic-data/commodity-prices/>), and it contains the average monthly Cocoa Prices in USD per Tonne from from January 2000 to April 1 2023. Cocoa prices are notoriously volatile, influenced by weather, global demand, and economic policies, making them an ideal candidate for a multi-model approach.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load and prepare cocoa price data\ncocoa_ts <- gh_cocoa |> \n  mutate(Date = yearmonth(Month)\n         ) |> \n  as_tsibble(index = Date) |> \n  select(Date, Price)\n\n# split into training and testing set (use 75% for training and 25% for testing)\nsplit <- initial_time_split(cocoa_ts, prop = 0.75)\ntraining_cocoa <- training(split)\ntesting_cocoa <- testing(split)\n```\n:::\n\n\nFirst we load and prepare the raw data into a `tsibble` format suitable for time series analysis and then create a temporal split with `initial_time_split()` function. We use 75% of the data (Jan 2020 - roughly mid 2017) for training and the remaining 25% (roughly mid 2017 - April 2023).\n\n### Building a Comprehensive Model Portfolio\n\nWe use the fable workflow to simultaneously fit a wide range of models to the training data. Each model represents a different hypothesis about how the cocoa prices evolve.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# build comprehensive model portfolio\ncocoa_model_portfolio <- training_cocoa |> \n  model(\n    # Benchmark models\n    naive = NAIVE(Price),\n    seasonal_naive = SNAIVE(Price),\n    mean = MEAN(Price),\n    \n    # Exponential Smoothing Family\n    ets_auto = ETS(Price),\n    ets_hw_mult = ETS(Price ~ trend(\"A\") + season(\"M\")),\n    ets_damped = ETS(Price ~ trend(\"Ad\")),\n    \n    # ARIMA family\n    arima_auto = ARIMA(Price),\n    arima_log = ARIMA(log(Price)),\n    arima_seasonal = ARIMA(log(Price) ~ pdq(0,1,1) + PDQ(0,0,1)),\n    \n    # Other simple linear models\n    linear = TSLM(Price ~ trend()),\n    linear_season = TSLM(Price ~ trend() + season())\n  )\n```\n:::\n\n\nEleven different models are created for our portfolio of models spanning simple benchmark models to sophisticated approaches.\n\n**Benchmark Models**\n\n-   `naive` assumes the future price will be exactly the same as the last observed price (random walk).\n-   `seasonal_naive` assumes the price next January will be on the same last January.\n-   `mean` predicts the historical average price.\n\n**Exponential Smoothing (ETS) Family**\n\n-   `ets_auto` lets the algorithm automatically select the best combination of **error**, **trend** and **seasonality** based on AICc.\n-   `ets_hw_mult` (Holt-Winters Multiplicative), explicitly models a linear trend and seasonality that grows in magnitude as prices rise.\n-   `ets_damped` models a trend that eventually flattens out (damped), assuming price rallies or crashes will not last forever.\n\n**ARIMA Family**\n\n-   `arima_auto` automatically selects the best orders $(p,d,q)(P,D,Q)$ to handle correlations and stationarity\n-   `arima_log` fits an ARIMA model to the the log transformed prices; this stabilises variance.\n-   `arima_seasonal`: explicitly forces the model to use the specified non-seasonal and seasonal autoregressive and moving average orders.\n\n**Linear Models**\n\n-   `linear` fits a simple straight line (regression) through the data; assuming a constant rate of price increase/decrease over time.\n-   `linear_season` fits a straight line but adds a fixed \"dummy\" effect for each month assuming a constant additive seasonality.\n\nThis ensemble of models creates a powerful \"tournament\" where we can rigorously test which mathematical description best matches the reality of the cocoa market.\n\n### Generating a 2-Year forecast\n\nWith our diverse portfolio of models fitted to the historical data, we can now project the behaviour of cocoa prices into the future. This is the moment where each model applies its learned patterns â€“ trend, seasonality, and level â€“ to predict what comes next.\n\nWe will generate a 24-month (2-year) forecast for all models in our portfolio.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate 24 month forecast from all models\ncocoa_forecasts <- cocoa_model_portfolio |> \n  forecast(h = 24)\n\n# view forecasts\ncocoa_forecasts |> hilo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 264 x 6 [1M]\n# Key:       .model [11]\n  .model     Date\n  <chr>     <mth>\n1 naive  2017 Jul\n2 naive  2017 Aug\n3 naive  2017 Sep\n4 naive  2017 Oct\n5 naive  2017 Nov\n# â„¹ 259 more rows\n# â„¹ 4 more variables: Price <dist>, .mean <dbl>, `80%` <hilo>, `95%` <hilo>\n```\n\n\n:::\n:::\n\n\nThe forecast produces a probabilistic prediction for each model for every future month including 80% and 95% confidence intervals. By comparing the forecasts from each model, we can gauge the consensus or divergence in the market outlook.\n\n## Visualising and Comparing Forecasts\n\nA well crafted visualisation can reveal more than a table of metrics. Let's see all our forecasts with the historical data. This helps to reveal consensus, divergence and model uncertainties far more effectively than reading a table of numbers. Since the models in the portfolio operate under different mathematical assumptions, their projections for the future will vary significantly.\n\n### Portfolio Forecast Plot (Consensus & Divergence)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot all forecasts against historical data\ncocoa_forecasts |> \n  autoplot(training_cocoa, level = 95) + \n  labs(title = \"Ghana Cocoa Prices: 24-Month Forecasts from 11 Models\",\n       subtitle = \"shaded areas show 95% prediction intervals\")+\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Forecasts from Models in Portfolio](forecast-evaluation_files/figure-html/fig-portfolio-forecast-1.png){#fig-portfolio-forecast fig-align='center' width=672}\n:::\n:::\n\n\nThe plot in @fig-portfolio-forecast above overlays all 11 forecasts onto the historical price series, highlighting the end of the training data (mid-2017) and the 24-month forecast period. The 11 models show high divergence. The forecast lines cluster tightly near the start of the forecast horizon but diverge dramatically over the 24 months. The red, orange, and pink shaded areas cover a huge vertical span, indicating extremely high uncertainty about the cocoa prices\n\nMany models (the $\\text{ETS}$ and $\\text{ARIMA}$ models) predict that the price will remain relatively flat or slightly trend upward from the starting point of approximately $\\$2,000$ to $\\$2,500$. The seasonal naive (SNAIVE) model (optimistic model) predicts a strong cyclical rebound, taking prices over $\\$3,000$ by the end of the horizon.\n\nIn summary, clustered lines suggest models agree, while diverging or large separated lines suggest high prediction uncertainty between models. Analysts must treat this forecast with caution as the 95% interval spans thousands of dollars.\n\n### Individual Model Forecasts\n\nFor a cleaner presentation you would want a clear visualisation of each model's forecast in a different panel.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create separate panels for each model\ncocoa_forecasts |> \n  autoplot(training_cocoa, level = 95) +\n  facet_wrap(~.model, nrow = 4) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::label_currency()) +\n  scale_x_yearmonth(date_breaks = \"5 years\") +\n  labs(title = \"Individual Model Forecasts for Ghana Cocoa Prices\",\n       subtitle = \"Training period: 2000-2017 | Forecast horizon: 24 months\")\n```\n\n::: {.cell-output-display}\n![Cleaner Presentation of Forecasts from Models in Separate Panels](forecast-evaluation_files/figure-html/fig-facet-forecast-mods-1.png){#fig-facet-forecast-mods fig-align='center' width=672}\n:::\n:::\n\n\n@fig-facet-forecast-mods presents each model's forecast in a separate panel for easy identification. The `mean`, `naive` and `ets_auto` models projects flat lines into the future.The `arima_seasonal` and `ets_hw_mult` captures some seasonality with a mild upward drift. The linear models show a consistent upward trend with the `linear_season` accounting for some seasonality using seasonal dummy variables. The `seasonal_naive` shows a strong highly volatile seasonal cycle, projecting massive price swings based on the last years pattern.\n\nThe overall comparison emphasizes the need for Model Evaluation on unseen data, as the choice between conservative stability and aggressive volatility in some models is still unresolved.\n\n## Model Accuracy Assessment Framework\n\nA good forecast is not just about low errors , it is about getting close to the actual values. Accuracy metrics tell us how wrong our models are, but different metrics emphasise different errors. Understanding these nuances is crucial for selecting the right model for your specific use case.\n\n### Initial Evaluation on Training Data\n\nWe begin by assessing the models' performance on the training data â€“ the historical period used to fit the model parameters. This shows how well each model captures the known historical patterns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate model accuracy on training data\ntraining_accuracy <- cocoa_model_portfolio |> \n  accuracy() |> select(.model, MASE, RMSE, MAPE, MAE) \n\ntraining_accuracy |> arrange(MASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 Ã— 5\n  .model          MASE  RMSE  MAPE   MAE\n  <chr>          <dbl> <dbl> <dbl> <dbl>\n1 arima_seasonal 0.265  134.  5.02  102.\n2 arima_auto     0.265  134.  5.03  102.\n3 arima_log      0.267  134.  5.00  103.\n4 ets_damped     0.268  136.  5.07  103.\n5 ets_auto       0.269  136.  5.08  104.\n# â„¹ 6 more rows\n```\n\n\n:::\n:::\n\n\nThe ARIMA/ETS models form the top 5 best performing models with $MASE \\approx 0.26$. The `seasonal_naive` model is the baseline error for the seasonal cycle ($MASE = 1$). The non-seasonal naive (`naive`) model has an $MASE$ of $0.27$. The fact that it is much better than the seasonal naive model suggests that the cocoa prices lack a stable repeating monthly seasonal pattern.\n\nThe linear models perform poorly ($MASE \\approx 0.88$), indicating that simple regression fails to capture the high volatility and non-linear trend of the cocoa prices.\n\n**Interpreting Key Accuracy Metrics**\n\n| Metric | What It Measures | Example Interpretation | Good for Cocoa Prices? |\n|------------------|------------------|--------------------|------------------|\n| MAE | Average dollar error | *\"We are usually off by \\$100/tonne\"* | Yes â€“ easy to understand |\n| RMSE | Square root average squared error | *\"Big mistakes hurt us badly\"* | Yes â€“ penalises large errors |\n| MAPE | Percentage error | *\"We are usually off by 12%\"* | Yes â€“ comparable across time |\n| MASE | Error vs naive forecast | 0.8 = 20% better than naive (baseline) | Yes â€“ best overall metric |\n\nA common pitfall is relying solely on point forecast metrics like RMSE or MAE. A model with great RMSE but terrible interval coverage is unreliable for risk management. After selecting the best model based on MASE, it is crucial to check the accuracy on the held out test data and verify the interval coverage. But before selecting the best model and validating on the test data we will perform a time series cross validation.\n\n## Time Series Cross Validation and Model Selection\n\nTraditional train-test splits can be misleading for time series because they only evaluate performance at one specific historical cut-off. . Time series cross-validation (TSCV) provides a more robust and realistic assessment by testing models on multiple historical periods. This simulates many \"what if?\" scenarios over the series' lifetime, leading to a more reliable error estimate.\n\n### Implementing Time Series Cross-Validation\n\nWe use the expanding window approach (`stretch_tsibble`) to create sequential training folds from the training data. @sec-splitting-data explains in detail how to create time series cross-validation folds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create time series cross validation\ncocoa_cv <- training_cocoa |> \n  stretch_tsibble(.init = 120, # 10-year initial fold\n                  .step = 6    # roll forward 6 months\n                )\n```\n:::\n\n\nThe core of the TSCV process is fitting the model portfolio to every single fold and then calculating two things; the **fit quality** (using $AICc$) and the **out-of-sample accuracy** ($MASE$).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# perform rolling origin evaluation \ncv_models <- cocoa_cv |> \n  model(\n    naive = NAIVE(Price),\n    seasonal_naive = SNAIVE(Price),\n    mean = MEAN(Price),\n    ets_auto = ETS(Price),\n    ets_hw_mult = ETS(Price ~ trend(\"A\") + season(\"M\")),\n    ets_damped = ETS(Price ~ trend(\"Ad\")),\n    arima_auto = ARIMA(Price),\n    arima_log = ARIMA(log(Price)),\n    arima_seasonal = ARIMA(log(Price) ~ pdq(0,1,1) + PDQ(0,0,1)),\n    linear = TSLM(Price ~ trend()),\n    linear_season = TSLM(Price ~ trend() + season())\n  )\n\n# check model fit quality from cross validation\ncv_modfit <- glance(cv_models) |> \n  group_by(.model) |> \n  summarise(\n    AVG_AICc = mean(AICc),\n    .groups = \"drop\"\n  ) |> arrange(AVG_AICc)\ncv_modfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 Ã— 2\n  .model         AVG_AICc\n  <chr>             <dbl>\n1 arima_log         -396.\n2 arima_seasonal    -392.\n3 linear            1967.\n4 linear_season     1989.\n5 arima_auto        2082.\n# â„¹ 6 more rows\n```\n\n\n:::\n:::\n\n\nAfter fitting the models to each fold, we use the average AIC across all folds to measure the model's complexity and goodness of fit to the training data. The ARIMA models applied to the log-transformed data shows the lowest average AIC (`AVG_AICc`) indicating they provide the most parsimonious fit for the training history.\n\nWe can now use the `forecast()` and `accuracy()` functions to evaluate the true predictive power on the \"future\" data of each fold.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_results <- cv_models |>  \n  forecast( h = 12) |>  # 1-year ahead forecast\n  accuracy(cocoa_ts |> \n             filter(Date <= yearmonth(\"2018 Jun\"))\n           )            # compare with actual data till forecst period\n\ncv_results |> select(.model, .type, MASE, RMSE, MAPE, MAE) |> arrange(MASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 Ã— 6\n  .model         .type  MASE  RMSE  MAPE   MAE\n  <chr>          <chr> <dbl> <dbl> <dbl> <dbl>\n1 naive          Test  0.769  391.  11.7  296.\n2 ets_damped     Test  0.773  394.  11.8  298.\n3 arima_auto     Test  0.779  394.  11.8  300.\n4 arima_seasonal Test  0.810  418.  12.4  312.\n5 ets_auto       Test  0.834  433.  12.8  321.\n# â„¹ 6 more rows\n```\n\n\n:::\n:::\n\n\nThe lowest $MASE$ value belongs to the `naive` model for the out of sample accuracy. Since $\\text{MASE} \\lt 1$, every model arranged above the `seasonal_naive` model (from row 1 to 7) is considered statistically better than the seasonal baseline. The `naive` model often performs well on commodity prices because the price is essentially a random walk (today's best guess for tomorrow is today's price).\n\n### Final Model Selection\n\nFor robust model selection, a combination of both internal fit ($AICc$) and out of sample performance ($MASE$) is used. This method penalises models that fit the training data perfectly but fails to generalise.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# select best model based on AICc and MASE\ncv_results |> \n  left_join(cv_modfit, by = \".model\") |> \n  select(.model, AVG_AICc, MASE) |> \n  mutate(\n    AIC_rank = rank(AVG_AICc),\n    MASE_rank = rank(MASE),\n    Combined_rank = (AIC_rank+MASE_rank)/2\n  ) |> \n  arrange(Combined_rank)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 Ã— 6\n  .model         AVG_AICc  MASE AIC_rank MASE_rank Combined_rank\n  <chr>             <dbl> <dbl>    <dbl>     <dbl>         <dbl>\n1 arima_seasonal    -392. 0.810        2         4           3  \n2 arima_log         -396. 0.853        1         6           3.5\n3 arima_auto        2082. 0.779        5         3           4  \n4 ets_damped        2472. 0.773        7         2           4.5\n5 ets_auto          2467. 0.834        6         5           5.5\n# â„¹ 6 more rows\n```\n\n\n:::\n:::\n\n\nThe `arima_seasonal` and `arima_log` models secure the top combined ranks, primarily due to their excellent AICc scores. This suggests that the correct mathematical specification ($ARIMA$ on log-transformed data) provides the most stable and parsimonious foundation for forecasting the volatile cocoa prices\n\n## Production Forecasting Workflow\n\nThe production workflow represents the culmination of al preceeding steps; data preparation to evaluation. This workflow is the operational blueprint for generating and ditributing the final, best-estimate forecast used for business planning and risk management.\n\n### Selecting the Final Model\n\nOur workflow begins by formally selecting the single best model determined by the robust Time Series Cross-Validation (TSCV) process â€“ the model with the lowest combined rank.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# select the final model \nfinal_model <- training_cocoa |> \n  model(\n    arima_seasonal = ARIMA(log(Price) ~ pdq(0,1,1) + PDQ(0,0,1)) # based on our evaluation\n    )\n```\n:::\n\n\nBased on the time series cross validation performed earlier $ARIMA(log(Price)~pdq(0,1,1)+PDQ(0,0,1))$ model was chosen as the most reliable, stable and parsimonious forecast. The model is fitted one last time to the entire available training data to ensure it uses the maximum possible history before generating the production forecast.\n\n### Generating the Production Forecast\n\nThe chosen model is used to project future values, and prediction intervals are calculated to quantify uncertainty\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate production forecasts for next 12 months\nproduction_forecast <- final_model |> \n  forecast(h = 12) |> \n  hilo()\n```\n:::\n\n\nWe use the `forecast()` function to generate point forecasts for the immediate next 12 periods (months), which is a common horizon for annual budgeting. The `hilo()` function calculates the 80% and 95% prediction intervals. These intervals are essential for management to assess risk; the 95% PI defines the expected worst and best-case scenarios for the cocoa prices.\n\n### Creating Production-Ready Output\n\nThe raw forecast object is cleaned and enriched with metadata for easy storage and reporting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create production ready output\nproduction_output <- production_forecast |> \n  as_tibble() |> \n  select(Date, .mean, `80%`, `95%`) |> \n  mutate(\n    Forecast_date = Sys.Date(),\n    Model_type = \"SARIMA\",\n    Model_version = \"1.0\"\n  )\n```\n:::\n\n\nHere, we select only the essential columns from the forecast object; the date, the point estimates (`.mean`) and the two prediction intervals. We then add vital metadata required for tracking and auditing;\n\n-   `Forecast_date`: To record when the forecast was run\n-   `Model_type`: To specify the type of model used\n-   `Model_version`: Important for version control, ensuring consistency in reporting over time\n\n### Creating Visualisations for Stakeholders\n\nThe final forecast must be communicated clearly, often requiring the visual overlay of the forecast onto the training data and, optionally, the held-out test data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualise for audience/stakeholders\nfinal_model |> forecast(h = \"3 years\") |> \n  autoplot(training_cocoa, level = 80) + \n  autolayer(testing_cocoa, Price, colour='red') +\n  labs(\n    title = \"Production Forecast: Monthy Cocoa Prices\",\n    subtitle = \"With 80% prediction intervals\",\n    y = \"Cocoa Price\"\n  ) +\n  scale_y_continuous(labels = scales::label_currency())\n```\n\n::: {.cell-output-display}\n![Forecast Production Workflow Visualisation](forecast-evaluation_files/figure-html/fig-prodflow-1.png){#fig-prodflow fig-align='center' width=672}\n:::\n:::\n\n\nFor visualisation purposes, we generate a 3-year forecast (extending beyond the 12-month target). The training history (black line), the forecast (blue line) and the held out test data (red line) are overlayed together on the same plot with an 80% PI (less conservative interval for visual communication). This allows stakeholders to immediately see how well the model would have performed on the most recent history.\n\n### Building a Validated Forecasting System\n\nThe ultimate goal\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# complete forecasting pipeline\nforecasting_pipeline <- function(data, horizon = 24){\n  \n  # Model training (simplified with less models)\n  models <- data |> \n    drop_na() |> \n    model(\n      ets = ETS(Price),\n      arima = ARIMA(log(Price) ~ pdq(0,1,1) + PDQ(0,0,1)),\n      s_naive = SNAIVE(Price)\n    )\n  \n  # Model evaluation\n  accuracy_metrics <- models |> \n    accuracy()\n  \n  # Model selection (simplified)\n  best_model_name <- accuracy_metrics |> \n    arrange(MASE) |> \n    slice(1) |> \n    pull(.model)\n  \n  best_model <- models |> \n    select(!!best_model_name)\n  \n  # Generate forecast\n  forecasts <- best_model |> \n    forecast(h = horizon) |> \n    hilo()\n  \n  # Extract key forecast details\n  next_period <- forecasts$.mean[1]\n  mid_term <- mean(forecasts$.mean[1:(horizon/2)])\n  long_term <- mean(forecasts$.mean[1:horizon])\n  \n  lower_80 <- forecasts$`80%`$lower\n  upper_80 <- forecasts$`80%`$upper\n  \n  # collapse report output into a single formatted string\n  model_report <- paste(\n    capture.output(report(best_model)),\n    collapse = \"\\n\"\n  )\n  \n  # Print descriptive summary\n  cat(\"\n    ðŸ“ˆ FORECAST SUMMARY\n    ================================\n    Selected Model: \", best_model_name, \"\n    \n    NEXT PERIOD FORECAST: \", round(next_period, 2), \"\n    \", horizon/2, \"-PERIOD AVERAGE: \", round(mid_term, 2), \"\n    \", horizon, \"-PERIOD AVERAGE: \", round(long_term, 2), \"\n    \n    80% CONFIDENCE INTERVAL:\n    Lower: \", round(lower_80, 2), \"\n    Upper: \", round(upper_80, 2), \"\n    \n    MODEL DETAILS:\n    \", model_report,\"\\n\")\n  \n  # Save forecast for record-keeping\n  write_csv(\n    forecasts |> as_tibble() |> \n      select(Date, .mean, `95%`, `80%`),\n    paste0(\"forecast_\", Sys.Date(), \".csv\")\n  )\n  \n  # return silently to avoid repeated printing\n  invisible(forecasts)\n}\n```\n:::\n\n\n## Final Validation Against Test Period\n\nThe ultimate test; how do our model(s) perform on the most recent completely unseen data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# test set evaluation\ntest_predictions <-  cocoa_model_portfolio |> \n  select(arima_seasonal, arima_log) |> \n  forecast(new_data = testing_cocoa)\n\n# Visualise test predictions\ntest_predictions |> \n  autoplot(testing_cocoa, level = NULL)\n```\n\n::: {.cell-output-display}\n![](forecast-evaluation_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# check accuracy metrics\ntest_predictions |> \n  accuracy(testing_cocoa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 10\n  .model         .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>          <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 arima_log      Test   317.  362.  320. 12.7   12.8   NaN   NaN 0.686\n2 arima_seasonal Test   248.  299.  254.  9.84  10.1   NaN   NaN 0.669\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "forecast-evaluation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}