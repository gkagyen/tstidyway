{
  "hash": "a1622be6d1d2382332b1603056c4b31b",
  "result": {
    "engine": "knitr",
    "markdown": "# Advanced Forecasting Models: ARIMA {#sec-arima}\n\nWhile ETS models excel at capturing smooth trends and seasonal patterns through exponential smoothing, ARIMA models take a fundamentally different approach. ARIMA (AutoRegressive Integrated Moving Average) models leverage the autocorrelation structure of your data–the relationship between observations at different time lags. Think of it as a model that learns how today's value relates to yesterday's, last week's and last month's values, then uses those relations to predict tomorrow.\n\nThe beauty of ARIMA in the `fable` framework is that it automates what was traditionally a complex, manual process of model identification, estimation, and diagnostic checking. What once required advanced statistical expertise now becomes accessible through intuitive functions that maintain the tidy workflow we have built throughout this book.\n\n\n\n\n\n## Understanding the ARIMA Framework\n\nARIMA models have three key components represented by the notation ARIMA(p,d,q):\n\n-   **AR(p) - AutoRegressive**: The autoregressive component is the *p-th* order dependence of the current observation on previous observations. It is simply how much each value depends on its immediate predecessors. This models the memory in the series.\n-   **I(d) - Integrated**: This component represents the number of times (d) the raw observation have been differenced to make the time series stationary. A stationary series as explained in @sec-checking-stationarity, is one whose statistical properties do not change.\n-   **MA(q) - Moving Average**: The moving average component is the *q-th* order dependence of the current observation on previous forecast errors (or residuals).\n\nFor practical application, the fable package can automatically select the best ARIMA configuration by minimising information criteria like AICc. We apply this to the cereal yield data (`training_yield`)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit automatic ARIMA to cereal yield data\nauto_arima <- training_yield |> \n  filter(!is.na(value)) |> \n  model(auto = ARIMA(value))\n\n# examine the selected model\nreport(auto_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: value \nModel: ARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.4196\ns.e.   0.1224\n\nsigma^2 estimated as 11674:  log likelihood=-280.26\nAIC=564.52   AICc=564.8   BIC=568.18\n```\n\n\n:::\n:::\n\n\nThe `ARIMA()` function is used to fit an ARIMA model in the `fable` framework, `ARIMA(value)` allows the algorithm to automatically select the best ARIMA model. The function tests multiple combinations of *p, d, q* parameters. It uses a stepwise algorithm to efficiently search the model space.\n\nThe output from the `report()` function shows the selected ARIMA order and estimated parameters. The automatic selection process chose the configuration **ARIMA(0,1,1)** for the cereal yield training data. This chosen configuration tells us exactly how the model accounts for the data's past and trend\n\n-   $p=0$ **(AutoRegressive)**: The model determined that, after applying a first order difference, there is no significant correlation between the current value and the previous lagged values of the differenced series\n-   $d=1$ **(Integrated)**: The model required a first-order difference (d=1) to achieve stationarity. This confirms the cereal yield series has a linear trend that must be removed before modelling the residual fluctuations.\n-   $q-1$ **(Moving Average)**: The model includes a first-order Moving Average term \\[MA(1)\\]. This means the current prediction depends on the forecast error from the immediate previous period.\n\nThe above model can be formally written as\n\n$$\\hat{y}_t=y_{t-1}+\\theta_1e_{t-1}+e_t$$\n\nWhere $y_t$ is the current observation, $\\hat{y}_t$ is the forecast, $e_t$ is the current error, $e_{t-1}$ is the previous error and $\\theta_1$ is the $MA(1)$ coefficient.\n\nThe estimated coefficients, residual variance and information criteria are also provided in the `report()` output. the $ma1$ coefficient (-0.4196) is the estimated value of $\\theta_1$ parameter. The negative sign suggests that the model uses the previous forecast error to adjust the current forecast in the opposite direction. The standard error ($s.e.$) of 0.1224 indicates the precision of this estimate.\n\n## ARIMA Model Components Demystified\n\n### AutoRegressive (AR) Component (p)\n\nThe AR part of an ARIMA model, as described earlier, captures the relationship between a current observation and a linear combination of its immediate predecessors. The current value, $y_t$, is expressed as a weighted sum of $p$ past values plus a random error term ($\\varepsilon_t$):\n\nEquation: $y_t=\\varphi_1y_{t-1}+\\varphi_2y_{t-2}+\\cdots+\\varphi_py_{t-p}+\\varepsilon_t$\n\nThe coefficients $\\varphi_1,\\varphi_2\\cdots,\\varphi_p$ quantify the strength and direction of the relationship with the lagged values.\n\nAn AR(1) model would simplify this to only the most recent predecessor:\n\n$\\text{Today's value}=\\text{constant}+\\varphi_1\\times\\text{yesterday's value}+\\text{random error}$\n\nIf $\\varphi_1$ is positive and high (e.g., 0.8), a high value yesterday strongly suggests a high value today.\n\n### Integrated (I) Component (d)\n\nThe Integrated (I) component handles the process of differencing the data to make the series stationary. Stationary is a requirement for **AR** and **MA** process to be statistically valid. The order $d$ specifies the number of times differencing must be applied:\n\nFirst Difference $(d=1)$: This removes a linear trend. It calculates the change from one period to the next:\n\n$$\n\\nabla y_t=y_t-y_{t-1}\n$$\\\nSecond Difference $(d=2)$: This removes an accelerating or non-linear trend. It calculates the change in the change (i.e. the acceleration) over time\n\n$$\n\\nabla^2 y_t=\\nabla y_t-\\nabla y_{t-1}\n$$\n\nWe have already seen how differencing our data is done in @sec-checking-stationarity.\n\n### Moving Average (MA) Component (q)\n\nThe Moving Average (MA) part models the relationship between an the current observation and previous forecast errors (or residuals). This component models the short-term shock or correction mechanisms in the series.\n\nThe current value, $y_t$, is expressed as a weighted sum of $q$ past error terms ($varepsilon_{t-1},\\cdots,\\varepsilon_{t-q}$) plus the current error ($\\varepsilon_t$)\n\nEquation: $y_t=\\varepsilon_t+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}$\n\nThe coefficients $\\theta_1,\\theta_2,\\cdots,\\theta_q$ quantify the magnitude of influence from the previous errors.\n\nAn MA(1) model would consider only the error from the most recent period: $\\text{Today's value}=\\text{constant}+\\theta\\times\\text{yesterday's forecast error}+\\text{random error}$\n\nThink of it like learning from mistakes - if you over-predicted yesterday, the model uses the coefficient $\\theta_1$ to adjust today's forecast, correcting the bias.\n\n## The ARIMA Modelling Workflow in `fable`\n\nThe ARIMA modelling process is systematic, beginning with diagnosis and preparation of the data before the fitting the final model. The standard workflow involves six steps. **Step 1: Visualize** → **Step 2: Check Stationarity** → **Step 3: Identify Model** → **Step 4: Fit** → **Step 5: Diagnose** → **Step 6: Forecast.** We already know how to visualise our time series data and how to ) difference our data from @sec-visualising-trends-with-ggplot2-and-autoplot and @sec-checking-stationarity respectively.\n\n### Visualise Time Series\n\nWe begin by first visualising our time series data, the training set of the cereal yield data (`training_yield`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntraining_yield |> \n  autoplot(value)\n```\n\n::: {.cell-output-display}\n![Visual Representation Of Annual Cereal Yield in Ghan from 1960 to 2007](advanced-models-2_files/figure-html/fig-view-ts-1.png){#fig-view-ts fig-align='center' width=672}\n:::\n:::\n\n\n### Check Stationarity\n\nA fundamental requirement for fitting an ARIMA model is that the series must be stationary, meaning its statistical properties like mean and variance remain constant over time. @sec-checking-stationarity talks more about this topic. The cereal yield series, due to its obvious trend is likely non stationary.\n\nThe autocorrelation function (ACF) plot is the first diagnostic tool for checking a series' stationarity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check staionarity visually with ACF plot\ntraining_yield |> \n  ACF(value, lag_max = 18) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![Autocorrelation Function Plot Annual Cereal Yield from 1960 to 2017](advanced-models-2_files/figure-html/fig-acf-cyeild-1.png){#fig-acf-cyeild fig-align='center' width=672}\n:::\n:::\n\n\nFor the cereal yield data, the bars of the ACF plot shows a gradual change or decay (slowly falling). This is a strong indicator of non-stationarity and confirms the existence of a trend in the time series data. This means the data needs differencing.\n\nTo statistically confirm non-stationarity, the KPSS unit root test (`unitroot_kpss`) is used.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# statistical test for stationarity\ntraining_yield |> \n  features(value, features = unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  indicator_name               kpss_stat kpss_pvalue\n  <chr>                            <dbl>       <dbl>\n1 Cereal yield _kg per hectare      1.00        0.01\n```\n\n\n:::\n:::\n\n\nThe small p-value (0.01) from the test suggests that we must reject the null hypothesis of stationarity. This statistically confirms the visual inspection; that the data is non-stationary and requires differencing (the I(d) component).\n\nWe can go ahead now to check how the differenced series and original series compare visually\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# visually compare original and differenced series\nk1 <- training_yield |> autoplot(value)\nk2 <- training_yield |> autoplot(difference(value))\nk1+k2\n```\n\n::: {.cell-output-display}\n![Visual Comparison of Differenced and Original Cereal Yield Series](advanced-models-2_files/figure-html/fig-stationary-1.png){#fig-stationary fig-align='center' width=672}\n:::\n:::\n\n\nThe differenced series plot (right) shows a stable series fluctuating around zero with no trend.\n\n### Identifying Model Order (p, d, q)\n\nWhile automatic fitting is possible, understanding how to manually determine the ARIMA order using ACF and PACF plots is crucial for advanced diagnostics.\n\nThe `fable` package can perform a thorough search of the model space and automatically choose the best components for the ARIMA model we fit.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit ARIMA for auto order selection\narima_model_auto <- training_yield |> \n  filter(!is.na(value)) |> \n  model(\n    stepwise_false = ARIMA(value, stepwise = FALSE), # more thorough search\n  )\n\nglance(arima_model_auto) |> \n  select(.model, AICc, ar_roots, ma_roots)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .model          AICc ar_roots  ma_roots \n  <chr>          <dbl> <list>    <list>   \n1 stepwise_false  565. <cpl [0]> <cpl [1]>\n```\n\n\n:::\n:::\n\n\n`stepwise = FALSE` performs a more exhaustive search (slower but more thorough). The best model is chosen based on the lowest AIC. The `glance()` provides a summary of the model's structure and fit statistics.\n\nWe can also specify the order of ARIMA to be used for the fitting. By observing the PACF and ACF plot, we can be guided on what the order for the ARIMA would be.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# look at ACF and PACF plots\ntraining_yield |> \n  gg_tsdisplay(value, plot_type = \"partial\")\n```\n\n::: {.cell-output-display}\n![Visual Inspection of Autocorrelation Plots To Determine ARIMA Order](advanced-models-2_files/figure-html/fig-pacf-acf-1-1.png){#fig-pacf-acf-1 fig-align='center' width=672}\n:::\n:::\n\n\nThe ACF plot (bottom left) shows a gradual decay, confirming the series is non-stationary (deeds differencing). The PACF shows a single large spike at lag 1, which then cuts off (falls immediately inside the confidence bounds). This behaviour strongly suggests a starting point of an AR(1).\n\nSince the series is non-stationary we will first difference it ($d=1$) and then re-examine the PACF and ACF plots to identify **p** and **q**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# look again at PACF and ACF plots after differencing\ntraining_yield |> \n  gg_tsdisplay(difference(value), plot_type = \"partial\")\n```\n\n::: {.cell-output-display}\n![Visual Inspection of the Autocorrelation Plots After Differencing To Determine ARIMA Order](advanced-models-2_files/figure-html/fig-pacf-acf-2-1.png){#fig-pacf-acf-2 fig-align='center' width=672}\n:::\n:::\n\n\nThe differenced series now shows stationarity (top plot), fluctuating around a constant mean (zero). The PACF and ACF plots both show a spike at lag 1 and then cuts off (fall within the significance bands). The behaviour of the two plots suggests a mixed ARIMA model with both **AR** and **MA** terms, specifically and **ARIMA(1,1,1)** or a related structure, as the behaviour suggests both an **AR(1)** and **MA(1)** model may be needed.\n\nThe ACF and PACF plots are the primary tools used to manually identify the appropriate orders for the AR(p) and MA(q) components of an ARIMA. The general guiding rule relies on observing how the correlations \"cut off\" (drop sharply to zero) in each plot after the series has been differenced to achieve stationarity.\n\n| Plot Behaviour | Indicates | ARIMA Components Implied |\n|------------------------|-------------------|-----------------------------|\n| ACF decays gradually | Non-stationarity | Needs differencing $(d\\ge1)$ |\n| ACF cuts off suddenly after lag q | Moving Average (MA) | Set $q$ to the lag where the cut-off occurs |\n| PACF cuts off suddenly after lag p | AutoRegressive (AR) | Set $p$ to the lag where the cut-off occurs |\n| Both ACF and PACF decay gradually | Mixed Model | Set both $p\\ge1$ and $q\\ge1$ (e.g., ARIMA(1,$d$,1)). |\n\nThe final model chosen by the automatic function, **ARIMA(0,1,1),** is often the result of the automatic routine simplifying this complex **ARIMA(1,1,1)** pattern into a more parsimonious **MA** model.\n\n### Fitting the Model: Manual ARIMA Specification\n\nOnce the components are manually determined we can now fit the model using these identified components\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# specify ARIMA model manually\nmanual_arima <- training_yield |> \n  drop_na() |> \n  model(\n    arima_111 = ARIMA(value ~ pdq(1,1,1))\n  )\n\nreport(manual_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: value \nModel: ARIMA(1,1,1) \n\nCoefficients:\n          ar1      ma1\n      -0.1579  -0.2938\ns.e.   0.3075   0.2939\n\nsigma^2 estimated as 11871:  log likelihood=-280.14\nAIC=566.27   AICc=566.84   BIC=571.76\n```\n\n\n:::\n:::\n\n\n### Model Diagnostic and Residual Analysis\n\nAfter an ARIMA model is fitted, the final and most crucial step before forecasting is diagnostic checking of the residuals (the forecast errors). A well-specified ARIMA model should capture all the underlying structure (AR, I, MA) in the data, leaving only white noise as the residual error. White noise has three key properties: the errors are uncorrelated, have zero mean and a constant variance.\n\n**Visual Residual Check**\n\nThe `gg_tsresiduals()` function from the `ggtime` package is able to generate a three-part diagnostic plot for the residuals (labelled as `.innov` or \"innovation residual\" in the plot)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visual residual check\nmanual_arima |> \n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![ARIMA Model Residual DIagnostic Plot](advanced-models-2_files/figure-html/fig-diag-arima-1.png){#fig-diag-arima fig-align='center' width=672}\n:::\n:::\n\n\nThe resulting plot from the `gg_tsresiduals()` function provides the following checks\n\n-   A plot of the residuals over time (top). A good model will show the residuals fluctuating randomly around zero, with no noticeable patterns or trends. The plot confirms the residuals are centred near zero and appear randomly scattered between -300 and 300\n-   ACF plot (bottom left) that checks for autocorrelation in the residuals. If the residuals are white noise, all correlation bars should fall within the blue dashed confidence bounds (or be very close to zero). The plot shows no significant spikes remaining, suggesting the ARIMA model successfully captured the dependence structure (AR and MA)\n-   A histogram (bottom left) that shows the distribution of the residuals. Ideally, residuals should be normally distributed (bell-shaped) with a mean near zero. The histogram shows a distribution that is roughly centred near zero but may not be perfectly symmetrical.\n\n**Statistical Residual Check**\n\nThe Ljung-Box test is a formal statistical method to confirm that the residuals, as a group, are not distinguishable from white noise.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(manual_arima) |> \n  features(.innov, features = ljung_box, lag = 16)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  indicator_name               .model    lb_stat lb_pvalue\n  <chr>                        <chr>       <dbl>     <dbl>\n1 Cereal yield _kg per hectare arima_111    12.7     0.695\n```\n\n\n:::\n:::\n\n\nThe innovation residuals (`.innov`) are extracted from the model with `augment()` and the Ljung-Box test (`ljung_box`) is applied to them via the `features()` function. `lag=16` tells the Ljung-Box test to check for serial correlation across the first **16 lags**.\n\nThe null hypothesis ($H_0$) for this test is that the residuals are independently distributed (i.e., they are white noise). Since the p-value (0.69) is greater than 0.05, we fail to reject the null hypothesis. The test confirms that the residuals are statistically good, meaning the ARIMA model is well-specified.\n\n**Normality Check (Q-Q Plot)**\n\nWe can further confirm if the residuals follow a normal distribution with a Q-Q plot. The Quantile-Quantile (Q-Q) plot is used to visually assess the assumption that the residuals follow a normal distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(manual_arima) |> \n  ggplot(aes(sample = .innov)) +\n  stat_qq() +\n  stat_qq_line()\n```\n\n::: {.cell-output-display}\n![QUantile Quantile Plot of ARIMA Model Residuals](advanced-models-2_files/figure-html/fig-qq-arima-1.png){#fig-qq-arima fig-align='center' width=672}\n:::\n:::\n\n\n`ggplot(aes(sample = .innov))` sets up the plot to compare the residuals (`.innov`) against the theoretical quantiles of a normal distribution. `stat_qq()` plots the residual quantiles as points and `stat_qq_line()` draws a straight line that represents a perfect normal distribution.\n\nThe plot shows that for a large portion of the residuals (the points near the middle), they closely follow the straight line. However, the points in the tails deviates noticeable from the line, especially on the upper right. This suggests the residuals may have heavier tails than a pure normal distribution, but for many forecasting purposes, this minor deviation may be tolerated if the Ljung-Box test passes.\n\n### Generating ARIMA Forecasts\n\nOnce the ARIMA model has passed the diagnostic checks - confirming its residuals are white noise - it can be used to generate reliable forecasts. We can use the fitted ARIMA(1,1,1) model (`manual_arima`) on the cereal yield training set to project future values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate forecasts\narima_forecast <- manual_arima |> \n  forecast(h = 8)\n\n# examine forecast details\nforecast_details <- arima_forecast |> \n  hilo() |> as_tibble()\n\nforecast_details |> \n  select(year, .mean, `95%`)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 3\n   year .mean                  `95%`\n  <dbl> <dbl>                 <hilo>\n1  2008 1336. [1122.015, 1549.114]95\n2  2009 1333. [1089.092, 1576.174]95\n3  2010 1333. [1055.932, 1610.260]95\n4  2011 1333. [1026.923, 1639.122]95\n5  2012 1333. [1000.359, 1665.710]95\n# ℹ 3 more rows\n```\n\n\n:::\n:::\n\n\nThe `forecast()` function is used to project the series forward by $h=8$ years. The resulting forecasts show the projected mean value and a 95% confidence intervals. The model suggests the current yield value depends on $d=1$ (Integrated; the previous year's yield (to model the trend), $p=1$ (AR); the previous year's differenced value and $q=1$ (MA); the previous year's forecast error.\n\nThe point forecasts (`.mean`) quickly stabilise to a constant value of 1333. The AR and MA components only influence the forecast horizon for a short period (typically 1-2 steps ahead), after which the forecast relies solely on the Integrated component. Since $d=1$ (first difference), the long term forecast for undifferenced series becomes a flat line (i.e, the rate of change is projected to settle at zero). The 95% confidence interval demonstrates increasing uncertainty, widening significantly from \\~427 units in 2008 to \\~804 units by 2015.\n\n### Visualising ARIMA Forecasts\n\nVisualising the forecast provides a useful assessment of the model's historical fit and its long-term projections\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create comprehensive forecast visualisation\narima_forecast |> \n  autoplot(training_yield, level = 95) +\n  autolayer(manual_arima |> augment(), .fitted, colour = \"red\") +\n  labs(title = \"ARIMA(1,1,1) Model Forecast\")\n```\n\n::: {.cell-output-display}\n![Forecasting With ARIMA(1,1,1) Model](advanced-models-2_files/figure-html/fig-arima-forecast-1.png){#fig-arima-forecast fig-align='center' width=672}\n:::\n:::\n\n\nThe historical fit (red line) from the resulting visuals tracks the general upward movement and fluctuations of the training data (black line), demonstrating that the ARIMA(1,1,1) model successfully captured the overall trend and short-term correlations.\n\nThe forecast line (blue line) stabilises and flattens out to a horizontal trajectory. This flattening is characteristic of ARIMA models with $d=1$ (first differencing) when projecting far into the future, as the underlying MA and AR influence eventually dies out. The blue shaded region visually confirms the increasing uncertainty, tapering outward.\n\n## Seasonal ARIMA Models\n\nFor data with seasonal patterns, the standard ARIMA model must be extended to account for correlation the seasonal lags. This is achieved through the Seasonal ARIMA (SARIMA) model.\n\n### Understanding the $SARIMA$ Notation\n\nA seasonal ARIMA model icludes both non-seasonal and seasonal components, represented by the notation $\\text{ARIMA(p,d,q)(P,D,Q)[m]}$.\n\n-   $(p,d,q)$ is the non-seasonal order that captures short term dependencies and trend removal as in standard ARIMA\n\n-   $(P,D,Q)$ is the seasonal order that captures the dependence on values and errors that occurred $m$ periods ago (i.e., in the same season last year).\n\n-   $[m]$ is the number of periods per season (e.g, $m=4$ for quarterly data, $m=12$ for monthly data)\n\n### Demonstrating the $SARIMA$ Workflow\n\nWe use the Australian quarterly cement production series to demonstrate the complete SARIMA workflow.\n\n**Visualise**\n\nWe start with a visual identification of seasonal patterns. Quarterly data means the seasonal period $m = 4$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# SARIMA workflow with the cement production data\n# visualise seasons\nlibrary(patchwork)\np1 <- training_cement |> \n  gg_season(Cement, period = \"2y\", show.legend = FALSE) +\n  labs(title = \"Bi-Annual Seasonal Pattern\")\n\n# subseries plot\np2 <- training_cement |> \n  gg_subseries(Cement) +\n  labs(title = \"Seasonal Subseries\")\n\np3 <- training_cement |> \n  model(STL(Cement)) |> \n  components() |> autoplot() +\n  labs(subtitle = NULL)\n\np1 + p2 + p3 + \n  plot_layout(\n    design = \"\n    AAABBB\n    #CCCC#\n    \"\n)\n```\n\n::: {.cell-output-display}\n![Visual Ensemble of Seasonal Plots from The Australian Quarterly Cement Production Series](advanced-models-2_files/figure-html/fig-season-plot-1.png){#fig-season-plot fig-align='center' width=672}\n:::\n:::\n\n\nWe produce three key plots to visualise and decompose the seasonality. The seasonal plot (top left) shows the data broken down by seasonal cycles, which helps to identify the nature of the seasonality. The lines representing different years run parallel to each other, but the level of the entire series is clearly increasing over time. The lines a grouped by a two-year period.\n\nThe seasonal subseries plot (top right) displays the time series for each individual quarter over the entire time period. Each panel shows the trend within that specific quarter. Every quarter shows a strong upward trend over the years. The blue horizontal line in each panel represents the average production for that quarter. The plot confirms that quarter 3 (Q3) and quarter 4 (Q4) consistently have the highest production levels.\n\nThe third plot at the bottom is an STL decomposed visuals of the time series, breaking it into its core components - a trend, a seasonal and remainder components. The overall visuals confirm the quarterly cement series requires a model that handles both non-seasonal trend removal ($likely \\d=1$) and seasonal differences ($D=1$) to remove the strong seasonal pattern.\n\n**Check Stationarity**\n\nBefore fitting the SARIMA model we must address the non-stationarity identified from the visual inspection (strong upward trend and fixed seasonal pattern)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check stationarity\ntraining_cement |> \n  features(Cement, features = unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      <dbl>       <dbl>\n1      3.09        0.01\n```\n\n\n:::\n:::\n\n\nWe use the KPSS unit root test to statistically confirm the presence of a deterministic trend, which causes stationarity. The p-values (0.01) is less than 0.05, leading us to reject the null hypothesis of stationarity. This is a clear indication that the series is non-stationary, requiring differencing.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# apply seaonal and first differencing \ntraining_cement |> \n  gg_tsdisplay(\n    difference(Cement) |> difference(lag=4), \n    plot_type = \"partial\"\n    )\n```\n\n::: {.cell-output-display}\n![](advanced-models-2_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe initial visualisation showed two types of non-stationarity; a strong upward trend and a fixed seasonal pattern. To remove both we apply two types of differencing; a first order differencing $(d=1)$ `difference(Cement)` to remove the linear trend and a seasonal differencing $(D=1)$ `... differencing(lag = 4)` to remove the seasonality (\\$m=4\\$ for quarterly data)\n",
    "supporting": [
      "advanced-models-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}