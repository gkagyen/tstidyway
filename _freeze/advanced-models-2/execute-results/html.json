{
  "hash": "92efe0bdb1e623182fb8ad2907194753",
  "result": {
    "engine": "knitr",
    "markdown": "# Advanced Forecasting Models: ARIMA {#sec-arima}\n\nWhile ETS models excel at capturing smooth trends and seasonal patterns through exponential smoothing, ARIMA models take a fundamentally different approach. ARIMA (AutoRegressive Integrated Moving Average) models leverage the autocorrelation structure of your data–the relationship between observations at different time lags. Think of it as a model that learns how today's value relates to yesterday's, last week's and last month's values, then uses those relations to predict tomorrow.\n\nThe beauty of ARIMA in the `fable` framework is that it automates what was traditionally a complex, manual process of model identification, estimation, and diagnostic checking. What once required advanced statistical expertise now becomes accessible through intuitive functions that maintain the tidy workflow we have built throughout this book.\n\n\n\n\n\n## Understanding the ARIMA Framework\n\nARIMA models have three key components represented by the notation ARIMA(p,d,q):\n\n-   **AR(p) - AutoRegressive**: The autoregressive component is the *p-th* order dependence of the current observation on previous observations. It is simply how much each value depends on its immediate predecessors. This models the memory in the series.\n-   **I(d) - Integrated**: This component represents the number of times (d) the raw observation have been differenced to make the time series stationary. A stationary series as explained in @sec-checking-stationarity, is one whose statistical properties do not change.\n-   **MA(q) - Moving Average**: The moving average component is the *q-th* order dependence of the current observation on previous forecast errors (or residuals).\n\nFor practical application, the fable package can automatically select the best ARIMA configuration by minimising information criteria like AICc. We apply this to the cereal yield data (`training_yield`)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit automatic ARIMA to cereal yield data\nauto_arima <- training_yield |> \n  filter(!is.na(value)) |> \n  model(auto = ARIMA(value))\n\n# examine the selected model\nreport(auto_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: value \nModel: ARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.4196\ns.e.   0.1224\n\nsigma^2 estimated as 11674:  log likelihood=-280.26\nAIC=564.52   AICc=564.8   BIC=568.18\n```\n\n\n:::\n:::\n\n\nThe `ARIMA()` function is used to fit an ARIMA model in the `fable` framework, `ARIMA(value)` allows the algorithm to automatically select the best ARIMA model. The function tests multiple combinations of *p, d, q* parameters. It uses a stepwise algorithm to efficiently search the model space.\n\nThe output from the `report()` function shows the selected ARIMA order and estimated parameters. The automatic selection process chose the configuration **ARIMA(0,1,1)** for the cereal yield training data. This chosen configuration tells us exactly how the model accounts for the data's past and trend.\n\n-   $p=0$ **(AutoRegressive)**: The model determined that, after applying a first order difference, there is no significant correlation between the current value and the previous lagged values of the differenced series.\n-   $d=1$ **(Integrated)**: The model required a first-order difference (d=1) to achieve stationarity. This confirms the cereal yield series has a linear trend that must be removed before modelling the residual fluctuations.\n-   $q=1$ **(Moving Average)**: The model includes a first-order Moving Average term \\[MA(1)\\]. This means the current prediction depends on the forecast error from the immediate previous period.\n\nThe above model can be formally written as\n\n$$\\hat{y}_t=y_{t-1}+\\theta_1e_{t-1}+e_t$$\n\nWhere $y_t$ is the current observation, $\\hat{y}_t$ is the forecast, $e_t$ is the current error, $e_{t-1}$ is the previous error and $\\theta_1$ is the $MA(1)$ coefficient.\n\nThe estimated coefficients, residual variance and information criteria are also provided in the `report()` output. the $ma1$ coefficient (-0.4196) is the estimated value of $\\theta_1$ parameter. The negative sign suggests that the model uses the previous forecast error to adjust the current forecast in the opposite direction. The standard error ($s.e.$) of 0.1224 indicates the precision of this estimate.\n\n## ARIMA Model Components Demystified\n\n### AutoRegressive (AR) Component (p)\n\nThe AR part of an ARIMA model, as described earlier, captures the relationship between a current observation and a linear combination of its immediate predecessors. The current value, $y_t$, is expressed as a weighted sum of $p$ past values plus a random error term ($\\varepsilon_t$):\n\nEquation: $y_t=\\varphi_1y_{t-1}+\\varphi_2y_{t-2}+\\cdots+\\varphi_py_{t-p}+\\varepsilon_t$\n\nThe coefficients $\\varphi_1,\\varphi_2\\cdots,\\varphi_p$ quantify the strength and direction of the relationship with the lagged values.\n\nAn AR(1) model would simplify this to only the most recent predecessor:\n\n$\\text{Today's value}=\\text{constant}+\\varphi_1\\times\\text{yesterday's value}+\\text{random error}$\n\nIf $\\varphi_1$ is positive and high (e.g., 0.8), a high value yesterday strongly suggests a high value today.\n\n### Integrated (I) Component (d)\n\nThe Integrated (I) component handles the process of differencing the data to make the series stationary. Stationary is a requirement for **AR** and **MA** process to be statistically valid. The order $d$ specifies the number of times differencing must be applied:\n\nFirst Difference $(d=1)$: This removes a linear trend. It calculates the change from one period to the next:\n\n$$\n\\nabla y_t=y_t-y_{t-1}\n$$\\\nSecond Difference $(d=2)$: This removes an accelerating or non-linear trend. It calculates the change in the change (i.e. the acceleration) over time\n\n$$\n\\nabla^2 y_t=\\nabla y_t-\\nabla y_{t-1}\n$$\n\nWe have already seen how differencing our data is done in @sec-checking-stationarity.\n\n### Moving Average (MA) Component (q)\n\nThe Moving Average (MA) part models the relationship between an the current observation and previous forecast errors (or residuals). This component models the short-term shock or correction mechanisms in the series.\n\nThe current value, $y_t$, is expressed as a weighted sum of $q$ past error terms ($varepsilon_{t-1},\\cdots,\\varepsilon_{t-q}$) plus the current error ($\\varepsilon_t$)\n\nEquation: $y_t=\\varepsilon_t+\\theta_1\\varepsilon_{t-1}+\\theta_2\\varepsilon_{t-2}+\\cdots+\\theta_q\\varepsilon_{t-q}$\n\nThe coefficients $\\theta_1,\\theta_2,\\cdots,\\theta_q$ quantify the magnitude of influence from the previous errors.\n\nAn MA(1) model would consider only the error from the most recent period: $\\text{Today's value}=\\text{constant}+\\theta\\times\\text{yesterday's forecast error}+\\text{random error}$\n\nThink of it like learning from mistakes - if you over-predicted yesterday, the model uses the coefficient $\\theta_1$ to adjust today's forecast, correcting the bias.\n\n## The ARIMA Modelling Workflow in `fable`\n\nThe ARIMA modelling process is systematic, beginning with diagnosis and preparation of the data before the fitting the final model. The standard workflow involves six steps. **Step 1: Visualize** → **Step 2: Check Stationarity** → **Step 3: Identify Model** → **Step 4: Fit** → **Step 5: Diagnose** → **Step 6: Forecast**.\n\n### Visualise Time Series\n\nWe begin by first visualising our time series data, the training set of the cereal yield data (`training_yield`). More information on visualising time series `tsibbles` can be found in @sec-visualising-trends-with-ggplot2-and-autoplot\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntraining_yield |> \n  autoplot(value)\n```\n\n::: {.cell-output-display}\n![Visual Representation Of Annual Cereal Yield in Ghana from 1960 to 2007](advanced-models-2_files/figure-html/fig-view-ts-1.png){#fig-view-ts fig-align='center' width=672}\n:::\n:::\n\n\n### Check Stationarity\n\nA fundamental requirement for fitting an ARIMA model is that the series must be stationary, meaning its statistical properties like mean and variance remain constant over time. @sec-checking-stationarity talks more about this topic. The cereal yield series, due to its obvious trend is likely non stationary.\n\nThe autocorrelation function (ACF) plot is the first diagnostic tool for checking a series' stationarity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check stationarity visually with ACF plot\ntraining_yield |> \n  ACF(value, lag_max = 18) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![Autocorrelation Function Plot of Annual Cereal Yield from 1960 to 2007](advanced-models-2_files/figure-html/fig-acf-cyeild-1.png){#fig-acf-cyeild fig-align='center' width=672}\n:::\n:::\n\n\nFor the cereal yield data, the bars of the ACF plot shows a gradual change or decay (slowly falling). This is a strong indicator of non-stationarity and confirms the existence of a trend in the time series data. This means the data needs differencing.\n\nTo statistically confirm non-stationarity, the KPSS unit root test (`unitroot_kpss`) is used.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# statistical test for stationarity\ntraining_yield |> \n  features(value, features = unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  indicator_name               kpss_stat kpss_pvalue\n  <chr>                            <dbl>       <dbl>\n1 Cereal yield _kg per hectare      1.00        0.01\n```\n\n\n:::\n:::\n\n\nThe small p-value (0.01) from the test suggests that we must reject the null hypothesis of stationarity. This statistically confirms the visual inspection; that the data is non-stationary and requires differencing (the I(d) component).\n\nWe can go ahead now to check how the differenced series and original series compare visually\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# visually compare original and differenced series\nk1 <- training_yield |> autoplot(value)\nk2 <- training_yield |> autoplot(difference(value))\nk1+k2\n```\n\n::: {.cell-output-display}\n![Visual Comparison of Differenced and Original Cereal Yield Series](advanced-models-2_files/figure-html/fig-stationary-1.png){#fig-stationary fig-align='center' width=672}\n:::\n:::\n\n\nThe differenced series plot (right) shows a stable series fluctuating around zero with no trend.\n\n### Identifying Model Order (p, d, q)\n\nWhile automatic fitting is possible, understanding how to manually determine the ARIMA order using ACF and PACF plots is crucial for advanced diagnostics.\n\nThe `fable` package provides room to manually specify the order in the `ARIMA()` function. Before we manually determine and fit the manual ARIMA model, It is important to note that `fable` can also perform a thorough search of the model space for the automatic model selection by specifying `stepwise = FALSE` in the `ARIMA()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit ARIMA for auto order selection\narima_model_auto <- training_yield |> \n  filter(!is.na(value)) |> \n  model(\n    stepwise_false = ARIMA(value, stepwise = FALSE), # more thorough search\n  )\n\nglance(arima_model_auto) |> \n  select(.model, AICc, ar_roots, ma_roots)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .model          AICc ar_roots  ma_roots \n  <chr>          <dbl> <list>    <list>   \n1 stepwise_false  565. <cpl [0]> <cpl [1]>\n```\n\n\n:::\n:::\n\n\n`stepwise = FALSE` performs a more exhaustive search (slower but more thorough). The best model is chosen based on the lowest AICc. The `glance()` provides a summary of the model's structure and fit statistics.\n\nWe intend to manually specify the order of ARIMA to be used for the fitting. By observing the PACF and ACF plot, we can be guided on what the order for the ARIMA would be.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# look at ACF and PACF plots\ntraining_yield |> \n  gg_tsdisplay(value, plot_type = \"partial\")\n```\n\n::: {.cell-output-display}\n![Visual Inspection of Autocorrelation Plots To Determine ARIMA Order](advanced-models-2_files/figure-html/fig-pacf-acf-1-1.png){#fig-pacf-acf-1 fig-align='center' width=672}\n:::\n:::\n\n\nThe ACF plot (bottom left) shows a gradual decay, confirming the series is non-stationary (requires differencing). The PACF shows a single large spike at lag 1, which then cuts off (falls immediately inside the confidence bounds). This behaviour strongly suggests a starting point of an AR(1).\n\nSince the series is non-stationary we will first difference it ($d=1$) and then re-examine the PACF and ACF plots to identify **p** and **q**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# look again at PACF and ACF plots after differencing\ntraining_yield |> \n  gg_tsdisplay(difference(value), plot_type = \"partial\")\n```\n\n::: {.cell-output-display}\n![Visual Inspection of the Autocorrelation Plots After Differencing To Determine ARIMA Order](advanced-models-2_files/figure-html/fig-pacf-acf-2-1.png){#fig-pacf-acf-2 fig-align='center' width=672}\n:::\n:::\n\n\nThe differenced series now shows stationarity (top plot), fluctuating around a constant mean (zero). The PACF and ACF plots both show a spike at lag 1 and then cuts off (fall within the significance bands). The behaviour of the two plots suggests a mixed ARIMA model with both **AR** and **MA** terms, specifically an **ARIMA(1,1,1)** or a related structure, as the behaviour suggests both an **AR(1)** and **MA(1)** model may be needed.\n\nThe ACF and PACF plots are the primary tools used to manually identify the appropriate orders for the AR(p) and MA(q) components of an ARIMA. The general guiding rule relies on observing how the correlations \"cut off\" (drop sharply to zero) in each plot after the series has been differenced to achieve stationarity.\n\n| Plot Behaviour | Indicates | ARIMA Components Implied |\n|------------------------|-------------------|-----------------------------|\n| ACF decays gradually | Non-stationarity | Needs differencing $(d\\ge1)$ |\n| ACF cuts off suddenly after lag q | Moving Average (MA) | Set $q$ to the lag where the cut-off occurs |\n| PACF cuts off suddenly after lag p | AutoRegressive (AR) | Set $p$ to the lag where the cut-off occurs |\n| Both ACF and PACF decay gradually | Mixed Model | Set both $p\\ge1$ and $q\\ge1$ (e.g., ARIMA(1,$d$,1)). |\n\nThe final model chosen by the automatic function, **ARIMA(0,1,1),** is often the result of the automatic routine simplifying this complex **ARIMA(1,1,1)** pattern into a more parsimonious **MA** model.\n\n### Fitting the Model: Manual ARIMA Specification\n\nOnce the components are determined through the ACF and PACF plots, we can now fit the model by manually specifying the order inside `pdq()` within the `ARIMA()`function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# specify ARIMA model manually\nmanual_arima <- training_yield |> \n  drop_na() |> \n  model(\n    arima_111 = ARIMA(value ~ pdq(1,1,1))\n  )\n\nreport(manual_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: value \nModel: ARIMA(1,1,1) \n\nCoefficients:\n          ar1      ma1\n      -0.1579  -0.2938\ns.e.   0.3075   0.2939\n\nsigma^2 estimated as 11871:  log likelihood=-280.14\nAIC=566.27   AICc=566.84   BIC=571.76\n```\n\n\n:::\n:::\n\n\n### Model Diagnostic and Residual Analysis\n\nAfter an ARIMA model is fitted, the final and most crucial step before forecasting is diagnostic checking of the residuals (the forecast errors). A well-specified ARIMA model should capture all the underlying structure (AR, I, MA) in the data, leaving only white noise as the residual error. White noise has three key properties: the errors are uncorrelated, have zero mean and a constant variance.\n\n**Visual Residual Check**\n\nThe `gg_tsresiduals()` function from the `ggtime` package is able to generate a three-part diagnostic plot for the residuals (labelled as `.innov` or \"innovation residual\" in the plot)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visual residual check\nmanual_arima |> \n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![ARIMA Model Residual DIagnostic Plot](advanced-models-2_files/figure-html/fig-diag-arima-1.png){#fig-diag-arima fig-align='center' width=672}\n:::\n:::\n\n\nThe resulting plot from the `gg_tsresiduals()` function provides the following checks\n\n-   A plot of the residuals over time (top). A good model will show the residuals fluctuating randomly around zero, with no noticeable patterns or trends. The plot confirms the residuals are centred near zero and appear randomly scattered between -300 and 300\n-   An ACF plot (bottom left) that checks for autocorrelation in the residuals. If the residuals are white noise, all correlation bars should fall within the blue dashed confidence bounds (or be very close to zero). The plot shows no significant spikes remaining, suggesting the ARIMA model successfully captured the dependence structure (AR and MA)\n-   A histogram (bottom left) that shows the distribution of the residuals. Ideally, residuals should be normally distributed (bell-shaped) with a mean near zero. The histogram shows a distribution that is roughly centred near zero but may not be perfectly symmetrical.\n\n**Statistical Residual Check**\n\nThe Ljung-Box test is a formal statistical method to confirm that the residuals, as a group, are not distinguishable from white noise.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# statictical residual check\naugment(manual_arima) |> \n  features(.innov, features = ljung_box, lag = 16)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  indicator_name               .model    lb_stat lb_pvalue\n  <chr>                        <chr>       <dbl>     <dbl>\n1 Cereal yield _kg per hectare arima_111    12.7     0.695\n```\n\n\n:::\n:::\n\n\nThe innovation residuals (`.innov`) are extracted from the model with `augment()` and the Ljung-Box test (`ljung_box`) is applied to them via the `features()` function. `lag=16` tells the Ljung-Box test to check for serial correlation across the first **16 lags**.\n\nThe null hypothesis ($H_0$) for this test is that the residuals are independently distributed (i.e., they are white noise). Since the p-value (0.69) is greater than 0.05, we fail to reject the null hypothesis. The test confirms that the residuals are statistically good, meaning the ARIMA model is well-specified.\n\n**Normality Check (Q-Q Plot)**\n\nWe can further confirm if the residuals follow a normal distribution with a Q-Q plot. The Quantile-Quantile (Q-Q) plot is used to visually assess the assumption that the residuals follow a normal distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(manual_arima) |> \n  ggplot(aes(sample = .innov)) +\n  stat_qq() +\n  stat_qq_line()\n```\n\n::: {.cell-output-display}\n![QUantile Quantile Plot of ARIMA Model's Residuals](advanced-models-2_files/figure-html/fig-qq-arima-1.png){#fig-qq-arima fig-align='center' width=672}\n:::\n:::\n\n\n`ggplot(aes(sample = .innov))` sets up the plot to compare the residuals (`.innov`) against the theoretical quantiles of a normal distribution. `stat_qq()` plots the residual quantiles as points and `stat_qq_line()` draws a straight line that represents a perfect normal distribution.\n\nThe plot shows that for a large portion of the residuals (the points near the middle), they closely follow the straight line. However, the points in the tails deviates noticeable from the line, especially on the upper right. This suggests the residuals may have heavier tails than a pure normal distribution, but for many forecasting purposes, this minor deviation may be tolerated if the Ljung-Box test passes.\n\n### Generating ARIMA Forecasts\n\nOnce the ARIMA model has passed the diagnostic checks - confirming its residuals are white noise - it can be used to generate reliable forecasts. We can use the fitted ARIMA(1,1,1) model (`manual_arima`) on the cereal yield training set to project future values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate forecasts\narima_forecast <- manual_arima |> \n  forecast(h = 8)\n\n# examine forecast details\nforecast_details <- arima_forecast |> \n  hilo() |> as_tibble()\n\nforecast_details |> \n  select(year, .mean, `95%`)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 3\n   year .mean                  `95%`\n  <dbl> <dbl>                 <hilo>\n1  2008 1336. [1122.015, 1549.114]95\n2  2009 1333. [1089.092, 1576.174]95\n3  2010 1333. [1055.932, 1610.260]95\n4  2011 1333. [1026.923, 1639.122]95\n5  2012 1333. [1000.359, 1665.710]95\n# ℹ 3 more rows\n```\n\n\n:::\n:::\n\n\nThe `forecast()` function is used to project the series forward by $h=8$ (8 years). The resulting forecasts show the projected mean value and a 95% confidence intervals. The model suggests the current yield value depends on $d=1$ (Integrated; the previous year's yield (to model the trend), $p=1$ (AR); the previous year's differenced value and $q=1$ (MA); the previous year's forecast error.\n\nThe point forecasts (`.mean`) quickly stabilise to a constant value of 1333. The AR and MA components only influence the forecast horizon for a short period (typically 1-2 steps ahead), after which the forecast relies solely on the Integrated component. Since $d=1$ (first difference), the long term forecast for undifferenced series becomes a flat line (i.e, the rate of change is projected to settle at zero). The 95% confidence interval demonstrates increasing uncertainty, widening significantly from \\~427 units in 2008 to \\~804 units by 2015.\n\n### Visualising ARIMA Forecasts\n\nVisualising the forecast provides a useful assessment of the model's historical fit and its long-term projections\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create comprehensive forecast visualisation\narima_forecast |> \n  autoplot(training_yield, level = 95) +\n  autolayer(manual_arima |> augment(), .fitted, colour = \"red\") +\n  labs(title = \"ARIMA(1,1,1) Model Forecast\")\n```\n\n::: {.cell-output-display}\n![Forecasting With ARIMA(1,1,1) Model](advanced-models-2_files/figure-html/fig-arima-forecast-1.png){#fig-arima-forecast fig-align='center' width=672}\n:::\n:::\n\n\nThe historical fit (red line) from the resulting visuals tracks the general upward movement and fluctuations of the training data (black line), demonstrating that the ARIMA(1,1,1) model successfully captured the overall trend and short-term correlations.\n\nThe forecast line (blue line) stabilises and flattens out to a horizontal trajectory. This flattening is characteristic of ARIMA models with $d=1$ (first differencing) when projecting far into the future, as the underlying MA and AR influence eventually dies out. The blue shaded region visually confirms the increasing uncertainty, tapering outward as the forecast moves into later years.\n\n## Seasonal ARIMA Models\n\nFor data with seasonal patterns, the standard ARIMA model must be extended to account for correlation the seasonal lags. This is achieved through the Seasonal ARIMA (SARIMA) model.\n\n### Understanding the $SARIMA$ Notation\n\nA seasonal ARIMA model includes both non-seasonal and seasonal components, represented by the notation $\\text{ARIMA(p,d,q)(P,D,Q)[m]}$.\n\n-   $(p,d,q)$ is the non-seasonal order that captures short term dependencies and trend removal as in standard ARIMA\n\n-   $(P,D,Q)$ is the seasonal order that captures the dependence on values and errors that occurred $m$ periods ago (i.e., in the same season last year).\n\n-   $[m]$ is the number of periods per season (e.g, $m=4$ for quarterly data, $m=12$ for monthly data)\n\n### Demonstrating the $SARIMA$ Workflow\n\nWe use the Australian quarterly cement production series to demonstrate the complete SARIMA workflow.\n\n**Visualise**\n\nWe start with a visual identification of seasonal patterns. Quarterly data means the seasonal period $m = 4$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# SARIMA workflow with the cement production data\n# visualise seasons\nlibrary(patchwork)\np1 <- training_cement |> \n  gg_season(Cement, period = \"2y\", show.legend = FALSE) +\n  labs(title = \"Bi-Annual Seasonal Pattern\") + scale_x_yearquarter(breaks = \"6 months\")\n\n# sub-series plot\np2 <- training_cement |> \n  gg_subseries(Cement) +\n  labs(title = \"Seasonal Subseries\")\n\n# STL decomposed plots\np3 <- training_cement |> \n  model(STL(Cement)) |> \n  components() |> autoplot() +\n  labs(subtitle = NULL)\n\np1 + p2 + p3 + \n  plot_layout(\n    design = \"\n    AAABBB\n    #CCCC#\n    \"\n)\n```\n\n::: {.cell-output-display}\n![Visual Ensemble of Seasonal Plots from The Australian Quarterly Cement Production Series](advanced-models-2_files/figure-html/fig-season-plot-1.png){#fig-season-plot fig-align='center' width=672}\n:::\n:::\n\n\nWe produce three key plots to visualise and decompose the seasonality. The seasonal plot (top left) shows the data broken down by seasonal cycles, which helps to identify the nature of the seasonality. The lines representing different years run parallel to each other, but the level of the entire series is clearly increasing over time. The lines are grouped by a two-year period.\n\nThe seasonal sub-series plot (top right) displays the time series for each individual quarter over the entire time period. Each panel shows the trend within that specific quarter. Every quarter shows a strong upward trend over the years. The blue horizontal line in each panel represents the average production for that quarter. The plot confirms that quarter 3 (Q3) and quarter 4 (Q4) consistently have the highest production levels.\n\nThe third plot at the bottom is an STL decomposed visualisation of the time series, breaking it into its core components - a trend, a seasonal and remainder component. The overall visualisation confirm the quarterly cement series requires a model that handles both non-seasonal trend removal ($\\text{likely }d=1$) and seasonal differences ($D=1$) to remove the strong seasonal pattern.\n\n**Check Stationarity**\n\nBefore fitting the SARIMA model we must address the non-stationarity identified from the visual inspection (strong upward trend and fixed seasonal pattern)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check stationarity\ntraining_cement |> \n  features(Cement, features = unitroot_kpss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  kpss_stat kpss_pvalue\n      <dbl>       <dbl>\n1      3.09        0.01\n```\n\n\n:::\n:::\n\n\nWe use the KPSS unit root test to statistically confirm the presence of a deterministic trend, which causes stationarity. The p-values (0.01) is less than 0.05, leading us to reject the null hypothesis of stationarity. This is a clear indication that the series is non-stationary, requiring differencing.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# apply seasonal and first differencing \ntraining_cement |> \n  gg_tsdisplay(\n    difference(Cement) |> difference(lag=4), \n    plot_type = \"partial\"\n    )\n```\n\n::: {.cell-output-display}\n![](advanced-models-2_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe initial visualisation showed two types of non-stationarity; a strong upward trend and a fixed seasonal pattern. To remove both we apply two types of differencing;\n\n-   a first order differencing $(d=1)$ `difference(Cement)` to remove the linear trend. This calculates the difference between consecutive periods.\n-   seasonal differencing $(D=1)$ `... differencing(lag = 4)` to remove the seasonality ($m=4$ for quarterly data). This calculates the difference between the current and the same quarter last year.\n\nWe use `gg_tsdisplay()` function to plot the double-differenced series. This is essential for identifying the final $\\text{p,q,P,Q}$ orders. After differencing the time series plot now fluctuates around zero with no clear trend or seasonal cycle, confirming stationarity. The ACF and PACF plots shows the remaining correlations structure in the stationary series. There are 2 key concepts to look out for in determining the $(p,q,P,Q)$ orders:\n\n-   **Early lags** (1,2,3...): These are the non seasonal lags and are used to determine the orders p and q\n-   **Seasonal lags** (e.g 4,8,16..for quarters, 12,24,36...for months): This is the seasonal component and they help to determine the P and Q orders.\n\nTo determine the seasonal order, we check the ACF and PACF plot at the seasonal lags (4,8,12,16...). There is a significant spike at lag 4 and 8 with subsequent seasonal lags falling within the significant bounds in the ACF plot. The lag cuts off after the 2nd seasonal lag suggesting seasonal MA(2) $Q=2$. The PACF decays for regular lags after the first lag but still prominent for seasonal lags till after seasonal lag 12. This still proves a seasonal MA model where the ACF cuts off and the PACF decays, pointing to a seasonal ARIMA(0,1,2).\n\nWith the non seasonal order, we look at the first few lags of the ACF and PACF plots. There is a significant spike at lag 1 and 2 (excluding seasonal spikes) of the ACF plot while the rest remain within bounds. This suggests an MA(2) $q=2$. The PACF also shows a significant spike at lag 1 and lag 3 (excluding seasonal spikes). This suggests an AR(3), $p=3$. The presence of significant spikes at lag 2 and lag 3 in both plots followed by cut-offs , suggest a simultaneous need for both AR and MA components. This leads to a non-seasonal ARIMA(3,1,2) structure.\n\nCombining both the non-seasonal and seasonal components, the ACF and PACF plots suggests $\\text{ARIMA}(3,1,2)(0,1,2)[4]$.\n\nHonestly, ACF/PACF interpretation is more of an art than science, especially for seasonal data. Always start small and focus on obvious patterns. Always remember that when the ACF and PACF patterns are not clear you can always use the automatic `ARIMA()` function and let the algorithm decide.\n\n**Fitting the Model**\n\nAfter identifying the order, we can now fit the model and prepare for forecasting. Here we will fit three models; an automatic model, another automatic model with a more thorough search and our identified model order from the ACF and PACF analysis.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit sarima models\nsarima_models <- training_cement |> \n  model(\n    auto_sarima = ARIMA(Cement),\n    thorough_sarima = ARIMA(Cement, stepwise = FALSE),\n    sarima = ARIMA(Cement ~ 0 + pdq(3,1,2) + PDQ(0,1,2))\n  )\n\n# compare models\nglance(sarima_models) |> arrange(AICc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 8\n  .model          sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots  \n  <chr>            <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>    \n1 auto_sarima      3897.   -882. 1778. 1779. 1800. <cpl [2]> <cpl [6]> \n2 thorough_sarima  3897.   -882. 1778. 1779. 1800. <cpl [2]> <cpl [6]> \n3 sarima           4296.   -886. 1787. 1788. 1812. <cpl [3]> <cpl [10]>\n```\n\n\n:::\n:::\n\n\nWe fit multiple candidate models, our manual specification and two automatic models, and evaluate their performance using statistical measures like AICc. In fable, you add the seasonal specification via `PDQ()` and combine it with a constant control using `0` or `1` in the formula. This is because fable uses a constant form parameterisation and the constant term $c$ relates to the mean $\\mu$ via $c=\\mu(1-\\varphi_1-\\cdots-\\varphi_p)$. By default fable will automatically include a constant term if it improves AICc (This only happens when you allow fable to automatically select the orders).\n\nThe `glance()` function provides these statistical fit metrics. Lower $AIcc$ indicates a better model, balancing goodness of fit with model complexity. Both automatic methods selected the same best model($ARIMA(2,0,2)(0,1,1)[4]$) with drift. They share the lowest AICc value suggesting the simpler stepwise search was effective and the more exhaustive search did not yield any better structure.\n\nThe manually specified model had a significantly higher AICc and a higher residual variance. This shows that the structure derived form the ACF and PACF plots was either overly complex or missed a key correlation structure, resulting in a worse fit compared to the automatic selection.\n\nWe can further check the significance of the estimated values using the `coef()` or `tidy()` function which presents all the models in a tabular structure with their corresponding coefficient estimates, standard errors and p-values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(sarima_models)\n```\n:::\n\n\nThe best model (`auto_sarima`) has all seasonal and non-seasonal estimates being statistically significant. The drift term (constant) which represents the average non zero change or the long run average slope in the series after differencing also shows a high statistical significance, implying a persistent slope in the differenced series.\n\nThese outputs validate that the model is a comprehensive representation of the cement production series, successfully accounting for the level, trend, seasonality and the residual autocorrelations\n\n**Residual Check & Forecasting**\n\nThe final stage of the SARIMA workflow involves confirming the model's validity through residual analysis and then generating forecasts as we did for the non seasonal ARIMA.\n\nA well specified SARIMA model should leave behind residuals that are indistinguishable from **white noise**. We use both visual and statistical tests to confirm this for the optimal $ARIMA(2,0,2)(0,1,1)[4]$ model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check residuals visually \ngg_tsresiduals(sarima_models |> select(auto_sarima))\n```\n\n::: {.cell-output-display}\n![](advanced-models-2_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe see the innovation residuals (`.innov`) (top plot) fluctuating randomly around zero with no obvious patterns or trends, suggesting the model has captured all systematic structure. Almost all spikes at both seasonal and non seasonal lags in the ACF plot are within the blue dashed confidence bounds with lag 16 and 18 slightly overlapping the confidence bound (negligible). This confirms no significant correlation remains in the residuals. The Histogram is also roughly bell-shaped and centred near zero, consistent with the normality assumption.\n\nWe further confirm these visual check with a formal statistical check using the Ljung-Box test.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check residuals statistically\naugment(sarima_models) |> \n  features(.innov, features = ljung_box)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .model          lb_stat lb_pvalue\n  <chr>             <dbl>     <dbl>\n1 auto_sarima      0.147      0.702\n2 sarima           0.0734     0.786\n3 thorough_sarima  0.147      0.702\n```\n\n\n:::\n:::\n\n\nThe large p-values for all three models suggest all models pass the residual check. Since `auto_sarima` had the lowest AICc (a measure of predictive power), it is selected for forecasting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate a 2year (8quarters) forecast for all models \nsarima_forecast <- sarima_models |> \n  forecast(h = 8)\n```\n:::\n\n\nThe chosen $ARIMA(2,0,2)(0,1,1)[4]$ model is used to generate an 8-quarter (2-year) forecast. The forecast table generated from the above shows the point forecast (`.mean`) for all models and their associated variance ($N(\\mu,\\varsigma^2)$) increasing with the horizon.\n\nWe create a forecast plot with the `auto_sarima` model to visually summarise the model's projection.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualise auto_sarima forecasts\nsarima_forecast |> filter(.model == \"auto_sarima\") |> \n  autoplot(training_cement)\n```\n\n::: {.cell-output-display}\n![](advanced-models-2_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe forecast successfully maintains the seasonal pattern (peaks and troughs matching the seasonal cycle) and the upward trend established by the historic data. This is due to the significant **seasonal MA(1)** term and the **drift term** which ensures the trend component continues to project growth rather than flattening out. The blue shaded region widens as the forecast extends further into the future, reflecting the inherent increase in forecast uncertainty over longer horizons.\n\n## Insights from ARIMA Modelling\n\nARIMA models remain a cornerstone of time series analysis, offering a balance between interpretability and predictive power. Here are some key insights drawn from our modelling process.\n\nThe automatic ARIMA selection in the fable package consistently identifies models that strike a balance between complexity and performance, often outperforming manual specified alternatives. This automation ensures that the chosen model is well suited to the data without unnecessary trial and error. In our cereal yield model, the inclusion of MA(1) component demonstrates how the model learns from past forecast errors. By adapting its predictions based on previous inaccuracies, ARIMA improves its ability to forecast future values effectively.\n\nARIMA models also account for uncertainty through their error structure, producing realistic prediction intervals that widen appropriately as the forecast horizon extends. This feature provides a more honest representation of future variability compared to point forecasts alone. Residual diagnostics also play a crucial role in validating the model. By confirming that residuals resemble white noise, we can be confident that the model has captured all predictable patterns in the data.\n\nFor seasonal data, SARIMA models extend this capability by capturing both short-term and seasonal patterns through their dual structure of (p,d,q)(P,D,Q). This ensures that cyclical behaviours are properly modelled alongside regular fluctuations.\n\nFinally, ARIMA models often outperforms alternative methods like ETS when dealing with complex autocorrelation structures, particularly in financial datasets. This comparative advantage makes ARIMA a powerful tool for time series forecasting in domains where relationships between observations are intricate.\n\n## Summary\n\nARIMA models represent a sophisticated yet accessible approach to time series forecasting that leverages the inherent autocorrelation in your data. The `fable` implementation makes this powerful technique available to analysts at all levels, automating the complex parts while maintaining transparency and control.\n\nIn our next and final chapter, we will bring everything together and rigorously evaluate model performance using the testing framework we established earlier.\n",
    "supporting": [
      "advanced-models-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}