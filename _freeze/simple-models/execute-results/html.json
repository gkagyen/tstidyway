{
  "hash": "89adab0067a0b05655db8ce368fd2e1f",
  "result": {
    "engine": "knitr",
    "markdown": "# Simple Forecasting Models {#sec-simple-forecasting}\n\nIn the world of forecasting, complexity does not always equal accuracy. Before we dive into sophisticated algorithms with dozens of parameters, we start with simple benchmark models that serve crucial purposes in any forecasting workflow. These elementary models provide:\n\n-   **Performance Baselines:** If your advanced model cannot beat a simple naive forecast, it is not adding value\n-   **Model Diagnostics:** Simple models reveal fundamental patterns and outliers in your data\n-   **Computational Efficiency:** Quick to fit and easy to interpret\n-   **Business Understanding:** Often more easily explained to stakeholders than complex models\n\nThis philosophy is simple: start basic, establish benchmarks, then gradually increase complexity only when it demonstrably improve performance.\n\n\n\n\n\n## Understanding and Fitting Benchmark Models\n\nBenchmark models are simple, non-complex forecasting techniques used as baseline to evaluate the performance of more sophisticated models. Let us first understand the three fundamental benchmark models we shall be working with.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create a simple example to illustrate models\nexample_data <- tibble(\n  Period = 1:12,\n  Expenditure = c(100, 110, 120, 130, 125, 135, 140, 150, 145, 155, 160, 170)\n)\nhead(example_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  Period Expenditure\n   <int>       <dbl>\n1      1         100\n2      2         110\n3      3         120\n4      4         130\n5      5         125\n# ℹ 1 more row\n```\n\n\n:::\n:::\n\n\n| Model | Core Principle | Prediction Rule | Example |\n|------------------|------------------|------------------|------------------|\n| Mean Model | simple constant | Always predicts the **historical average** of the series | for our `example_data` the mean = **137.5** for all future periods. |\n| Naïve Model | random walk | Predicts the last observed value $\\hat{Y}_{t+h}=Y_t$ | for our `example_data` this will be the last `Ependiture` for the `Period`, **170.** |\n| Seasonal Naïve | seasonal repetition | Predicts the value from the same season in the last period. $\\hat{Y}_{t+h}=Y_{t+h-m}$, where $m$ is the seasonal period | for our example_data, assuming the seasonal period is 4, then the predictions for the next season will be; **145, 155, 160 and 170.** |\n\n### Fitting Simple Models with `fable`\n\nThe `fable` package provides a streamlined syntax for fitting any time series model. We will apply these models to the training set (`training_data`) of our `sales_ts` data using the unified `fable` syntax.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit simple benchmark models to our training data\nsimple_models <- training_data |> \n  model(\n    mean = MEAN(Sales),\n    naive = NAIVE(Sales),\n    snaive = SNAIVE(Sales ~ lag(12))  # lag 12 for monthly data\n  )\n\n# display the fitted models\nsimple_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 3\n     mean   naive   snaive\n  <model> <model>  <model>\n1  <MEAN> <NAIVE> <SNAIVE>\n```\n\n\n:::\n:::\n\n\n`model()` is the unified function for fitting all types of time series models in `fable`. `MEAN(Sales)` fits a mean model that predicts the historical average and stores it in a variable `mean`, `NAIVE(Sales)` fits a naive model that predicts the last observed value and `SNAIVE(Sales ~ lag(12))` fits a seasonal naive model using a lag of 12 periods (Months).\n\nThe output (`simple_models`) is a `mable` (model table), a special type of tibble data structure that stores one or more fitted models for a time series data. Each of the columns represent and contain the fitted model object. If the time series `tsibble` contains unique keys, the rows inside the `mable` would represent those keys.\n\n### Understanding Model Components\n\nTo assess how well a model captures the historical pattern, we extract its fitted values and residuals using the `augment()` function. Let us now examine what each model has learned from our data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extract components and parameters from each model\nmodel_components <- simple_models |> \n  augment()\n\n# View the first few rows of the model components\nmodel_components \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 126 x 6 [1M]\n# Key:       .model [3]\n  .model    Month Sales .fitted .resid .innov\n  <chr>     <mth> <dbl>   <dbl>  <dbl>  <dbl>\n1 mean   2015 Jan 29167  28745.   422.   422.\n2 mean   2015 Feb 43929  28745. 15184. 15184.\n3 mean   2015 Mar 37679  28745.  8934.  8934.\n4 mean   2015 Apr 37457  28745.  8712.  8712.\n5 mean   2015 May 36575  28745.  7830.  7830.\n# ℹ 121 more rows\n```\n\n\n:::\n:::\n\n\n`augment()` extracts the fitted values , residuals and other components from all three fitted models. `.model` identifies which of the fitted models the data belongs to, `.fitted` represents the model's prediction for each time period, `.resid` is the difference between the actual and predicted values (residuals), and `.innov` represents the innovations (or shocks) which is the one-step ahead forecast errors. These values are similar to the residuals but are more meaningful in models like ARIMA and ETS.\n\nAll these components helps us to understand how each model interprets and attempts to replicate the historical time series data.\n\n## Generating and Visualising Forecasts from Benchmark Models\n\nAfter fitting the benchmark models to the training data, the next critical step is to generate future predictions and visualise these forecasts along with their associated uncertainty (prediction intervals).\n\n### Generating Forecasts\n\nWe will now generate forecasts from our simple models. The primary function that will help us do this is `forecast()`, which is part of the `fabletools` package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate forecast for the next 12 months\nsimple_forecasts <- simple_models |> \n  forecast(h = 12)\n\n# display the forecasts\nsimple_forecasts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 36 x 4 [1M]\n# Key:     .model [3]\n  .model    Month\n  <chr>     <mth>\n1 mean   2018 Jul\n2 mean   2018 Aug\n3 mean   2018 Sep\n4 mean   2018 Oct\n5 mean   2018 Nov\n# ℹ 31 more rows\n# ℹ 2 more variables: Sales <dist>, .mean <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# pivot wider for easy comparison\nsimple_forecasts |> \n  as_tibble() |> \n  select(.model, Month, .mean) |> \n  pivot_wider(names_from = .model, values_from = .mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 4\n     Month   mean naive snaive\n     <mth>  <dbl> <dbl>  <dbl>\n1 2018 Jul 28745. 15383  22173\n2 2018 Aug 28745. 15383  41127\n3 2018 Sep 28745. 15383  36467\n4 2018 Oct 28745. 15383  20974\n5 2018 Nov 28745. 15383  15707\n# ℹ 7 more rows\n```\n\n\n:::\n:::\n\n\n`forecast(h = 8)` generates 12 periods (months) into the future (from July 2018 to June 2019), starting from where the data ended in June 2018. The output includes point forecasts and the distribution forecasts (uncertainty). The output `simple_forecasts` is a **fable** (forecast table), which is a special `tsibble` data structure for representing forecasts. Each row in the **fable** represents a forecast for a specific time point.\n\nThe `.mean` column is the point forecast (expected future value) which is **28,745** for all periods in the **mean model** because the mean model predicts the same value for every future point, **15,383** for all periods in the **naive model** because it just repeats the last observed value for every future period (random walk) and the **last 12 Sales values** (seasonal period) repeated from the original series data for the **seasonal naive mode**l.\n\nThe `Sales` column is the distribution object representing the forecasts and its uncertainty $N(Mean, \\sigma^2)$. For instance the distribution for the first prediction by the mean model is **N(28745, 1e+08)**, this means a normal distribution with a point forecast of 28745 and variance of 100000000 (`SD = 10000`). The uncertainty in the distribution is the same for all the mean and seasonal naive models but increases with increasing forecast horizon for the naive model.\n\n### Prediction Intervals with `hilo()`\n\nBy default the forecast table does not explicitly show prediction intervals but they are calculated internally. We can display them using the `hilo()` function from `fabletools`. The intervals are also automatically added when we plot the forecast with `autoplot()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# add prediction intervals to the forecast table\nsimple_forecasts |> hilo(level = 95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 36 x 5 [1M]\n# Key:       .model [3]\n  .model    Month\n  <chr>     <mth>\n1 mean   2018 Jul\n2 mean   2018 Aug\n3 mean   2018 Sep\n4 mean   2018 Oct\n5 mean   2018 Nov\n# ℹ 31 more rows\n# ℹ 3 more variables: Sales <dist>, .mean <dbl>, `95%` <hilo>\n```\n\n\n:::\n:::\n\n\n`hilo()` calculates the prediction intervals and adds them to the forecast table output. By default it returns 80% and 95% intervals if no level is specified. Different models have different intervals based on their inherent uncertainty. The naive model typically has wider intervals due to its assumption that the next value could be anywhere near the last observed value. The mean model 's intervals are constant because the forecast is constant. With the seasonal naive model the intervals widen but the forecast itself repeats the last seasonal cycle, giving it structure.\n\n### Visualising Simple Forecasts\n\nPlotting the forecast allows for quick visual comparison of how each benchmark model interprets the series' future. We can create a comprehensive visualisation to compare our benchmark forecasts with the `autoplot()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot all simple forecasts together\nsimple_forecasts |> \n  autoplot(training_data, level = 95, linewidth = 0.7) + \n  facet_wrap(~ .model, scales = 'free_y', ncol=2) +\n  guides(colour = \"none\", fill = \"none\") +\n  ggtitle(\"Benchmark Forecasts: Mean, Naïve and Seasonal Naïve Models\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Monthly Sales Series Forecast with Simple Models, Providing Crucial Performance Baselines](simple-models_files/figure-html/fig-simple-forecast-1.png){#fig-simple-forecast fig-align='center' width=672}\n:::\n:::\n\n\nWhen the `autoplot()` function is fed a forecast table (**fable object**) together with the time series tsibble, it automatically plots the forecasts with the historical data \\[`simple_forecast |> autoplot(training_data)`\\]. Specifying the level (`level = 95`) adds 95% confidence bands around the forecast values. If no level is specified, it uses the default which is 80% and 95% together. To exclude the confidence bands, set `level=NULL`. The plot is faceted on the models to get each model's forecast in a separate panel. The `guides(colour = \"none\", fill = \"none\")` line suppresses the redundant legends for th model name.\n\nFrom the above visualisations we can clearly see each model's forecast behaviour. A flat forecast line predicting the average value (mean model), a flat forecast line extending the last observed value with rapidly widening intervals (naive model) and a cyclical forecast line that repeats the last 12-month seasonal pattern (seasonal naive model).\n\n### Deeper Dive: Visualising Fitted Values and Forecast\n\nFor a deeper understanding, we can also plot the individual forecasts separately and then overlay the fitted values on top of the original series with the help of the `autolayer()` function from `ggtime`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# make plot for only the seasonal naive model\nsnaive_model_fitted <- simple_models |> \n  augment() |> \n  filter(.model == \"snaive\") |> \n  select(.fitted) \n\nsnaive_model_forecast <- simple_models |> \n  select(snaive) |> \n  forecast(h = \"1 year\")    # same as 12 Months\n\nsnaive_plot <- snaive_model_forecast |> \n  autoplot(training_data, level = 95) +\n  autolayer(snaive_model_fitted, .fitted, colour = 'red')\n```\n:::\n\n\nWe extract only the seasonal naive model and plot the predicted values and the actual Sales series overlayed together on the same panel. The `autolayer()` function is designed to easily overlay components from a **tsibble** unto an existing `autoplot()`. This visually demonstrate that the fitted values are simply the actual sales values from 12 months prior (notice how the forecast starts after the first 12 months of the original series).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sales Series and SNAIVE Predictions + Forecast (12 months)](simple-models_files/figure-html/fig-fit-for-1.png){#fig-fit-for fig-align='center' width=672}\n:::\n:::\n\n\nThis final plot provides powerful insight: the **SNAIVE** model forecast by simply projecting the last observed seasonal cycle into the future.\n\n## Initial Accuracy Assessment\n\nEven before formal testing, we can assess how well these models accurately produce the patterns observed in the historical training data. This initial check helps identify obvious flaws and provides a baseline for evaluating complexity.\n\nThe fable framework simplifies this process with the `accuracy` function, which automatically calculates a comprehensive set of error metrics for all fitted models in the **mable object (`simple_models`).**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate training accuracy metrics\ntraining_accuracy <- simple_models |> \n  accuracy()\n\nprint(training_accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model .type           ME   RMSE    MAE   MPE  MAPE  MASE RMSSE    ACF1\n  <chr>  <chr>        <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 mean   Training -8.66e-13  9821.  8378. -16.8  38.4 0.892 0.819 -0.0982\n2 naive  Training -3.36e+ 2 14583. 11914. -18.9  50.2 1.27  1.22  -0.602 \n3 snaive Training -2.26e+ 3 11999.  9393. -24.5  45.5 1     1     -0.301 \n```\n\n\n:::\n:::\n\n\nThe fitted models are passed to the `accuracy()` function, which calculates metrics based on the residuals generated from the training data used to fit the models. The output is a tibble displaying several common error metrics.\n\n| Metric | Full Name | Interpretation |\n|------------------------|------------------------|------------------------|\n| MAE | Mean Absolute Error | The average magnitude of the forecast errors |\n| RMSE | Root Mean Square Error | Penalises larger errors more heavily than MAE Always $RMSE\\ge MAE$. |\n| MASE | Mean Absolute Scaled Error | ***Key Scaled Metric.*** Measures accuracy relative to the NAIVE model. |\n| ACF1 | Autocorrelation of Residuals | Measures the correlation between the error at time $t$ and the error at time $t-1$. Should be close to zero for a good model |\n\n: Key Metrics in Accuracy Output\n\nThe low $MASE$ score for the `mean` model (0.892) indicates that even the simplest forecast (the historical average) performs better on the training data than the `naive` model. The `snaive` model has a value of 1 under $MASE$, indicating a benchmark for the seasonality in the data.\n\nFor a time series with seasonality (like Monthly Sales Data), the appropriate reference point for evaluating forecast accuracy is in the **Seasonal Naive model (`snaive`**). By standard convention in time series analysis, the $MASE$ is typically scaled against the `snaive` error when the data is seasonal. The unscaled error vales of the `snaive` model ($RMSE=11999$ and $MAE=9393$) suggests its ability to capture some seasonality making it the best benchmark model for this training data. The true test, however, will be its accuracy on the unseen testing data.\n\n## Understanding Simple Models For Strategic Decision Making\n\nForecasting models, even simple benchmarks, provide essential information for business planning, inventory management, budgeting and others. By summarising the 12-month forecast horizon, we can extract three key data points; the overall expected demand, the most pessimistic forecast, and the stability of the prediction.\n\nThe following code aggregates the month-by-month forecast for each model to provide these high-level strategic summaries.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninsightful_summary <- simple_forecasts |> \n  as_tibble() |> \n  group_by(.model) |> \n  summarise(\n    average_forecast = mean(.mean),\n    confidence = sd(.mean) / mean(.mean),   # coefficient of variation\n    .groups = 'drop'\n  ) |> \n  mutate(growth_trend = case_when(\n      .model == \"naive\" ~ \"flat\",\n      .model == \"mean\" ~ \"flat\",\n      .default = \"seasonal\"\n    )\n  )\n\nprint(insightful_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .model average_forecast confidence growth_trend\n  <chr>             <dbl>      <dbl> <chr>       \n1 mean             28745.      0     flat        \n2 naive            15383       0     flat        \n3 snaive           25802.      0.451 seasonal    \n```\n\n\n:::\n:::\n\n\nFor easier manipulation, the **fable object** (`simple_forecast`) which is actually a specialises `tsibble` is converted to a tibble and then grouped by the model name (`.model`) so that we can calculate our preferred summary statistics separately for each model's forecast. We calculate the average forecasted value over the 12-month horizon, the coefficient of variation (`confodence`) which measures the variability in the data compared to its mean, and then finally add a description column `growth_trend` to explicitly label the pattern that each model projects.\n\nThe **Mean** model provides a stable, conservative forecast that ignores both trends and seasonality. It is the most optimistic and the coefficient of variation (CV) value of 0 confirms it is a perfectly flat line. Useful as an Absolute minimum performance benchmark. Management can use this forecast **(28,745)** as an optimistic baseline for revenue goals.\n\nThe **Naive** model captures the most recent level but assumes no change going forward. Since the last value was low, this model drags the 12-month average down significantly, representing a worst case baseline. Surprisingly it could be difficult to beat sometimes for many short term forecasting problems, especially in volatile environments. Management can use this as a highly conservative estimate for minimum expected sales.\n\nThe **Seasonal Naive** excels at capturing stable seasonal patterns. It has the highest average among all the 3 models and the CV value of 0.451 confirms the model predicts some monthly fluctuations (peaks and troughs) due to seasonality. This model can remarkably be effective for data with consistent seasonality and provides an excellent seasonal benchmark. It is the most actionable for operations. The identified variability signals the need for flexible inventory and staffing plans to handle predictable surges and drops throughout the year. The actual values from this model's forecast could be used to set monthly targets, not the overall average.\n\nThe difference between these forecasts reveal fundamental characteristics of our data. Large discrepancies between models indicate strong trend or seasonality that more sophisticated models should capture. If complex models cannot outperform these simple benchmarks, it may indicate over-fitting or that our data lacks predictable patterns beyond basic persistence.\n\nThese simple models are not just academic exercises – they are practical tools that establish the minimum performance threshold any advanced model must clear. They teach us humility in forecasting and provide interpretable baselines that stakeholders can easily understand.\n\nIn our next section we will build upon these these foundations with more sophisticated models, but we will continually refer back to these benchmarks to ensure our added complexity is actually adding value\n",
    "supporting": [
      "simple-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}