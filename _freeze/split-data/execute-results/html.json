{
  "hash": "457c58e3f8fd3f359363a119c944af40",
  "result": {
    "engine": "knitr",
    "markdown": "# Splitting Temporal Data for Modelling {#sec-splitting-data}\n\nWe have now reached a pivotal point in our forecasting journey. Our data has been cleaned, its variance stabilised and stationarity confirmed. Yet, before we proceed to build forecasting models, one fundamental question remains: How will we determine whether our forecasts are truly effective?\n\nThis is where proper data splitting becomes essential. Unlike cross-sectional data where random sampling is appropriate, time series data has a strict temporal order that must be preserved. Splitting time series data incorrectly can lead to **data leakage**, where future information unintentionally influences past predictions, making model evaluations completely unreliable.\n\n\n\n\n\n## Importance of Temporal Splitting\n\nIn standard predictive models that uses cross-sectional data like linear regression, the goal is to predict an outcome without regard to order, so random train-test splitting is common. However, with time series data care must be taken to maintain the chronological order when splitting because it has observations that are dependent on previous observations (autocorrelation).\n\nA random split ignores the time dependency, leading to data leakage and unrealistic evaluations. First, let us understand why time series requires special care when splitting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# demonstrate the wrong way to split time series data\nset.seed(523)  \n\n# WRONG: random sampling for time series\nwrong_split <- sales_ts |> \n  mutate(Split = if_else(runif(n()) > 0.7, \"Testing\", \"Training\"))\n```\n:::\n\n\nThe above code creates a random 70/30 split. We set a seed '`set.seed(523)`' so thst the random numbers generated will remain the same anytime the code runs. This approach is problematic because the testing points are scattered throughout the timeline as displayed in @fig-wrong-split below. Fitting a model using this data could make it appear accurate by \"memorising\" nearby values, but will fail in real forecasting scenarios.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A Scatter Plot Showing Random Splitting of Temporal Data with Clear Signs of Data Leakage](split-data_files/figure-html/fig-wrong-split-1.png){#fig-wrong-split fig-align='center' width=672}\n:::\n:::\n\n\nThis plot shows a clear example of data leakage because the model will train on past data but then has to predict values for periods where it has already seen future information (e.g., training on 2019 sales but testing on 2016 sales).\n\nA model evaluated this way will report optimistically high accuracy on the test data because it is not performing true forecasting into an unknown future (unrealistic evaluation).\n\n## Proper Temporal Train-Test Splitting\n\nThe correct approach is to maintain the temporal order, using earlier data for training and later data for testing. When this happens, the model uses the past to predict the future which is exactly what you need your model to do.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# proper temporal split\ntemporal_split <- sales_ts |> \n  mutate(\n    Split = if_else(Month < yearmonth(\"2018 Jul\"), \"Training\", \"Testing\")\n  )\n```\n:::\n\n\nThis new code splits based on a specific time point. All data before July 2018 is used for training and all data after for testing. This preserves the temporal sequence as can be seen in @fig-proper-split below, and simulates real forecasting scenarios.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A Time Series Plot Showing Proper Splitting of Temporal Data with Clear Chronological Order](split-data_files/figure-html/fig-proper-split-1.png){#fig-proper-split fig-align='center' width=672}\n:::\n:::\n\n\nWe see in this plot how the entire data before the split point (dashed vertical line-July 2018) is used as the training data (blue), followed orderly by the testing data (red). This train-test split is necessary for a valid forecasting evaluation.\n\n### Creating Training and Testing Datasets\n\nNow we can formally create our training and testing datasets the right way. To make the splitting easier, we are going to use the `initial_time_split()` function from the `rsample` package. This function will automatically split the data proportionally while maintaining the temporal order.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create forma training and testing datasets\nlibrary(rsample)\n\nsplit <- initial_time_split(sales_ts, prop = 0.7)\n\ntraining_data <- training(split)\ntesting_data <- testing(split)\n\n# verify order of splits\npartition_table <- tibble(\n  Partition = c(\"training data\", \"testing data\"),\n  Start_period = c(min(training_data$Month),\n                   min(testing_data$Month)\n                   ),\n  End_period = c(max(training_data$Month),\n                 max(testing_data$Month)\n                 ),\n  N_Obs = c(nrow(training_data), nrow(testing_data))\n  )\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Partition     |Start_period |End_period | N_Obs|\n|:-------------|:------------|:----------|-----:|\n|training data |2015 Jan     |2018 Jun   |    42|\n|testing data  |2018 Jul     |2019 Dec   |    18|\n\n\n:::\n:::\n\n\n`initial_time_split(sales_ts, prop = 0.7)` is the core function for our temporal splitting. It takes our time series data (`sales_ts`) and automatically creates the split object such that the first $70\\%$ (`prop = 0.7`) of the data chronologically forms the training set, and the remaining $30\\%$ forms the testing set.\n\n`training(split)` / `testing(split)` are accessor functions that extract the actual data frames (tibble) for the respective partitions. To confirm we have the right train-test split create a table to verify the chronological order. As expected we see in the table that the test data is immediately after the end date of the training data, confirming that the split is purely temporal and not random.\n\n## Time Series Cross Validation\n\nFor more robust model evaluation, we can use time series cross-validation (TS-CV). This creates multiple training and testing folds while preserving temporal order. We us the `stretch _tsibble()` function from the `tsibble` package to create cross-validation folds in a time series data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create time series cross-validation folds\ntscv_folds <- training_data |> \n  stretch_tsibble(.init = 26, .step = 1) \n\n# examine the cross-validation structure\ncv_summary <- tscv_folds |> \n  as_tibble() |> \n  group_by(.id) |> \n  summarise(\n    start_date = min(Month),\n    end_date = max(Month),\n    n_obs = n(),\n    .groups = \"drop\"\n  )\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n| .id|start_date |end_date | n_obs|\n|---:|:----------|:--------|-----:|\n|   1|2015 Jan   |2017 Feb |    26|\n|   2|2015 Jan   |2017 Mar |    27|\n|   3|2015 Jan   |2017 Apr |    28|\n|   4|2015 Jan   |2017 May |    29|\n|   5|2015 Jan   |2017 Jun |    30|\n|   6|2015 Jan   |2017 Jul |    31|\n|   7|2015 Jan   |2017 Aug |    32|\n|   8|2015 Jan   |2017 Sep |    33|\n|   9|2015 Jan   |2017 Oct |    34|\n|  10|2015 Jan   |2017 Nov |    35|\n|  11|2015 Jan   |2017 Dec |    36|\n|  12|2015 Jan   |2018 Jan |    37|\n|  13|2015 Jan   |2018 Feb |    38|\n|  14|2015 Jan   |2018 Mar |    39|\n|  15|2015 Jan   |2018 Apr |    40|\n|  16|2015 Jan   |2018 May |    41|\n|  17|2015 Jan   |2018 Jun |    42|\n\n\n:::\n:::\n\n\n`stretch_tsibble(.init = 25, .step = 1)` creates expanding window folds. `.init` sets the initial training window size and `.step` specifies how many months to move forward for each fold. Our code creates multiple folds where each fold starts with at least 25 observations (months) and expands by 1 observation each time. Here the first fold uses months 1to 25 for training and month 26 for testing. The next fold uses months 1 to 37 for training and month 38 for testing and so on till the end of the series.\n\nEach fold is marked by an `.id` column allowing us to track different train-test combinations. This approach provides multiple evaluations across different time periods.\n\n### Visualise Cross-Validation Folds\n\nLet us visualise how our cross validation works\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualise the first few cross-validation folds\ntscv_folds |> \n  filter(.id <= 4) |>      # show first 4 folds\n  group_by(.id) |>         # temporarily group data\n  mutate(\n    fold_info = paste(\"Fold\",.id,\": \",first(Month),\"to\",last(Month)),\n  ) |> \n  ungroup() |>             # remove grouping\n  ggplot(aes(x = Month, y = Sales, group = .id)) + \n  geom_line(linewidth = 0.7) +\n  facet_wrap(vars(fold_info), ncol = 2) \n```\n\n::: {.cell-output-display}\n![Time Series Cross Validation Folds For A 5-Year Period Monthly Sales](split-data_files/figure-html/fig-tscv-1.png){#fig-tscv fig-align='center' width=672}\n:::\n:::\n\n\nThe above plot displays the expanding nature of the cross validation folds produced from `stretch_tsibble`. We clearly see how each fold contains all the data from the previous fold plus the additional (`step`) month.\n\n### Walk-Forward Validation\n\nFor forecasting, walk-forward validation (also known as rolling-origin evaluation) provides the most realistic assessment. The `slide_tsibble()` function from the `tsibble` package is designed for walk-forward validation. It creates overlapping windows of data that move step by step.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# implement walk forward vaidation\nwalkf_cv_folds <- training_data |> \n  slide_tsibble(.size = 28, .step = 1) \n\n# examine the walk-forward cv structure\nwalkf_cv_summary <- walkf_cv_folds |> \n  as_tibble() |> \n  group_by(.id) |> \n  summarise(\n    start_date = min(Month),\n    end_date = max(Month),\n    n_obs = n(),\n    .groups = \"drop\"\n  )\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n| .id|start_date |end_date | n_obs|\n|---:|:----------|:--------|-----:|\n|   1|2015 Jan   |2017 Apr |    28|\n|   2|2015 Feb   |2017 May |    28|\n|   3|2015 Mar   |2017 Jun |    28|\n|   4|2015 Apr   |2017 Jul |    28|\n|   5|2015 May   |2017 Aug |    28|\n|   6|2015 Jun   |2017 Sep |    28|\n|   7|2015 Jul   |2017 Oct |    28|\n|   8|2015 Aug   |2017 Nov |    28|\n|   9|2015 Sep   |2017 Dec |    28|\n|  10|2015 Oct   |2018 Jan |    28|\n|  11|2015 Nov   |2018 Feb |    28|\n|  12|2015 Dec   |2018 Mar |    28|\n|  13|2016 Jan   |2018 Apr |    28|\n|  14|2016 Feb   |2018 May |    28|\n|  15|2016 Mar   |2018 Jun |    28|\n\n\n:::\n:::\n\n\n`slide_tsibble(.size = 28, .step = 1)` creates fixed-size windows that slides forward. Here `size = 28` uses 28 months for the first fold (fixed) and each new fold (window) moves forward by 1 month (`.step = 1`). This simulates real-world forecasting where we use available history to predict the future.\n\nThe key difference between the `stretch_tsibble()` and `slide_tsibble()` functions is that the former uses expanding folds (i.e., training sets grow over time) whiles the latter uses fixed folds (old observations drop off as new ones come in, keeping the window fixed). Implementing TS-CV either with `stretch_tsibble` or `slide_tsibble` provide more robust model error estimates than a single train-test split.\n\n## Handling Multiple Series'\n\nWhen dealing with a collection of time series (tsibble with 2 or more keys), a proper train-test split must be applied to each series individually but based on a consistent temporal rule.\n\n### Consistent Temporal Splitting by Date\n\nThis demonstration manually applies a split boundary (`year < 2005`) to a selection of indicator names in the `gh_ts` tibble.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# split multiple series consistently\nmulti_series_split <- gh_ts |> \n  filter(indicator_name %in% c(\n    \"Annual GDP growth rate\",\n    \"Cereal yield _kg per hectare\",\n    \"Gross national expenditure (% of GDP)\"\n  )) |> \n  select(indicator_name, year, value) |> \n  group_by(indicator_name) |> \n  mutate(\n    split = if_else(year < 2005, \"Training\", \"Testing\")\n    ) |> \n  ungroup()\n```\n:::\n\n\nIn order to apply the split boundary logic to all the series independently, the data is grouped by their unique indicator name for the series. The consistent temporal rule specified in the mutate function creates a marker for the training and testing split.\n\nAll data before the year 2005 $(~70%)$ is marked as the **training set** and 2005 upwards $(~30)$ is the **testing set**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualise the splits for all series\nmulti_series_split |> \n  ggplot(aes(x = year, y = value, colour = split)) +\n  geom_line(linewidth  = 0.7) +\n  facet_wrap(vars(indicator_name), scales = \"free_y\", ncol = 2) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Temporal Time Split for Multiple Series - Consistent Split Points Across All Series](split-data_files/figure-html/fig-mult-tscv-1.png){#fig-mult-tscv fig-align='center' width=672}\n:::\n:::\n\n\nVisualising the series, we see that each of the three indicators - GDP growth rate, Cereal yield and GNE (% of GDP) - are all split exactly at the year 2005, as indicated by the separate **red** and **blue** lines. This confirms our intended chronological split for each series where the training period precedes the testing period for all series simultaneously.\n\n### Automated Splitting and Extraction\n\nWhile manual splitting works, automatically creating and extracting the training and testing datasets for many series requires a more programmatic approach compared to doing the same for a single series by utilising some useful functions from the `purrr` and `rsample` packages.\n\n**A. Applying the Split per Series Key**\n\nThe `fable` ecosystem relies on the key columns (here, `indicator_name`) to perform operations across multiple series in a tsibble.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# apply initial time split per key\nsplits_multiple <- gh_ts |> \n  filter(indicator_name %in% c(\n    \"Annual GDP growth rate\",\n    \"Cereal yield _kg per hectare\",\n    \"Gross national expenditure (% of GDP)\"\n  )) |> \n  select(indicator_name, year, value) |> \n  group_by_key() |> \n  group_map(~ initial_time_split(.x, prop = 0.7))\n```\n:::\n\n\nThe `group_by_key()` function is specific to `tsibbles`. It groups the data by the unique time series identifier (`indicator_name`). The core operation of what we want to achieve is carried out by `group_map()`, which iterates over each series (each indicator) and applies the `initial_time_split()` function to create a chronological split, using the first $70\\%$ of the observations for training. The resulting output from this code is a list of **3** split objects, each representing a unique series.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n<Training/Testing/Total>\n<45/20/65>\n\n[[2]]\n<Training/Testing/Total>\n<45/20/65>\n\n[[3]]\n<Training/Testing/Total>\n<45/20/65>\n```\n\n\n:::\n:::\n\n\n**B. Extracting and Organising the Datasets**\n\nThis next code uses `purrr`'s `map()` function to extract the training and testing datasets from the list of split objects, and then organising them into a tidy tibble.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extract training and testing sets and convert back to tsibble\ntrain_sets <- map(splits_multiple, training) |> \n  lapply(as_tsibble, index = year)\n\ntest_sets <- map(splits_multiple, testing) |> \n  lapply(as_tsibble, index = year)\n\n# combine with labels (indicator names) and convert to a tidy tibble\nkeys <- gh_ts |> \n  filter(indicator_name %in% c(\n    \"Annual GDP growth rate\",\n    \"Cereal yield _kg per hectare\",\n    \"Gross national expenditure (% of GDP)\"\n  )) |> \n  distinct(indicator_name) |> pull()\n\nmulti_split_list <- tibble(\n  series = keys, training = train_sets, testing = test_sets\n)\n```\n:::\n\n\nthe `map()` function works similarly to the `group_map()` function. It also iterates through the list of split objects (`splits_multiple`) and applies the `training()` and `testing()` accessor functions to each one , returning a list of three separate training and testing `tibbles` (one for each series). The `lapply()` function converts all the returned `tibbles` back to time series `tsibbles`.\n\nWe then combine everything into a single structured tibble (`multi_split_list`)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  series                                training          testing          \n  <chr>                                 <list>            <list>           \n1 Annual GDP growth rate                <tbl_ts [45 × 2]> <tbl_ts [20 × 2]>\n2 Cereal yield _kg per hectare          <tbl_ts [45 × 2]> <tbl_ts [20 × 2]>\n3 Gross national expenditure (% of GDP) <tbl_ts [45 × 2]> <tbl_ts [20 × 2]>\n```\n\n\n:::\n:::\n\n\nThe `multi_split_list` object is a tidy summary where each row represents a single time series, and the `training` and `testing` columns contain the properly split `tsibbles`. This structure is ideal for cross-validation and concurrent model fitting on multiple series.\n\n## Summary\n\nIn this chapter we have addressed an important aspect of time series forecasting; creating a validation framework that produces honest, reliable performance assessments. We moved beyond simple random sampling to embrace the temporal nature of our data\n\nYou have seen the reason why preserving chronological order prevents data leakage and ensures realistic model evaluation. You now understand that testing should always follow training in time and performing cross-validation on time series gives us multiple assessments of model performance, making our evaluation more reliable than a single train-test split.\n\nWith our data properly prepared, variance stabilised, stationarity achieved and rigorous validation splits established we are now ready for the most exciting phase: building actual forecasting models. In our next section we will explore various forecasting models and how they effectively they fit to our temporal data.\n",
    "supporting": [
      "split-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}