{
  "hash": "3b70a044ced749adc02fd44a5cddae0d",
  "result": {
    "engine": "knitr",
    "markdown": "# Exploring Patterns with `feasts` and `fabletools` {#sec-exploring-patterns-with-feasts-and-fabletools}\n\nVisual inspection with `ggplot2` and `autoplot()` gives us an intuitive feel for our time series. We might suspect a trend, see hints of seasonality, and notice unusual observations. But how can we move from intuition to a more formal, statistical understanding of these components? This is where the `feasts` and `fabletools` package comes in.\n\nThe package `feasts`, which stands for **F**eature **E**xtraction **A**nd **S**tatistics for **T**ime **S**eries, is a core part of the tidyverts framework which we have mentioned earlier (@sec-introduction). Its purpose is to provide a toolkit for analysing the *features* of a time series data (tsibble), while `fabletools` supplies the underlying infrastructure that makes this analysis seamless and efficient. Think of it as a Doctor's diagnostic kit: It does not treat the patients (that is the job of forecasting models in @sec-simple-forecasting and @sec-advanced-forecasting), but it runs the tests needed to understand what is going on.\n\n\n\n\n\n## Decomposing a Time Series with `STL()`\n\nA fundamental concept in time series analysis is decomposition. The idea is to split, $y_{t}$, into three additive parts (where the time series is represented as the sum of its parts):\n\n-   **Trend (**$T_{t}$**):** The long-term progression (the increasing or decreasing direction over time)\n-   **Seasonal (**$S_{t}$**):** Regular, repeating patterns over a fixed period (example, yearly or quarterly).\n-   **Remainder (**$R_{t}$**):** The \"leftover\" part after trend and seasonality are removed; this is often considered the random noisy component.\n\n$$\ny_{t}=Trend_{t}+Seasonal_{t}+Remainder_{t}\n$$\n\n### Basic and Advanced Decomposition with Special Arguments\n\nThe most robust method for this **STL,** which stands for **S**easonal and **T**rend decomposition using **L**oess. Loess is a method for estimating non-linear relationships, and its use makes STL method versatile and robust to outliers as it handles a wide range of seasonal and trend shapes. The `feasts` package provides the `STL()` function to perform this decomposition directly on a `tsibble` object. Let us decompose the `Sales` series in the `sales_ts` data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Decompose the Sales series using STL\ndcmp <- sales_ts |> \n  model(STL(Sales))\n```\n:::\n\n\nThe simplest implementation use the default `STL()` function, which automatically selects suitable window sizes for the trend and seasonal components as illustrated in the above code. The `model()` function from the `fabletools` package is used to fit a \"model\" (STL Decomposition) to the tsibble `sales_ts`. The variable to decomposed is specified inside the `STL` function.\n\nFor greater control you can include the variable of interest as a formula where you manually define the smoothing window for the trend and the seasonal components using the special arguments `trend` and `season`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# decompose Sales series using STL with special arguments\ndcmp_sp <- sales_ts |> \n  model(stl = STL(Sales ~ trend(window = 7) + season(window = \"periodic\")))\n```\n:::\n\n\n`trend(window = 7)` sets the window size for the trend smoothing, specifying that a trend should be estimated using a window of $2\\times7+1=15$ observations. This comes from how the **Loess** (Locally Estimated Scatterplot Smoothing) method is typically implemented within the STL algorithm to calculate the **trend component**. A larger window results in a smoother trend.\n\n### Extracting and Viewing Components\n\nThe `dcmp` object we created now contains the fitted decomposition model. To see the results, we use the `components()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# extract the components from the decomposition model\ncomponents(dcmp)\n```\n:::\n\n\nThis returns a `fabletools` object dataset \"`dable`\" (decomposition table) which is a tsibble-like data structure for representing decompositions. It contains the original time index with the Sales series together with four new columns;\n\n-   `trend`: The estimated trend component.\n-   `season_year`: The estimated seasonal component.\n-   `remainder`: The residual or error component (noise).\n-   `season_adjust`: The final seasonally adjusted series (Original Series - Seasonal Component)\n\nThis allows for direct analysis and visualisation of how much of the variations in the `Sales` series is attributed to long-term movements, seasonal factors, and irregular noise. The `autoplot()` function when applied on this `dable` output will automatically generate a multi-panel plot displaying the decomposition of the `Sales` series into its components as calculated by the STL method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# visualise components\ncomponents(dcmp) |> \n  autoplot(linewidth = 0.7)\n```\n\n::: {.cell-output-display}\n![](exploring-patterns_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe plot features for main panels; the original Sales time series ($y_{t}$), The long term direction of the trend, the estimated periodic pattern and the noise or irregular fluctuations.\n\nFrom the STL decomposed plot we see that the original series is highly affected by both seasonality and a trend. The smooth long-term trend shows the overall level of sales over the years. Its starts around 35000 and dips noticeably around early 2018 and then rises back up towards 40,000 by late 2019\n\nThe seasonal panel shows a strong, consistent repeating pattern. The vertical scale (from approximately -5000 to 5000) indicates the magnitude of the seasonal effect relative to the trend.\n\nThe last panel shows the irregular, left over variations (from approximately -10,000 to 10,000). The spikes are quite large relative to the seasonal components. This implies a high amount of **irregular unexplained variability** in the data. The large spikes indicate months where sales were unusually high or low, which could be attributed to a variety of factors.\n\n## Analysing Autocorrelation Patterns\n\nAnother crucial property of time series is **autocorrelation**. Simply put, autocorrelation measures the relationship between a value in a series and its past values (called \"lags\"). For example, is today;s sales figure correlated with the sales figure from the same quarter last year? This is a key concept for building effective forecasting models.\n\nThe `feasts` package provides two very useful functions, `ACF` (Autocorrelation Function) and `PACF()` (Partial Autocorrelation Function) to do this. Below, we see how to check autocorrelation for our `Sales` series.\n\n### Autocorrelation Function (ACF)\n\nThe **ACF** measures the correlation between a time series $y_{t}$ and its past values $y_{t-k}$ (where $k$ is the lag) at all observed lags, without isolating the effect of intermediate lags. High autocorrelation at a seasonal lag (e.g. lag 12 for monthly data) is a strong indicator of seasonality. We use the `ACF()` function to calculate these correlations for the `Sales` series\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# compute the Autocorrelation for Sales\nsales_acf <- sales_ts |> \n  ACF(Sales)\n\nsales_acf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 17 x 2 [1M]\n       lag      acf\n  <cf_lag>    <dbl>\n1       1M -0.0897 \n2       2M  0.0896 \n3       3M -0.271  \n4       4M -0.00704\n5       5M -0.191  \n# ℹ 12 more rows\n```\n\n\n:::\n:::\n\n\nWe can also specify the maximum number of lags to use for computing the correlations with the `lag_max` argument. By default it calculates this number using the formula \"$10\\times log_{10}(N/m)$\".\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# compute autocorrelation with specified lags\nsales_acf <- sales_ts |> ACF(Sales, lag_max = 24)\n```\n:::\n\n\nThe resulting `sales_acf` object is a tsibble that contains the correlation coefficient at the different lags. A significant spike at a particular lag indicates a strong relationship.\n\nWe can visualise the output for easier interpretation using `autoplot()`. The plot of the ACF displays a series of vertical lines known as **correlogram**, where the height of each line represents the correlation at that specific lag.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(sales_acf)\n```\n\n::: {.cell-output-display}\n![Correlogram of Sales series Autocorrelations](exploring-patterns_files/figure-html/fig-acf-1.png){#fig-acf fig-align='center' width=672}\n:::\n:::\n\n\nThe dashed blue lines in the plot represents the significance bounds. Any bar extending outside these bounds is considered a statistically significant correlation. The spikes do not show any slow, steady decline in the values from positive to negative. This suggests there is no strong trend.\n\nThere is no single large significant spike at regular intervals (like at lag 12M or 24M for monthly data). This indicates a lack of strong consistent annual seasonality. However there is a pattern of significant spikes in the first few lags, followed by a series of alternating pattern of negative and positive correlations in the short term. This implies a short term cyclical behaviour with a period of about 6 months.\n\nThis is what this ACF plot might mean in a business context:\n\n-   A customer makes a purchase (**Lag0**).\n-   The negative correlation at **Lag 3** suggests that three months after a purchase, sales tend to be opposite to what they were. If sales were high in a given month, they are likely to be low three months later and vice versa.\n-   The positive correlation at **Lag 6** suggests that whatever the sales pattern was at a given time, it tends to **repeat itself every 6 months**.\n\nThe other spikes later on at lag **14, 15 and 18** are likely harmonics of the primary 6-month cycle. Once a cyclical pattern is in the data, it can create correlations at multiple or fractions of its main period. They are part of the same underlying effect and not separate seasonal effects\n\n### Partial Autocorrelation Function (PACF)\n\nThe **PACF** calculates the correlation between $y_{t}$ and $y_{t-k}$ **after removing the influence of the** **intermediate lags** $y_{t-1}$, $y_{t-2}$, $\\dots$, $y_{t-(k-1)}$. This helps to precisely identify the direct relationship between the current observation and a past observation, filtering out the \"flow-through\" correlation effects. Think of it this way: The sales at lag 3 (3 months ago) might be correlated with sales today because;\n\n-   Sales today are correlated with sales at lag 1 (ACF at lag 1)\n-   Sales at lag 1 are correlated with sales at lag2.\n-   Sales at lag 2 are correlated with sales at lag 3.\n\nThe PACF cuts through this chain and asks: \"If I already know the values at lags 1 and 2, does lag 3 still provide new, direct information today?\n\nThe `PACF()` function is used to calculate the partial autocorrelations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# compute the Partial Autocorrelation for Sales\nsales_pacf <- sales_ts |> \n  PACF(Sales, lag_max = 20) \n\nsales_pacf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 20 x 2 [1M]\n       lag    pacf\n  <cf_lag>   <dbl>\n1       1M -0.0897\n2       2M  0.0822\n3       3M -0.260 \n4       4M -0.0571\n5       5M -0.170 \n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\nsimilar to the ACF, the `sales_pacf` object also contains the partial autocorrelation values for each lag. We can again visualise this output using `autoplot()`. The PACF plot is primarily used to identify the ***order*** of an ***autoregressive*** *(**AR**)* component in a model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsales_pacf |> autoplot()\n```\n\n::: {.cell-output-display}\n![Correlogram of Sales series Partial Autocorrelations](exploring-patterns_files/figure-html/fig-pacf-1.png){#fig-pacf fig-align='center' width=672}\n:::\n:::\n\n\nThe most important aspect of our PACF plot is the behaviour in the first few lags. We notice that the is a significant negative spike at **lag 3**, with the subsequent lags generally becoming smaller and non significant, with no clear pattern. This is a classic signature of a PACF plot where the beginning few vertical bars shows a significant spike and the cuts off.\n\nThe PACF and ACF plots are considered diagnostic plots for identifying an ARIMA model's specification (more information in @sec-advanced-forecasting).\n\n## Feature Based Explorations\n\nAnother powerful aspect of the `fabletools` is its ability to compute a wide array of summary statistics or features from a time series. This is incredibly useful when you have many series (like in the `gh_ts` data with several indicators) and you want to quickly understand their characteristics or group similar series together.\n\n### Identifying Seasonal Patterns and Strength\n\nThe most common feature to compute is the **seasonal strength**, which measures quantitatively how dominant the recurring patterns are compared to the overall trend and random noise. This is typically done using the **STL decomposition**. We demonstrate this with the `tsibbledata::aus_production` tsibble dataset which contains the quarterly production of selected commodities in Australia.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# convert wide data to long \naus_prod_long <- aus_production |> \n  pivot_longer(cols = -Quarter, names_to = 'Product', values_to = 'Quantity')\n\n# calculate seasonal strength and other key features\naus_prod_features <- aus_prod_long |> \n  features(Quantity, features = feat_stl)\n\n# Display seasonal and trend metrics\naus_prod_features |> \n  select(Product, contains(c('seasonal', 'trend')))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  Product     seasonal_strength_year seasonal_peak_year seasonal_trough_year\n  <chr>                        <dbl>              <dbl>                <dbl>\n1 Beer                         0.952                  0                    2\n2 Bricks                       0.859                  3                    1\n3 Cement                       0.834                  3                    1\n4 Electricity                  0.906                  3                    1\n5 Gas                          0.981                  3                    1\n# ℹ 1 more row\n# ℹ 1 more variable: trend_strength <dbl>\n```\n\n\n:::\n:::\n\n\nYou have to change the data to a long format so that the tsibble has a 'key' variable (`Product`). The structure is essential for the `fabletools` feature functions. The `features()` function calculates metrics based on the STL decomposition for each `Product` series separately. The `feat_stl` feature set automatically returns several key metrics including:\n\n-   **`seasonal_strength_year`**: Measures the proportion of variance explained by the seasonal components. The values range between 0 and 1 (higher values mean stronger seasonality).\n-   **`seasonal_peak`/`seasonal_trough_year`**: Identifies the periods (Month for monthly data & Quarter for Quarterly data) of highest and lowest seasonal activity\n-   **`trend_strength`**: Measures the proportion of variance explained by the trend component (higher values mean stronger trend)\n\n### Comprehensive Feature Analysis\n\nBeyond standard seasonality, `fabletools` allows you to calculate a wide array of statistics that characterises every nuance of a time series' behaviour, including measures of shape, volatility and distribution.\n\n**STL Features for Non-Seasonal Characteristics**\n\nThe STL feature set offers metrics beyond simple strength, focusing on the **shape** of the trend and the nature of the remainder (noise).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# compute stl feature set for non-seasonal (yearly) data \nstl_features <- gh_ts |> \n  features(value, features = feature_set(tags = 'stl'))\n```\n:::\n\n\nThis line of code within the the `feature()` function; `faeture_set(tags = 'stl')` instructs `fabletools` to compute the full set of STL-related features. These features include;\n\n-   **`trend_strength:`** Measures how strong the trend component is relative to the remainder (noise). Values close to 1 have strong trend and values close to zero have weak or no trend. Example \"*Female population\"* ($0.9999953$) has and extremely strong trend but \"*Annual GDP growth rate\"* ($0.3091004$) has a weak trend.\n\n-   **`spikiness:`** Measures the presence of sharp intermittent spikes in the remainder (noise) component. High values = more spiky/volatile. Example *\"Rural Population\"* has **extremely high spikiness** ($2.086846\\times10^{14}$), indicating major irregularities.\n\n-   **`linearity:`** Measures how linear the trend is (vs curved) based on fitting a linear model to the trend component. Positive values suggest trend can be well-approximated by a straight line. Negative values suggest non-linear trends. \\*\"Cereal yield \\_kg per hectare\"\\* ($3395.64$) is highly linear.\n\n-   **`curvature:`** Measures the degree of bending in the trend. It complements linearity -captures how much the trend curves. Higher absolute values means more curved. *\"Female population percentage\"* ($-1.741482$) shows negative curvature (concave down)\n\n-   **`stl_e_acf1:`** First order autocorrelation of the remainder component (lag1). The values ranges from -1 to 1. Values near 0 means remainders are random (which is good!). High positive or negative values indicates patterns left in residuals that the STL decomposition did not capture.\n\n-   **`stl_e_acf10:`** Sum of squares of the first 10 autocorrelations of the remainder. Measures the overall autocorrelation structure in residuals. Low values are good and high value indicate significant patterns remain in the residuals.\n\n**Custom Feature Calculations with Lambda Expressions**\n\nIf a desired feature is not pre-defined, you can compute it using a built in function or a simple lambda expression (a short, anonymous function)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# compute any other features using their function or a lambda expression\npercentile_features <- aus_prod_long |> \n  features(Quantity, features = ~quantile(., na.rm=TRUE)) \n```\n:::\n\n\n`features = ~quantile(., na.rm=TRUE)` defines a custom function to run on the Quantity values. The `~` symbol creates the lambda expression and the `.` refers to the vector of time series values. This is a compact way of calculating the $0^{th}$, $25^{th}$, $50^{th}$, $75^{th}$ and $100^{th}$ percentiles for each `Product` series, giving insight into its distribution and volatility range.\n\n**Multiple Custom Features (List of Functions)**\n\nYou can also calculate several custom features simultaneously by passing a named list of functions to the `features` argument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# for multiple custom features create a list of functions \ncent_tend_features <- gh_ts |> \n  features(value, features = list(sd = ~sd(., na.rm=T), median = ~median(., na.rm=T), mean = ~mean(., na.rm=T)))\ncent_tend_features\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 × 4\n  indicator_name                            sd     median       mean\n  <chr>                                  <dbl>      <dbl>      <dbl>\n1 Annual GDP growth rate                 4.20        4.40       3.71\n2 Annual population groeth rate          0.290       2.46       2.50\n3 Cereal yield _kg per hectare         476.       1141.      1249.  \n4 Crude death rate_per 1000 people       4.18       11.4       12.6 \n5 Female population                4132889.    8127176    8926929.  \n# ℹ 17 more rows\n```\n\n\n:::\n:::\n\n\nThe custom anonymous functions supplied to the `features` argument in a list, calculates t**hree measures of central tendency and dispersion**. For every indicator in the `gh_ts` data, the standard deviation (`sd`), the median, and the mean are calculated. This can help you understand the **average level** of the series and its overall volatility across the entire period.\n\n## Comprehensive Diagnostic Plots for Model Selection\n\nThe `gg_tsdisplay()` function from the `feasts` package (now often found in the `ggtime` package) makes it possible to create specialised plots that combine multiple diagnostics to guide model selection. Especially an ARIMA model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a comprehensive diagnostic plot for Gross National Expenditure\ngh_ts |> \n  filter(indicator_name == \"Gross national expenditure (% of GDP)\") |> \n  gg_tsdisplay(value, plot_type = \"partial\") +\n  labs(title = \"Comprehensive Diagnostic Plot for GNE (% of GDP)\",\n       subtitle = \"Series plot, ACF, and PACF for model selection guidance\")\n```\n\n::: {.cell-output-display}\n![An Ensemble of TIme Series Plots Containing the Orignial Series(Top), ACF plot (bottom left) and PACF plot (bottom right)](exploring-patterns_files/figure-html/fig-diag-plot-1.png){#fig-diag-plot fig-align='center' width=672}\n:::\n:::\n\n\nHere we filter the `gh_ts` data to isolate the specific series of interest; the `Gross national expenditure (% of GDP)` indicator. `gg_tsdisplay(value, plot_type = \"partial\")` generates a three panel plot:\n\n-   The main panel (top) shows the Series Plot of the `Gross national expenditure (% of GDP)` values over the time period\n-   The bottom left panel shows the ACF\n-   The bottom right panel shows the PACF (specified by `plot_type = \"partial\")` )\n\nThe `plot_type` argument controls what type of plot is displayed in the bottom right panel it can either be a PACF, histogram, lagged scatterplot or spectral density depending on what you specify. The values that can be specified for this argument can be found in its help documentation.\n\nThe series show high **volatility** with **no** **clear** **trend**, the ACF shows a slow decay where the spikes remain significantly positive until **lag 7**, the PACF shows a **sharp cut-off** after **Lag 1**. All these indications serve as diagnostic signs for a forecasting model selection.\n\n## Summary and Next Steps\n\nIn this chapter we moved beyond visual inspection and explored the `feasts` and `fabletools` package to perform a rigorous, quantitative diagnosis of our time series. We systematically:\n\n-   Decomposed the a time series using `STL` to isolate the trend, seasonal and remainder components.\n\n-   Analysed autocorrelation with `ACF` and `PACF` to understand how each value depends on its past.\n\n-   Quantified pattern strength using `features()` to calculate statistics like `seasonal_strength` and `trend_strenght`, confirming our visual observations with hard numbers.\n\n-   Created diagnostic plots using `gg_tsdisplay()` to get comprehensive multi-panel view of a time series' behaviour, which is essential for guiding our model selection.\n\nWith a deep understanding of patterns in our data, we are now ready to build forecasting models. But before we do, a critical step remains. In the next section, we will cover **\"Preparing Data for Forecasting\"**.\n",
    "supporting": [
      "exploring-patterns_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}