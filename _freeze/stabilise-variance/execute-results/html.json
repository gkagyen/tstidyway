{
  "hash": "597c4f1a9debdda8d7d858ab1bee6f3e",
  "result": {
    "engine": "knitr",
    "markdown": "# Transformations for Variance Stabilisation {#sec-variance-stabilisation}\n\nMany time series models operate under the crucial assumption of **homoscedasticity**–that the variance of the data remains constant over time. However, real world data often violates this assumption, displaying heteroscedasticity where the variability changes with the level of the series. Think of a growing business where seasonal sales swings become larger as the overall sales volume increases, or a temperature variation that become more extreme as climate patterns shift.\n\nWhen variance is not constant, our models can produce unreliable prediction intervals–they might be too narrow during high variance periods and too wide during low variance periods. Variance stabilisation transforms our data to have more consistent variability throughout, making it better suited for modelling while often helping to normalise the distribution of errors.\n\n\n\n\n\n## Identifying Heteroscedasticity in Time Series Data\n\nHeteroscedasticity refers to a change in the volatility and spread of a time series over time. Time series models (like ARIMA or ETS) assume that the variance of the forecast errors is constant. If the variance is changing it compromises the accuracy of confidence intervals and predictions.\n\nWe will examine the `gh_ts` and `sale_ts` data and check for signs of changing variance\n\n### Visual Inspection of Variance\n\nThe initial step is always to plot the data and visually inspect the magnitude of the fluctuations\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\n# monthly sales data\nsales_ts |> \n  autoplot(Sales, linewidth = 0.7)\n\n# Ghana: annual GDP growth rate \ngh_ts |> \n  filter(indicator_name == \"Annual GDP growth rate\") |> \n  autoplot(value, linewidth = 0.7)\n```\n\n::: {.cell-output-display}\n![Monthly Sales Time Series (2015-2019)](stabilise-variance_files/figure-html/fig-ts-plots-1.png){#fig-ts-plots-1 fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Annual GDP Growth Rate Time Series (1960-2024)](stabilise-variance_files/figure-html/fig-ts-plots-2.png){#fig-ts-plots-2 fig-align='center' width=672}\n:::\n:::\n\n\nThe monthly sales plot does not show a **clear change in variance** across the observed period. The magnitude of the fluctuations (the vertical distance between the seasonal peaks and troughs) remains **roughly the same** across the entire 5-year period. The values consistently oscillate between approximately $\\text{GH₵}10,000$ and $\\text{GH₵}50,000$ from 2015 through 2019. This implies the series is generally homoscedastic with respect to the mean.\n\nOn the other hand The Annual GDP growth rate plot is a classic example of variance change. The fluctuations are **much larger and more dramatic** in the early period (1960s to early 1980s). During this time, the values frequently swing between +10 and -10. After the mid-1980s, the fluctuations generally **reduce and stabilise**, mostly staying within the 0 to +10 range. There is a strong indication of variance reduction around the mid 1980s.\n\n### Quantitative Check Using the `guerrero` Feature\n\nWhile a formal statistical test for heteroscedasticity requires fitting a model and examining the squared residuals, the **Guerrero method** offers a quick quantitative diagnostic to determine if a transformation is required to achieve homoscedasticity or not. This is a technique to automatically select the optimal parameter ($\\lambda$) for a Box-Cox transformation. Th `feature()` function will help us do this.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate optimum lambda value for sales data\nsales_ts |> \n  features(Sales, guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  lambda_guerrero\n            <dbl>\n1           0.726\n```\n\n\n:::\n\n```{.r .cell-code}\n# calculate optimum lambda value for Annual GDP growth rate series\ngh_ts |> \n  filter(indicator_name == \"Annual GDP growth rate\") |> \n  drop_na() |> \n  features(value, feature = guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  indicator_name         lambda_guerrero\n  <chr>                            <dbl>\n1 Annual GDP growth rate            1.48\n```\n\n\n:::\n:::\n\n\nThe $\\lambda$ value is interpreted based on its proximity to three key values: 1, 0, or a non-integer value. The lambda value produced for the sale series (0.73) is close to 1 which suggests no transformation. This strongly supports the initial visual inspection which showed that the monthly sales data is largely homoscedastic (stable variance).\n\nThe lambda value for the annual GDP growth rate series (1.48, also close to 1) suggests that the variance is not proportional to the mean in a way that requires a a Box-Cox transformation (where $\\lambda\\le1$). This means the variance change observed in the visual inspection is not a simple multiplicative effect. The standard Box-Cox transformation is not sufficient here.\n\n## Applying Box-Cox Transformations\n\nThe Box-Cox transformation which we mentioned earlier is a family of power transformations that can stabilise variance across different levels of our time series. The transformation is defined as:\n\n$$\ny^{(\\lambda)}= \\left\\{ \\begin{array}{cl}\\frac{y^{\\lambda}-1}{\\lambda} & if\\ \\ \\lambda\\geq 0 \\\\log(y) & if\\ \\ \\lambda = 0\\end{array} \\right.\n$$\n\n\\$\\$\\$\\$\n\nRather than guessing the optimal $\\lambda$ value, we can use the `guerrero()` feature as seen earlier from the `feasts` package to find it automatically.\n\nWe have already seen how to do this and also know that the `Sales` series and the `Annual GDP growth rate` series do not warrant any type of Box-Cox transformation. We will use the `aus_production` data to demonstrate how the Box-Cox application works\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Visualise Quarterly Cement production in Australia \naus_production |> \n  autoplot(Cement, linewidth = 0.7)\n```\n\n::: {.cell-output-display}\n![Quarterly Cement Production in Australia (1956-2010)](stabilise-variance_files/figure-html/fig-cement-aus-1.png){#fig-cement-aus fig-align='center' width=672}\n:::\n:::\n\n\nThe visual inspection shows a clear evidence of unequal variance, specifically increasing variance over time. The series is at a **low level** in the early periods (1960-1980Q1) and the fluctuations are small relative to the mean (seasonal peaks and troughs closer together).\n\nIn the later period (1990 - 2008Q4) the series is at a **high level** and the fluctuations are visually much larger (the distance between peaks and troughs has increased). This makes the series heteroscedastic because the **variance increase as the level (mean production) increases**.\n\nThis pattern is known as multiplicative variance and is a characteristic of a time series that exhibit growth. We will now calculate the optimal lambda needed for a Box-Cox transformation for this series.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate and extract optimal lambda using the guerrero method\noptimal_lambda <- aus_production |> \n  features(Cement, features = guerrero) |> \n  pull(lambda_guerrero)\n```\n:::\n\n\nThe optimal lambda value calcuated is $-0.309022$ (close to 0) which clearly suggests a transformation is required and confirms our visual inspection. In practice you would typically use a log transformation $(\\lambda=0)$ for such values (closer to zero) rather than the exact $\\lambda=-0.309022$. This is because a log transformation is simpler and more interpretable which performs similarly to nearby lambda values (in which our `optimal_lambda` falls squarely in that range). Now let us transform and compare the results\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Apply Box-Cox transformation with optimal lambda\ntransformed_aus_cement <- aus_production |> \n  select(Quarter, Cement) |> \n  mutate(trans_Cement = box_cox(Cement, lambda = round(optimal_lambda)))\n\n# Visualise original vs transformed series\ntransformed_aus_cement |> \n  pivot_longer(\n    cols = -Quarter,\n    names_to = 'Version',\n    values_to = 'Sales_value'\n  ) |> \n  autoplot(Sales_value, linewidth = 0.7, show.legend = FALSE) +\n  facet_wrap(vars(Version), scales = 'free_y',\n             labeller = labeller(Version = c('Cement' = 'Original Series', 'trans_Cement' = \"Transformed Series\"))) +\n  labs(title = \"Original vs. Box-Cox Transformed Cement Production Series\",\n       subtitle = paste(\"Transformation parameter λ =\", round(optimal_lambda)),\n       y = 'Cement Production',\n       x = \"Quarter\")\n```\n\n::: {.cell-output-display}\n![Visually Comparing the Original Cement Series with The Log Transformed Series](stabilise-variance_files/figure-html/fig-bc-comp-1.png){#fig-bc-comp fig-align='center' width=672}\n:::\n:::\n\n\nThe transformation is applied using the `box_cox()` function from the `fabletools` package. It takes the raw `Cement` values and applies the Box-Cox formula using the provided lambda. We round lambda to the nearest whole number (0) to get a log transformation.\\\nComparing the original and transformed series you can clearly see how the fluctuations look similar throughout the entire period for the transformed series, whiles still showing the upward trend from the original series (preserves trend direction).\n\nThe peaks and troughs for all quarters seems to have similar amplitudes throughout the years. Also notice how the y-axis scale changed from \\~500 - 2500 in the original series to \\~6.5 - 7.5 (log scale) in the transformed series giving it a much more constant variance over time. The transformed series is now suitable for ARIMA or ETS modelling as the constant amplitude of fluctuations satisfies the constant variance assumption.\n\n::: callout-note\nRounding the exact $\\lambda$ to a simpler, nearby value (like 0 for a log transformation) is a common, pragmatic step for better interpretability. After forecasting we can back-transform to original scale: `exp(log_forecast)`\n:::\n\n## Interpreting Transformation Parameters and Assessing Effectiveness\n\n### Interpreting Transformation Parameters\n\nThe **Box-Cox** transformation is governed by the parameter $\\lambda$ (lambda), which dictates the power applied to the data. Different lambda values correspond to different transformations. Understanding the relationship between lambda and the resulting transformation is key to time series data preparation.\n\n| lambda $(\\lambda)$ value | Recommended Transformation | Variance Implication |\n|----|----|----|\n| $\\lambda=1$ | No Transformation $(Y_{t})$ | Series is already Homoskedastic |\n| $\\lambda\\ \\text{close to 1}$ *(e.g., 0.7 to 1.3)* | Transformation provides minimal benefit | Variance is largely stable |\n| $\\lambda=0.5$ | Square Root $(\\sqrt{Y_{t}})$ | Moderate stabilisation required |\n| $\\lambda=0$ | Natural Logarithm $(\\log(Y_{t}))$ | Variance is proportional to the level (Multiplicative variance) |\n| $\\lambda=-1$ | Inverse $(1/Y_{t})$ | Severe stabilisation required; common when variance decreases as the mean decreases |\n\nWe demonstrate how different lambda values visually impact the cement production data with the code below.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# apply and compare multiple common lambda values for demonstration\ncomparison_data <- aus_production |> \n  select(Quarter, Original = Cement) |>\n  mutate(\n    Log_transform = box_cox(Original, lambda = 0),    # Natural log\n    Sqrt_transform = box_cox(Original, lambda = 0.5), # Square root\n    Cube_root = box_cox(Original, lambda = 1/3),      # Cube root\n    Optimal_λ = box_cox(Original, lambda = optimal_lambda)\n  )\n\n# plot all transformations for comparison\ncomparison_data |> \n  pivot_longer(\n    cols = -Quarter,\n    names_to = \"Transformations\",\n    values_to = \"Value\"\n  ) |> \n  ggplot(aes(x=Quarter, y=Value, colour = Transformations)) +\n  geom_line(linewidth = 0.7) +\n  facet_wrap( ~Transformations, scales = \"free_y\", ncol = 2) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Comparing Different Box-Cox Transformations with their Original Series Visually](stabilise-variance_files/figure-html/fig-compare-lambda-1.png){#fig-compare-lambda fig-align='center' width=672}\n:::\n:::\n\n\nThe comparative plot confirms that transformations with lambda values near zero (Log, Cube Root, and Optimal λ) produce visually similar variance-stabilised series, while the Square Root transform $(\\lambda=0.5)$ slightly undershoots the required stabilisation. Notice how each series has varying y-scale due to the type of transformation applied.\n\n::: callout-tip\nFor log transformations use `log1p()` if your series data contains zeros\n:::\n\n### Assessing Transformation Effectiveness\n\nAlthough visual inspections provide insightful patterns and trends, we can statistically verify that the transformation successfully made the variance independent of the mean. This is done by examining the relationship between the rolling mean (level) and the rolling standard deviation (volatility)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(slider)          # for roling mean and standard deviation\nlibrary(patchwork)       # for plot layout\n\n# Calculate rolling mean and variance\nvariance_mean_relationship <- transformed_aus_cement |> \n  as_tibble() |> \n  mutate(\n    # Original series\n    orig_mean = slide_dbl(Cement, mean, .before = 16, .complete = TRUE),\n    orig_sd = slide_dbl(Cement, sd, .before = 16, .complete = TRUE),\n    \n    # Transformed series\n    trans_mean = slide_dbl(trans_Cement, mean, .before = 16, .complete = TRUE),\n    trans_sd = slide_dbl(trans_Cement, sd, .before = 16, .complete = TRUE)\n  ) |> \n  filter(!is.na(orig_mean))\n\n# Plot variance-mean relationship\np1 <- variance_mean_relationship |> \n  ggplot(aes(x = orig_mean, y = orig_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Original Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\np2 <- variance_mean_relationship |> \n  ggplot(aes(x = trans_mean, y = trans_sd)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n  labs(\n    title = \"Transformed Series\",\n    x = \"Rolling Mean\",\n    y = \"Rolling SD\"\n  ) +\n  theme_minimal()\n\np1 + p2 \n```\n\n::: {.cell-output-display}\n![Mean-Variance Relationship For Original Series and Transformed Series](stabilise-variance_files/figure-html/fig-var-mean-1.png){#fig-var-mean fig-align='center' width=672}\n:::\n:::\n\n\nThe code above uses the `slide_dbl` function from the `slider` package to calculate the rolling standard deviation over a 17-quarter window (`.before = 16`)\n\nThe Left plot (Original Series) shows a clear linear relationship between its rolling mean and rolling standard deviation (SD). This is a clear heteroscedasticity where variance grows with the series' level, which further confirms that the original series has a **variance proportional to the mean**. The nearly flat blue line in the left plot indicates how the transformation has now **stabilised the variance** across all levels. The rolling SD stays relatively constant (\\~0.07 to \\~0.125) regardless of the rolling mean.\n\nPicking a rolling window that captures the full business cycle is very essential for a stable estimation while still remaining sensitive to local patterns. With quarterly data, 17 quarters or \\~4 years (rolling window) is a good compromise between smoothness and responsiveness. We could also quantify this relationship with correlations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# correlation for original series and transformed series \ncor_original <- cor(\n  variance_mean_relationship$orig_mean,\n  variance_mean_relationship$orig_sd\n)\n\ncor_transformed <- cor(\n  variance_mean_relationship$trans_mean,\n  variance_mean_relationship$trans_sd\n)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal Series correlation (mean vs SD): 0.883\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTransformed series correlation (mean vs SD: 0.018\n```\n\n\n:::\n:::\n\n\nThis is the most objective evidence for assessing the transformations effectiveness. The strong positive correlation **(88.3%)** in the original series confirms heteroscedasticity (volatility depends on the level). The near zero correlation **(1.8%)** in the transformed data provides conclusive statistical evidence that the Box-Cox transformation was highly effective and has properly prepared the Cement Production time series for subsequent time series modelling.\n\nThe optimal transformation often represents a balance between variance stabilization and interpretability. While mathematical optimization helps, domain knowledge should guide final decisions, for example, log-transformed sales data can be interpreted in terms of percentage changes, which may be more meaningful for business stakeholders.\n\n## Summary\n\nWe have tackled a fundamental challenge in time series forecasting; dealing with changing variances over time. We learned that many forecasting models assume constant variance, but real world data often displays a changing variance, where variability decreases or increases with the series level.\n\nThe key insight emerged that variance stabilisation is not just a mathematical exercise, it is about creating a series where forecasting models can learn consistent patterns rather than being misled by changing variability.\n\nIn our next chapter we will look at another critical assumption of time series modelling - stationarity. We will explore what it means for a data to be stationary tests to confirm it and techniques to transform our series into a stationary one.\n",
    "supporting": [
      "stabilise-variance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}